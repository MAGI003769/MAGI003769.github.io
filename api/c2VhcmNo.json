[{"title":"强行置顶：关于这个博客 About this blog","date":"2069-01-01T08:00:00.000Z","date_formatted":{"ll":"Jan 1, 2069","L":"01/01/2069","MM-DD":"01-01"},"updated":"2019-05-04T03:40:24.000Z","content":"\n“If you can’t explain it to a six year old, you don’t understand it yourself” \n—— Albert Einstein \n\nWhy this blog?开篇的这句话，大概就是我见这个博客的目的了。在自己学习的过程中，很难去界定究竟什么才算是掌握。通过阅读书籍，我有可能能把专业术语和一些大的概念说的头头是道，像模像样。但这却不一定说明自己真的掌握了知识，所读所学也未必记得牢靠。因此，学习不能够只有输入，还需找一个地方输出。除了不停地练习，习题也好代码也罢，向别人解释你所学的专业知识也会很有帮助。可以从这个过程中审视自己的学习，有没有漏洞，是不是真的明白。\n然而，跟别人讲述的机会总不是太多。不能给别人讲，还不能说给自己听吗。所以这个博客很大程度上是写给自己看的，算是有个记录，有些东西忘了也好回来找。如果您误打误撞点了进来，发现了我这边哪里有错误，欢迎批评指正。当然，既然是个人博客也会没事儿发发牢骚，分享一点自己喜欢的东西。另外，这个博主人很丧，可能没事儿写一点有的没的，可以忽略。\nChinese or English?最开始写这个博客的时候，是会用英语写的。从早几篇的 Hexo 教程和 C++ 基础知识开始，本来的想法是在学东西的过程中也提升一下英语水平。结果发现，效果并不好，很多东西因为当时是在看英文文档，也懒得去 paraphrase 了，有很明显的和文档重合的痕迹。经过自己大脑处理的东西略少，记得就不是那么牢了，而且效率也并不是很高。\n后来慢慢的开始用中文写了一些，发现情况会稍微好一些。可能我本身的英语逻辑并不强，还是需要一个转化成中文逻辑的过程，自己才能更好地消化和吸收学到的知识。正如上面所提到的，创建这个博客的目的之一就是希望能对自己所学知识有一个收集和整理。用什么语言写的问题我也纠结了很久，既然是一辅助学习为目的，那么英语能力提升这个功能就相对淡化了一些。所以，以后还是多用中文写吧，谁让我是英语渣…….\nWelcome！感谢主题的开发者，这两天再配置一下评论插件，Feel free to leave your comments！\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/home-2.jpg","plink":"https://magi003769.github.io/post/关于这个博客/"},{"title":"MAC上配置Hexo部署GitHub Page","date":"2020-05-10T07:00:00.000Z","date_formatted":{"ll":"May 10, 2020","L":"05/10/2020","MM-DD":"05-10"},"updated":"2020-05-11T20:31:19.919Z","content":"前天熬过最后一场final，终于闲下来给这地方扫扫灰。这两天在MAC上配置了一些Hexo，虽然还有404页面和一些渲染的小问题，但是折腾了两天总算是能把最新的deploy到GitHub Page上了。在这写一下踩过的一些坑，顺便庆祝暑假开始，希望这个假期能多学点多写点。\n配置 Hexo这个就跟之前写的Windows上面配置差不多，完全可以参考Hexo的官方文档。或者看一下这篇也可以mac下搭建hexo+github。Git肯定都已经有了，剩下的就是安装Node.js。\n1brew install nodeNode.js利用Homebrew进行安装就方便管理，虽然中间踩了一些坑（后面会提到），但是也因此让操作变得简单了一些。然后就是安装Hexo了，一句话结束。\n1npm install -g hexo-cli到这里基本的框架都已经安装完成了，剩下的就是初始化博客。在合适的位置新建一个folder用来初始化。注意这里的folder要是空的，否则就彻底删除重头再来。\n1hexo init然后安装npm\n1npm install这之后folder里面Hexo博客必须的文件了，包括node中的一些必要的module。因为是从之前机器上迁移过来，只要把 source/_post 路径下单的Markdown文件替换，并加入想要的theme就行了。\n踩坑遇到的第一个坑是在Homebrew上面，安装Node的时候一直提示错误，没有某个路径的修改权限。输入brew doctor 会出现比如下面的问题。\n1\"Warning: /usr/local/lib/pkgconfig isn't writable.\"其实可以按照终端里面给出的建议就可以了，只是StackOverflow的这篇帖子解释的更清楚些，并讲了一下相关的过程。核心其实就是获取该路径的修改权限。\n1sudo chown -R $(whoami) /usr/local/lib/pkgconfig之后搭建博客的过程中，就遇到了和这篇知乎文章几乎一模一样的问题。不论是运行 hexo g 或是 hexo s，终端都会显示如下烦人的warning。最后 hexo d 临门一脚就大功告称的时候，直接因为这几个warning无法部署。\n1234567(node:9876) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency(Use `node --trace-warnings ...` to show where the warning was created)(node:9876) Warning: Accessing non-existent property 'column' of module exports inside circular dependency(node:9876) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency(node:9876) Warning: Accessing non-existent property 'lineno' of module exports inside circular dependency(node:9876) Warning: Accessing non-existent property 'column' of module exports inside circular dependency(node:9876) Warning: Accessing non-existent property 'filename' of module exports inside circular dependency这个情况就和Node.js的版本有关系了，之前用Homebrew安装的是默认最新版的Node.js 14，和Hexo之间似乎存在兼容性问题。而想要解决上面的问题就需要把安装的Node.js回滚到更稳定、更加兼容的老版本（12即可）。具体的操作是参照StackExchange的一篇帖子来的，分为以下几个步骤：\n检查利用brew安装的node版本\n1brew search node安装想要的node版本\n1brew install node@12我们可以在本地安装多个版本的node，但是我们不能同时使用它们。所以我们需要将最新版的node与brew解除连接。\n1brew unlink node禁用最新版之后，我们需要将brew和后来安装的老版本进行连接\n1brew link node@12输入这个之后发现link失败，提示该版本的node为key-only。这样我们就用下面的命令进行强制连接，从而使得老版本的node成为Hexo的依赖。\n1brew link --force --overwrite node@12","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/sweetgun-k_pBB5wJtaU-unsplash.jpg","plink":"https://magi003769.github.io/post/MAC-hexo/"},{"title":"Java多线程基础","date":"2019-08-02T07:00:00.000Z","date_formatted":{"ll":"Aug 2, 2019","L":"08/02/2019","MM-DD":"08-02"},"updated":"2020-05-12T08:48:54.425Z","content":"多线程几乎是开发必考的问题，需要梳理一下相关的知识和概念。进程与线程的关系、从JVM的角度分析二者、并发与并行的区别、为什么要使用多线程以及使用多线程可能带来的问题。\n进程与线程从简单的日常使用出发，我们可以说运行的文本编辑器、音乐播放器以及浏览器都分别是一个进程。\n\n进程是程序一次执行的过程，是系统运行程序的基本单位。\n\n进程是动态的，系统运行一个程序就是一个进程从创建、运行再到消亡的过程。但是操作系统调度的最小单元不是进程而是线程，二者之间是一种包含关系。\n\n线程是操作系统调度的最小单元。它比进程更小，可以看做轻量级的进程，一个进程可以包括单个或多个线程。\n\n多个线程会共享一部分资源（堆和方法区），而进程则各自有相对独立的内存资源。因此线程之间的相互切换的开销跟进程相比要小很多。\n\n进程vs线程两者的优缺点上讲，多进程的优点在于稳定性相比多线程要高，一个进程崩溃了并不影响其他进程，而多线程的情况下任何线程崩溃则可能导致整个进程崩溃。虽然多进程有稳定性上的优势，然而却有前面提到了开销的问题，具体在以下两个方面：\n创建进程比创建线程开销大，尤其是在Windows系统上；\n进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。\nJava的多线程Java实际上内置了多线程：一个Java进程是一个JVM进程，程序的主线程执行main()方法，而在main()内部我们可以启动多个线程。另外在程序执行期间，JVM还有负责垃圾回收的其他工作线程。常用的Windows、Linux等操作系统都采用抢占式多任务，如何调度线程完全由操作系统决定，程序自己不能决定什么时候执行，以及执行多长时间。\n因此，对大多数Java程序来说，实现多任务实际上通常是指如何用多线程实现多任务。Java多线程的特点就在于：\n多线程模型是Java最基本的并发模型\n后续读写网络、数据库、web开发基本都依赖于多线程。\n核单线程相比，多线程容易带来的问题在于：数据需要共享合同步。我们使用多线程实现并发编程，是为了更高的效率，但由于部分资源和数据是共享的，就会出现一些具体问题，例如：内存泄漏、上下文切换、死锁以及资源限制。\n并发与并行上面提到了并发，这里注意区分一下并发与并行的概念：\n并发：同一时段，多个任务都在执行（单位时间内不一定同时执行）\n并行：单位时间内，多个任务同时执行\n一个是来回切换，另一个是严格意义上的同时进行。\nJVM的视角下面是抽象的Java运行时的内存区域。我们可以看到，同一进程下的多个线程有共享资源，也有各自私有的资源：\n共享资源：堆和方法区\n私有资源：虚拟机栈、本地方法栈、程序计数器\n每个进程的资源为线程所共享，进程可以在多CPU的情况下同时运行，而线程则因为共享相同的资源而需要在CPU的调度下分时段执行。\n\n堆和方法区堆和方法区是所有线程的共享资源，其中堆是最大的一块内存区域，主要用于新创建的对象（所有的对象都是在这里分配内存），方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。\n为什么会有私有的部分程序计数器：它的作用是实现代码流程控制，因此当我们使用多线程在不同线程之间来回切换的时候，则需要记住当前执行到的位置，保证下次在其换回来的时候能继续按照指令运行。\n剩下两个，都是为了保证局部变量不被其他线程访问到而私有的。\n虚拟机栈：Java方法在执行时都会创建一个栈帧用于储存局部变量表、操作数栈、常量池引用等信息。从方法调用，直到完成的过程都对应着一个栈帧在虚拟机栈中入栈和出栈的过程。因此通常说的Stack Overflow其实是发生在这个位置。\n本地方法栈：这个其实作用和虚拟机栈差不多，区别在于这里的方法是JVM中C/C++实现的Native方法，为虚拟机使用到的Native方法提供服务。\nReferences廖雪峰Java教程\nBunoob - Java教程\nJava Guide\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/index-bg.jpg","plink":"https://magi003769.github.io/post/Java多线程基础/"},{"title":"CS61B Week1：Java入门","date":"2019-08-01T07:00:00.000Z","date_formatted":{"ll":"Aug 1, 2019","L":"08/01/2019","MM-DD":"08-01"},"updated":"2019-08-17T03:57:28.000Z","content":"Java基础这部分是CS61b Lecture 1 内容的总结，非常概括的介绍了 Java 的一些基础知识。我们需要了解这些有关 Java 的基本特性，才能在后面的过程中更好的学习和写 Java 代码。\nComplation下图展示是比较简略的 Java 程序运行流程。.java 文件是源代码，它通过JDK中的 javac编译器产生 .class 文件，方便后续的JRE中的解释器执行。\n\nObject-Oriented所有的 .java 文件都必须包含类的声明。所有的代码实现都要依赖类的存在，甚至包括 helper functions、global constants 等等。如果一个编译之后的 .class 文件想要直接运行，那么它的源文件中定义的类就必须包含 main 方法\n1public static void main(String[] args)Java中所有的函数都必须是某个类的一部分，所以Java中的函数（Function）都是类的方法（Method）。想要声明一个函数时，public static是关键，这个会在后面解释。\nStatic TypingJava中的变量都要在使用前声明，并明确数据类型且在之后的代码中无法修改。代码中使用的变量、传递的参数、函数返回的值（不能返回多个），这些都需要声明好。Java会在编译阶段进行类型检查，而不是像 Python 在执行的过程中进行。这也是造成程序执行速度差的原因之一。Static Typing 可以为我们带来一下的好处：\n获取特定类型的错误，代码可读性更强，也更易理解，可以使得程序员debug更加方便\n用户端可以尽可能避免类型引发的错误\n避免运行时的类型检查（runtime type check），程序运行更高效\nSyntax Issues鉴于我是用惯了 Python 才开始学 Java 的菜鸡选手，再加上面提到了变量类型声明的相关需要，Java 的一些格式或者说语法要求就在这里多说一遍：\n用 { }来区分段落，而不是缩进\n每一行命令后面要以 ; 结尾，花括号后面不需要\n变量声明的时候要注明数据类型，e.g. int x = 0\n函数声明要注明返回值的类型，是否静态等等。\n声明类的文件必须和韦德名称保持一致，否则会出现如下的错误\n\n类相关基础关于类的这部分是 Lecture 2 中的内容，主要介绍 Java 中类的定义、实例化以及主要包括静态方法和实例方法的一些细节。课程中定义了 Dog 这个类作为例子，其中声明了它的属性 weight和各种方法，包括静态和非静态两种。\n1234567891011121314151617181920212223242526272829303132333435363738public class Dog&#123;    public int weight; // attribute        /** Constructor */    public Dog(int w)&#123;        weight = w;    &#125;        /** Static methods */    public static void simpleNoise() &#123;        System.Out.println(\"bark!\");    &#125;        public static Dog maxDog(Dog d1, Dog, d2) &#123;        if (d1.weight &gt; d2.weight) &#123;            return d1;        &#125;        return d2;    &#125;        /** None-static/Instance method */    public void makeNoise() &#123;        if (weight &lt; 10)&#123;            System.Out.println(\"yip!\");        &#125; else if (weight &lt; 30)&#123;            System.Out.println(\"bark!\");        &#125; else &#123;            System.Out.println(\"woooof!\");        &#125;    &#125;        public Dog maxDog(Dog d) &#123;        if (this.weight &gt; d.weight) &#123;            return this;        &#125;        return d;    &#125;&#125;我们另外声明了一个类，用来声明主方法并调用我们的 Dog。后面讨论的各种代码都是在这个部分作更改再编译运行的。\n12345public class DogLauncher &#123;    public static main(String[] args) &#123;        /** Some codes here */    &#125;&#125;定义和实例化我们可以通过以下指令创建 Dog 类的两个实例d1、d2。实例化通过使用一个数的 int 作为参数来设置实例的属性。需要注意的是实例创建时需要声明类型，创建时需要使用关键词 new 和构造器来进行实例化，这是与 Python 中最大的区别。\n12Dog d1 = new Dog(15);Dog d2 = new Dog(50);如果我们想要创建一系列的实例组成一个 array，我们可以\n1Dog[] manyDogs = new Dog[4]这里创建的实例没有使用需要整数的构造器，因此仅仅是确定了 array 中元素的类型，其中的元素都是 Null。如果在这之后试图直接获取每个元素的weight属性，就会报错：Exception in thread &quot;main&quot; java.lang.NullPointerException。\n静态方法与实例方法按照我们在类中的声明，我们有两个maxDog()方法，它的作用是选出两只狗体重较大的一个。仅有一个实例作为参数的是非静态方法（实例方法），有两个实例作为参数的是静态方法。依然使用上面的两个实例，我们使用实例方法，可以获得结果50。\n12345678Dog bigger = d1.maxDog(d2);System.out.println(bigger.weight);/*PS E:\\GitHub\\CS61b-Data-Structures&gt; javac DogLauncher.javaPS E:\\GitHub\\CS61b-Data-Structures&gt; java DogLauncher50*/同样的，我们按照下面的方式使用静态方法也可以得到相同的正确结果。我们需要注意到两个方法的调用方式：前者通过具体的实例调用方法；后者可以直接使用类的名称来调用，而不需要创建实例。这也是这两种函数声明方式产生的最大区别。如果想要使用一些其实不需要依赖对象存在的操作或者逻辑过程，比如说Math.xxx() 这种，我们就可以将所有的方法都声明成 static 方便调用。\n12345678Dog bigger = Dog.maxDog(d1, d2);System.out.println(bigger.weight);/*PS E:\\GitHub\\CS61b-Data-Structures&gt; javac DogLauncher.javaPS E:\\GitHub\\CS61b-Data-Structures&gt; java DogLauncher50*/这里展示一下，不通过实例而通过类调用非静态方法，是无法通过编译的。\n12Dog bigger = Dog.maxDog(d2);System.out.println(bigger.weight);\n总之，如果一个方法或者属性是静态的，我们就通过类来直接调用，不需要实例化；反之，对于非静态函数，我们就必须先创建一个实例然后才能使用。非静态函数中，想要访问调用方法的实例本身的某个属性，使用this.weight 或者直接使用 weight都是可行的。而静态函数的使用根本不涉及实例，因此除了可以使用参数实例的属性（d1.weight &gt; d2.weight）之外是存在其他属性可以使用的，也就没有所谓的 this 指针。而\n关于main函数1public static void main(String[] args)这是开篇提到的一个函数声明，根据之前的只是我们就可以解决它的意义：public static 使其成为一个方便调用的静态方法，void表示它不存在返回值，那么参数列表里的 String[] args是什么意思呢？它实际上是涉及到 Java 程序的命令行使用。如果我们在 DogLauncher 的主方法的最前面加上指令\n1System.out.println(args[0]);并在运行时在原有命令的后面加上一串字符&#39;hello&#39;\n1$ java DogLauncher hello那么在执行各种实例化和其他操作之前，这串字符就会被输出出来。\n使用Library写代码可以使用各种现成的库，除了Java的内建库（e.g. Math, String, Integer, List, Map）之外，还有Princeton: Standard Java Libraries 可以使用。\nReferenceUC Berkley - CS61b 2019 Spring\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Java/HD_Java%E5%85%A5%E9%97%A8.jpg","plink":"https://magi003769.github.io/post/CS61B_Java入门/"},{"title":"机器学习模型——支持向量机（2）","date":"2019-07-03T07:00:00.000Z","date_formatted":{"ll":"Jul 3, 2019","L":"07/03/2019","MM-DD":"07-03"},"updated":"2020-05-10T23:03:48.000Z","content":"上一篇关于SVM的post里面讨论了硬间隔和软间隔两种情况，而实际上SVM是可以扩展到比较极端的线性不可分情况的。本篇就接着前篇的内容，继续讨论SVM在线性不可分情况下的解决方案——引入核方法。之前关于SVM模型参数估计的方法也只有数学上的推导，这里还要介绍具体实现时要用到的SMO算法。中间的部分会穿插一些代码，解释相关步骤的实现。\n核方法正如上一篇讨论 SVM 的 post 中提到的，如果我们的数据在输入空间上是线性不可分的（如下左图所示，两个类在一个二维输入空间上成圆环分布），那么我们应该怎么应对这种情况呢？答案就是利用核技巧（kernel trick）。这个过程实际上就是通过一个非线性变化将输入空间（欧式空间 \n\\mathbb{R}^n\n\n\n\n\n\n \n \n\n）映射到一个特征空间（希尔伯特空间 \n\\mathbb{H}\n\n\n\n\n \n\n），而在这个特征空间中可以有一个超平面来利用线性支持向量机求解。\n\n\n定义：设 \n\\mathcal{X}\n\n\n\n\n \n\n 为输入空间（欧式空间），\n\\mathcal{H}\n\n\n\n\n \n\n 为特征空间（希尔伯特空间），如果存在一个从 \n\\mathbb{X}\n\n\n\n\n \n\n 到 \n\\mathbb{H}\n\n\n\n\n \n\n 的映射\n\n&gt; \\phi(x): \\mathcal{X} \\rightarrow \\mathcal{H}\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n使得对所有 \nx, z \\in \\mathcal{X}\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n ，函数 \nK(x, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 满足条件\n\n&gt; K(x, x) = \\phi(x) \\cdot \\phi(z)\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n则称 \nK(x, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 为核函数，\n\\phi(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 为映射函数，\n\\phi(x) \\cdot \\phi(z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n 为内积。\n\n从这个定义是我们就可以看出，和技巧的想法实际上只是定义了函数本身，而并没有直接显式地定义和表达映射函数 \n\\phi\n\n\n\n\n \n\n。确定这个映射关系实际上是不容易的：一来特征空间空间一般是高维的甚至可以说希尔伯特空间没有像欧式空间这样 well-define 的维度概念；二来到特征空间的映射并不是唯一确定的，而且我们解决问题甚至可以取不同的特征空间。\n核函数需要满足的条件既然确定这么一个映射函数 \n\\phi\n\n\n\n\n \n\n 并不容易，那么我们能不能直接判断一个给定的函数 \nK(x,z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是否是核函数，或者说函数 \nK(x, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 在什么情况下才能被成为核函数呢？我们通常所说的核函数就是正定核函数，其证明在《统计学习方法》7.3.2 里面有介绍，步骤包括：（1）定义映射 \n\\phi\n\n\n\n\n \n\n 并构造向量空间 \n\\mathcal{S}\n\n\n\n\n \n\n；（2）然后在 \n\\mathcal{S}\n\n\n\n\n \n\n 上定义内积构成空间；（3）最后将 \n\\mathcal{S}\n\n\n\n\n \n\n 完备构成希尔伯特空间。\n像我这种可能拿来直接用的调包选手实际上是并不关心这部分证明的，学习的时候我也是大概看了一下这部分内容，然后就直接看了正定核的充要条件或者正定核的等价定义\n\n定义：设 \n\\mathcal{X} \\subset \\mathbb{R}^n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，\nK(x, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是定义在 \n\\mathcal{X} \\times \\mathcal{X}\n\n\n\n\n\n \n \n \n\n 上的对称函数，如果对任意 \nx_i \\in \\mathcal{X}, i = 1,2,...,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n，\nK(x, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 对应的 Gram 矩阵\n\n&gt; K = [K(x_i, x_j)]_{m\\times m}\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n\n \n\n\n是半正定矩阵，则称 \nK(x,z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是正定核\n\n其实只是应用的话，很少有人关心这个条件甚至不知道这个玩意儿。大多数人还是把几个常见的核函数都试过来一遍，然后选效果最好。这也算是一个坑吧，以后更深入学习的时候可以想一想：究竟什么样的函数适用于哪些具体的问题呢？知道了kernel之后，选kernel也是个技术活，或者就类似与深度方法里的玄学调参了。最近深度上有很多用RL做architecture search的工作，想了想能不能用在这，结果发现 ICML08 就已经有了类似的工作。果然是你能想到的东西肯定都被人给做掉了2333。\n核函数的使用这里我们recall一下之前软间隔SVM的推导结果。该模型的学习最终被确定为一个关于参数 \n\\alpha\n\n\n\n\n \n\n 的限制优化问题\n\n\\min\\limits_{\\alpha} \\quad \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) - \\sum\\limits_{i=1}^{N} \\alpha_i \\\\\n\\begin{aligned}\ns.t. \\quad &amp; \\sum\\limits_{i=1}^{N} \\alpha_iy_i = 0 \\\\\n&amp; 0 \\leq \\alpha_i \\leq C\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n\n\n \n \n \n \n\n\n\n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n\n\n\n\n\n核函数 \nK(x_i, x_j)\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n 是直接来替换目标函数中 \n(x_i, x_j)\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 的。而实际上内积也是一种核函数，被称为 linear kernel，因此线性SVM实际上现在讨论的核技巧 SVM 的一种特殊情况。线性SVM的参数 \nw^\\star, b^\\star\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 表达式为\n\nw^\\star = \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i x_i \\\\\nb^\\star = y_j - \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i (x_i \\cdot x_j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n\n\n这里出现的内积部分同样使用核函数进行替代，而整个模型的函数式可以写成如下的形式。\n\n\\begin{aligned}\nf(x) &amp;= sign(w^\\star \\cdot x + b^\\star) \\\\\n&amp; = sign(\\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i x_i \\cdot x + b^\\star)\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n\n\n\n\n权重 \nw^\\star\n\n\n\n\n\n \n \n\n 与输入特征点乘后，依然会出现内积的的形式，也要用核函数进行替换。因此我们有了非线性SVM的表达式\n\nf(x) = sign(\\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i K(x, x_i) + b^\\star)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n\nSMO算法接着上面，经过前面求解对偶问题、引入核函数的一系列推倒后，我们将问题转化为了关于参数 \n\\alpha\n\n\n\n\n \n\n 的有限制优化问题。那么我们用什么方法来求解这样一组参数 \n\\alpha^\\star\n\n\n\n\n\n \n \n\n 并得到我们想要的权重 \nw^\\star\n\n\n\n\n\n \n \n\n 和 \nb^\\star\n\n\n\n\n\n \n \n\n 呢？SVM模型的学习可以形式化为求解凸二次规划问题，在众多的优化算法中，序列最小化算法（Sequential Minimal Optimization，SMO）是比较高效的一种。它的基本思路是：如果所有变量都满足KKT条件，那么这个最优化问题就得解了。\n123456789def _KKT(self, i):    ''' 7.111 - 7.113 '''    y_g = self._g(i) * self.y[i]    if self.alpha[i] == 0:       return y_g &gt;= 1    elif 0 &lt; self.alpha[i] &lt; self.C:       return y_g == 1    else:       return y_g &lt;= 1在实际操作中，每次迭代都选中两个变量，固定其他变量为常量。这两个变量中，一个是违反KKT条件最严重的，另一个由约束条件自动决定。假设我们选择的变量是 \n\\alpha_1, \\alpha_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，SMO的最优化问题就可以改写为一下的形式（忽略其他视为常数的变量）\n\n\\min\\limits_{\\alpha_1, \\alpha_2} W(\\alpha_1, \\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1\\alpha_2 - (\\alpha_1+\\alpha_2) + y_1\\alpha_1\\sum\\limits_{i=3}^{N}y_i\\alpha_iK_{i1} + y_2\\alpha_2\\sum\\limits_{i=3}^{N}y_i\\alpha_iK_{i2} \\\\\n\n\\begin{aligned}\ns.t. &amp;\\quad \\alpha_1y_1 + \\alpha_2y_2 = -\\sum\\limits_{i=3}^{N}y_i\\alpha_i = \\varsigma \\\\\n&amp;\\quad 0 \\leq \\alpha_i \\leq C, \\quad i=1,2\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n \n \n\n\n\n \n\n \n \n\n\n\n \n \n \n\n \n\n\n\n \n \n\n\n\n \n\n \n \n\n\n\n \n \n \n\n \n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n\n \n \n\n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n\n\n\n \n \n \n \n\n\n\n\n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n \n \n \n\n\n\n\n\n有上述问题的限制条件我们可以得出变量 \n\\alpha_1, \\alpha_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 的取值被限制在下图所示的box里面。另外，我们的数据集里面 \ny \\in \\{+1, -1\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n，所以这两个变量由于线性约束又被限制在一条和对角线平行的线上。两个标签 \ny_i\n\n\n\n\n\n \n \n\n 存在同号和异号两种情况，就有了下图的两种情况。\n\n在我们使用上述代码获得不符合KKT条件的变量 \n\\alpha_1\n\n\n\n\n\n \n \n\n 之后，我们就用上图所示的情况来确定 \n\\alpha_2\n\n\n\n\n\n \n \n\n 的取值范围 \nL \\leq \\alpha_2 \\leq H\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n。如果 \ny_1 = y_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，我们有边界\n\nL = max(0, \\alpha_2^{old} - \\alpha_1^{old}), \\quad H = min(C, C + \\alpha_2^{old} - \\alpha_1^{old})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n如果 \ny_1 \\neq y_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，我们有边界\n\nL = max(0, \\alpha_2^{old} + \\alpha_1^{old} - C), \\quad H = min(C, \\alpha_2^{old} + \\alpha_1^{old})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n1234567# define boundary of alpha_2if self.y[i1] == self.y[i2]:    L = max(0, self.alpha[i2] + self.alpha[i1] - self.C)    H = min(self.C, self.alpha[i2] + self.alpha[i1])else:    L = max(0, self.alpha[i2] - self.alpha[i1])    H = min(self.C, self.C + self.alpha[i2] - self.alpha[i1])下面我们就按照前面的约束来考虑 \n\\alpha_2\n\n\n\n\n\n \n \n\n 的最优解 \n\\alpha_2^{new, unc}\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n\n，再用上述的参数大小限制来剪辑得到 \n\\alpha_2^{new}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n。我们先将当前模型的预测值记为 \ng(x_i)\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n，它与真实值 \ny_i\n\n\n\n\n\n \n \n\n 的差记为 \nE_i\n\n\n\n\n\n \n \n\n，我们有\n\nE_i = g(x_i) - y_i = \\left(\\sum\\limits_{i=1}^N \\alpha_iy_iK(x_i,x)+b\\right) - y_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n\n\n\n则原来最优化问题沿约束方向未经剪辑的解为（注意这里的误差 \nE_1, E_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 都是更新前的值，直接访问 self.E对应的元素，而不需要计算）\n\n\\begin{aligned}\n\\alpha_2^{new, unc} &amp;= \\alpha_2^{old} + \\frac{y_2(E_1 - E_2)}{\\eta} \\\\\nwhere \\quad \\eta &amp;= K_{11} + K_{12} - 2K_{12}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n\n\n \n \n \n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n \n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n\n\n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n\n\n\n\n\n则经过剪辑后，\n\\alpha_2^{new}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n 的取值为\n\n\\alpha_2^{new} = \\left\\{\n\\begin{aligned}\nH, \\quad &amp; \\alpha_2^{new,unc} &gt; H \\\\\n\\alpha_2^{new,unc}, \\quad &amp; L \\leq \\alpha_2^{new,unc} \\leq H \\\\\nL, \\quad &amp; \\alpha_2^{new,unc} &lt; L \\\\\n\\end{aligned}\n\\right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n\n\n\n\n再由 \n\\alpha_2^{new}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n 通过下面的式子求得 \n\\alpha_1^{new}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n，并且更新 my_SVM 对象中使用的参数 self.alpha\n\n\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n除了更新 \n\\alpha_1\n\n\n\n\n\n \n \n\n 和 \n\\alpha_2\n\n\n\n\n\n \n \n\n，我们计算中还用到了\nE_i\n\n\n\n\n\n \n \n\n 和 \nb\n\n\n\n\n \n\n。因此，在更新过两个参数 \n\\alpha_i\n\n\n\n\n\n \n \n\n 之后，我们还需要更新误差 self.E 和 self.b 为下次的迭代做准备。我们会根据  \n\\alpha_1, \\alpha_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 的大小，计算出两个 \nb\n\n\n\n\n \n\n 的值。\n\nb^{new} = \\left\\{\n\\begin{aligned}\n&amp;b_1^{new} = -E_1 - y_1 K_{11} (\\alpha_1^{new} - \\alpha_1^{old}) - y_2 K_{21} (\\alpha_2^{new} - \\alpha_2^{old}) + b^{old} \\quad, 0 \\leq \\alpha_1^{new} &lt; C\\\\\n\\\\\n&amp;b_2^{new} = -E_2 - y_1 K_{12} (\\alpha_1^{new} - \\alpha_2^{old})  - y_2K_{22}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old} \\quad, 0 \\leq \\alpha_2^{new} &lt; C\\\\\n\\\\\n&amp;\\frac{b_1^{new}+b_2^{new}}{2} \\quad, otherwise\\\\\n\\end{aligned}\n\\right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n两个参数符合上述条件就直接使用一个新的 \nb\n\n\n\n\n \n\n，否则计算两个值的中点。在所有相关的参数更细完毕之后，我们也就结束了SMO算法的一次迭代。这部分我没有涉及到数学原理的解释，也仅仅过了一遍算法的步骤。\n简单的SVM实现这里贴一个 SVM 的简单实现，具体的效果可以看上传的repo。虽然 .ipynb 文件中显示这个从头实现的SVM可以达到0.92的准确率，但是在多次试验中，这个只很不稳定，差的时候只有0.7左右。这其中的原因值得探究，不知道什么时候才能想 sklearn 的实现一样稳如老狗。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151class my_SVM(object):        def __init__(self, max_iter=100, kernel='linear'):        self.max_iter = max_iter        self._kernel = kernel        def init_args(self, features, labels):        self.X = features        self.y = labels        self.num_samples = features.shape[0]        self.num_dims = features.shape[1]        self.b = 0.0                # Paramters used in SMO        self.alpha = np.ones(self.num_samples)        self.E = [self._E(i) for i in range(self.num_samples)]        self.C = 1.0            def _E(self, i):        ''' 7.105 '''        return self._g(i) - self.y[i]        def _g(self, i):        ''' 7.104 '''        r = self.b        for j in range(self.num_samples):            r += self.alpha[j] * self.y[j] * self.kernel_cal(self.X[i], self.X[j])        return r        def kernel_cal(self, x1, x2):        if self._kernel == 'linear':            #return sum([x1[k] * x2[k] for k in range(self.num_dims)])            return np.dot(x1, x2)        elif self._kernel == 'poly':            return (np.dot(x1, x2) + 1) ** 3        else:            raise NotImplementedError(\"This kind of kernel is not support yet!\")        def _KKT(self, i):        ''' 7.111 - 7.113 '''        y_g = self._g(i) * self.y[i]        if self.alpha[i] == 0:            return y_g &gt;= 1        elif 0 &lt; self.alpha[i] &lt; self.C:            return y_g == 1        else:            return y_g &lt;= 1                def _select_alpha(self):        index_list, non_satisfy = [], []        # indeces of alpha_i that satisfy the constrain        for i in range(self.num_samples):            if 0 &lt; self.alpha[i] &lt; self.C:                index_list.append(i)            else:                non_satisfy.append(i)        index_list.extend(non_satisfy)        for i in index_list:            if self._KKT(i):                continue            E1 = self.E[i]            if E1 &gt;= 0:                j = min(range(self.num_samples), key=lambda x: self.E[x])            else:                j = max(range(self.num_samples), key=lambda x: self.E[x])            return i, j        def _compare(self, _alpha, L, H):        ''' 7.108 '''        if _alpha &gt; H:            return H        elif _alpha &lt; L:            return L        else:            return _alpha        def fit(self, features, labels):        ''' Method to fit model with given data '''                # initialize arguments of model        self.init_args(features, labels)                # training iteration, procedures of SMO algorithm        for t in range(self.max_iter):            i1, i2 = self._select_alpha()                        # define boundary of alpha_2            if self.y[i1] == self.y[i2]:                L = max(0, self.alpha[i2] + self.alpha[i1] - self.C)                H = min(self.C, self.alpha[i2] + self.alpha[i1])            else:                L = max(0, self.alpha[i2] - self.alpha[i1])                H = min(self.C, self.C + self.alpha[i2] - self.alpha[i1])                        # select old error            E1 = self.E[i1]            E2 = self.E[i2]                        # eta = K_11 + K_22 - 2*K_12 (7.105)            eta = self.kernel_cal(self.X[i1], self.X[i1]) + self.kernel_cal(self.X[i2], self.X[i2]) - 2 * self.kernel_cal(self.X[i1], self.X[i2])            if eta &lt;= 0:                continue                        # calculate alpha_2_new            alpha2_new_unc = self.alpha[i2] + self.y[i2] * (E2 - E1) / eta # 7.106            alpha2_new = self._compare(alpha2_new_unc, L, H)                        # calculate alpha_1_new (7.109)            alpha1_new = self.alpha[i1] + self.y[i1]*self.y[i2]*(self.alpha[i2] - alpha2_new)                        # calculate b (7.115)            b1_new = - E1 - self.y[i1] * self.kernel_cal(self.X[i1], self.X[i1]) * (alpha1_new - self.alpha[i1]) \\            - self.y[i2] * self.kernel_cal(self.X[i2], self.X[i1])* (alpha2_new - self.alpha[i2]) + self.b                        # 7.116            b2_new = - E2 - self.y[i1] * self.kernel_cal(self.X[i1], self.X[i2]) * (alpha1_new - self.alpha[i1]) \\            - self.y[i2] * self.kernel_cal(self.X[i2], self.X[i2])* (alpha2_new - self.alpha[i2]) + self.b                        if 0 &lt; alpha1_new &lt; self.C:                b_new = b1_new            elif 0 &lt; alpha2_new &lt; self.C:                b_new = b2_new            else:                b_new = (b1_new + b2_new) / 2.0                        # update parameters            self.alpha[i1] = alpha1_new            self.alpha[i2] = alpha2_new            self.b = b_new            # re-calculate (update) error            self.E[i1] = self._E(i1)            self.E[i2] = self._E(i2)                        print('Traning step: &#123;&#125; / &#123;&#125; iteration'.format(t+1, self.max_iter), end='\\r')        print('\\nTraining Done !')        return        def _predict_one(self, data):        r = self.b        for i in range(self.num_samples):            r += self.alpha[i] * self.y[i] * self.kernel_cal(data, self.X[i])        return 1 if r &gt; 0 else -1        def predict(self, test_X):        res = []        for sample in test_X:            tmp = self._predict_one(sample)            res.append(tmp)        return resReferences知乎 - 浅谈正定矩阵和半正定矩阵\n知乎 - 如何理解希尔伯特空间？\n知乎 - 机器学习里的 kernel 是指什么？\n知乎 - 机器学习有很多关于核函数的说法，核函数的定义和作用是什么？\nPython · SVM（四）· SMO 算法\n机器学习算法实践-SVM中的SMO算法\nCS229  - Simplified SMO\nsklearn user guide - 1.4. Support Vector Machines\nsklaern API Docs - sklearn.svm.SVR\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_SVM_2.jpg","plink":"https://magi003769.github.io/post/支持向量机（2）/"},{"title":"拉格朗日对偶与SVM","date":"2019-06-30T07:00:00.000Z","date_formatted":{"ll":"Jun 30, 2019","L":"06/30/2019","MM-DD":"06-30"},"updated":"2020-05-10T22:56:32.000Z","content":"在SVM的推导中，我们需要在限制条件下去优化\n\\frac{1}{2}||w||^2\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n \n \n\n\n，这时就需要利用拉格朗日对偶来构造一个方便计算机求解的优化问题。这篇post就介绍一下拉格朗日对偶和kkt条件等相关的概念。\n方向导数、梯度与拉格朗日乘子法\n方向导数梯度的意义为了方便，我们将问题定义在一个二维空间中。对于一个标量函数（scalar function），\nf(x, y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n，它的梯度（gradient）是一个矢量场，定义为\n\n\\nabla f = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n\n\n可以看出函数 \nf(x,y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 的表达式固定后，我们实际上在这个二维空间内有个点都有确定的向量来表达这一点的梯度。那这个梯度具体有什么用呢？解释它的意义，我们需要借助另外一个单位向量 \n\\mathbf{u}\n\n\n\n\n \n\n。两个矢量做点乘得到一个标量，这个标量就是在该单位向量方向上的方向导数（directional derivative）\n\n(\\nabla f(\\mathbf{x})) \\cdot \\mathbf{u} = D_v f(\\mathbf{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n在单变量函数的情况下，导数用来描述函数变化的速率。那么在我们将问题上升到二维空间之后，方向导数就是用来描述在某一特定方向上函数的变化速率。我们都知道矢量点乘的结果可以表示为 \n|\\nabla f(\\mathbf{x})||\\mathbf{u}|cos\\theta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n，于是方向导数的大小数值上就是 \n|\\nabla f(\\mathbf{x})|cos\\theta\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n，也就是梯度在单位向量方向上的投影。那么当单位向量方向与梯度方向一致的时候，夹角 \n\\theta\n\n\n\n\n \n\n 为零，方向导数可以再数值上达到最大。因此，梯度方向就是函数变化速率最快的方向，并且与等高线垂直。\n拉格朗日乘子法前面介绍梯度相关的知识是为了给讲拉格朗日乘子法做铺垫。对于限制条件 \ng(x,y) = 0\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n，它的解可以组成下图的红色曲线，对于需要优化的函数 \nf(x,y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 则用蓝色的曲面表示。\n\n当这个限制条件投影到这个suface上的时候，我们就会得到一条新的曲线，它会在函数表面与不同的等高线相交。假设函数面上的等高线是和红色的曲线相交，那就意味着，接下来还会有更值更小的等高线与相交。因此，临界状态的切线就是我们这个有限制优化问题想要的点。\n\n将这个等高线投影回到 x-y 平面中去，它和 \ng(x,y) = 0\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n 的解组成的曲线也是相切的，就有了最开始的那一幅图。由此我们看出，我们想要的这个点实际上满足这样的关系：\nf(x,y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 的梯度和 \ng(x,y)=c\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n 曲线的法线实现性相关的。由此我们边构造了一个新的目标函数\n\n\\mathcal{L}(x,y,\\lambda) = f(x,y) - \\lambda (g(x,y) - c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n令其偏导等于0，就可以求出对应的解，这就是拉格朗日乘子法。\n拉格朗日对偶性与KKT条件原始问题假设 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n，\nc(x)\n\n\n\n\n\n\n\n \n \n \n \n\n，\nh(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 是定义在 \n\\mathbb{R}^n\n\n\n\n\n\n \n \n\n 上的连续可微函数。我们有如下一个约束优化问题\n\n\\max\\limits_{x\\in \\mathbb{R}^n} \\  f(x) \\\\\ns.t. \\quad c_i(x) \\leq 0, \\quad i = 1,2,...,k \\\\\n\\quad h_j(x) = 0, \\quad j = 1,2,...l\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n称这个约束最优化问题为原始问题。首先我们一如拉格朗日乘子法，得到广义拉格朗日函数\n\n\\mathcal{L}(x,\\alpha,\\beta) = f(x) + \\sum\\limits_{i=1}^{k} \\alpha_i c_i(x) + \\sum\\limits_{j=1}^{k} \\beta_i h_j(x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n \n\n\n这里 \nx\n\n\n\n\n \n\n 表示一个 n 维腾正空间中的一个向量，\n\\alpha_i,\\ \\beta_j\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 为拉格朗日乘子，\n\\alpha_i \\geq 0\n\n\n\n\n\n\n\n \n \n \n \n\n。我们现在来考虑这个刚一拉格朗日函数的最大值\n\n\\theta_P(x) = \\max\\limits_{\\alpha, \\beta: \\alpha \\geq 0} \\mathcal{L}(x,\\alpha,\\beta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n由于我们引入了两个乘子，如果某些 \nx\n\n\n\n\n \n\n 违反原始院问题的约束条件，我们就可以通过改变这两个乘子使得 \n\\theta p\n\n\n\n\n\n \n \n\n(x) 的值为正无穷，而其余乘子的取值均为0。相反的，如果 \nx\n\n\n\n\n \n\n 均满足约束条件，那么我们就有 \n\\theta_P(x) = f(x)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n，即在限制条件下最小化化目标函数 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 可以转化为最小化该广义拉格朗日函数的最大值 \n\\theta_P(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n。\n\n\\min\\limits_{x} \\theta_P(x) = \\min\\limits_{x} \\max\\limits_{\\alpha, \\beta: \\alpha \\geq 0} \\mathcal{L}(x,\\alpha,\\beta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n \n \n\n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n该问题被称为拉格朗日的极小极大值问题。这样的等价转化就会使得求解约束优化问题变得方便，这里我们定义原始优化问题的最优值为\n\np^\\star = \\min\\limits_{x} \\theta_P(x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n\n\n \n \n\n \n \n \n\n\n对偶问题那么这个问题的对偶又是什么呢？直观上看就是优化顺序的调整：先极小再极大。我们假设上面广义拉格朗日函数的最小值为\n\n\\theta_D(\\alpha, \\beta) = \\min\\limits_{x} \\mathcal{L}(x,\\alpha,\\beta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n \n \n \n \n \n \n\n\n再考虑对 \n\\theta_D(\\alpha, \\beta)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 求最大值，即\n\n\\max\\limits_{\\alpha, \\beta} \\theta_D(\\alpha, \\beta) = \\max\\limits_{\\alpha, \\beta} \\min\\limits_{x} \\mathcal{L}(x,\\alpha,\\beta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n \n \n\n \n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n\n \n \n \n \n\n \n \n \n \n \n \n \n \n\n\n该问题被称为广义拉格朗日函数的极大极小值问题。它实际上是有一个限制条件的：\n\\alpha_i \\geq 0\n\n\n\n\n\n\n\n \n \n \n \n\n。这就是原始问题的对偶问题，我们记它的最优值为\n\nd^\\star = \\max\\limits_{\\alpha, \\beta} \\theta_D(\\alpha, \\beta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n \n \n\n \n \n \n \n \n\n\n这里注意原始问题和对偶问题的下标，我们的原始问题是求解一个合适的标量/向量 \nx\n\n\n\n\n \n\n，而对偶问题则是找到使得问题最优的参数 \n\\alpha\n\n\n\n\n \n\n 和 \n\\beta\n\n\n\n\n \n\n。转换这个解空间也是我们进行对偶转化的一个意义所在。那我么我们求出了这个参数，又怎么转回求解原来的变量 \nx\n\n\n\n\n \n\n 的取值呢？这就需要下面的内容了：原始问题和对偶问题的关系以及 KKT 条件。\n原始问题与对偶问题的关系这一节就直接以三个定理的形式来阐述两者之间的关系\n\n定理 1. 若原始问题和对偶问题都有最优解，则\n\n&gt; d^\\star = \\max\\limits_{\\alpha, \\beta} \\min\\limits_{x} \\mathcal{L}(x,\\alpha,\\beta) \\leq \\min\\limits_{x} \\max\\limits_{\\alpha, \\beta: \\alpha \\geq 0} \\mathcal{L}(x,\\alpha,\\beta) = p^\\star\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n\n\n \n \n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n\n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n\n\n在这个基础上，我们又有了一个推论\n\n推论 1. 设 \nx^\\star\n\n\n\n\n\n \n \n\n 和 \n\\alpha^\\star\n\n\n\n\n\n \n \n\n，\n\\beta^\\star\n\n\n\n\n\n \n \n\n 分别是原始问题核对偶问题的可行解，并且 \nd^\\star = p^\\star\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则\nx^\\star\n\n\n\n\n\n \n \n\n 和 \n\\alpha^\\star\n\n\n\n\n\n \n \n\n，\n\\beta^\\star\n\n\n\n\n\n \n \n\n 分别是原始问题核对偶问题的最优解\n\n在某些条件下，原始问题核对欧条件的最优值相等，\nd^\\star=p^\\star\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n。这时就可以用解决对偶问题来等价的解决原问题。\n\n定理 2. 假设函数 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 和 \nc_i(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 均为凸函数，\nh_j(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 是仿射函数；并且假设不等式约束 \nc_i(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 是严格可行的；则存在 \nx^\\star,\\ \\alpha^\\star,\\ \\beta^\\star\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n，使 \nx^\\star\n\n\n\n\n\n \n \n\n 是原始问题的解，\n\\alpha^\\star,\\beta^\\star\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n是对偶问题的解为\n\n&gt; d^\\star = p^\\star =\\mathcal{L}(x^\\star,\\alpha^\\star,\\beta^\\star)\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n而上面这种情况成立的充要条件就是我们所说Karush-Kuhn-Tucker(KKT)条件:\n\n\\nabla_x \\mathcal{L}(x^\\star,\\alpha^\\star,\\beta^\\star) = 0 \\\\\n\\alpha^\\star_i c_i(x^\\star) = 0, \\quad i = 1,2,...,k \\\\\nc_i(x^\\star) \\leq 0, \\quad i = 1,2,...,k \\\\\n\\alpha_i^\\star \\geq 0, \\quad i=1,2,...,k \\\\\nh_j(x\\star) = 0, \\quad j = 1,2,...,l\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n拉格朗日对偶在SVM中的应用（推导）线性可分SVM or 硬间隔回想一下我们SVM中的模型训练及间隔最大化，思路是最大化几何间隔 \n\\gamma\n\n\n\n\n \n\n。再考虑到几何间隔与函数间隔的关系是\n\\hat{\\gamma} = ||w||\\gamma\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n，且无论函数间隔取何值总有成比例缩放的 \nw\n\n\n\n\n \n\n 与之对应。所以，为了方便我们这里就使得函数间隔 \n\\hat{\\gamma} = 1\n\n\n\n\n\n\n\n \n \n \n \n\n，来求解 \n\\frac{1}{||w||}\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n\n 的最大值。最终，经过等价，问题可以被转化成了对目标函数\n\\frac{1}{2}||w||^2\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n \n \n\n\n的有限制优化\n\n\\min \\limits_{w,b} \\frac{1}{2}||w||^2 \\\\\ns.t. \\quad y_i(w\\cdot x_i+b) - 1 \\geq 0, \\quad i = 1,2,...,N\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n利用拉格朗日乘子法，我们得到了新的拉格朗日函数（中间将不等式取反，与前一节中的形式保持一致）\n\n\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum\\limits_{i=1}^{N} \\alpha_i y_i(w\\cdot x_i+b) + \\sum\\limits_{i=1}^{N} \\alpha_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n根据拉格朗日对偶性，原始问题的对偶问题是极大极小值问题\n\n\\max\\limits_\\alpha\\ \\min\\limits_{w,b} \\mathcal{L}(w,b,\\alpha)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n如我们之前所说，有限制优化的极值点会出现在相切的位置。放在了格朗日函数中就是鞍点，即梯度为0的位置。于是对求 \n\\mathcal{L}(x,w,b)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n 对于 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 的偏导，我们就有\n\n\\nabla_w \\mathcal{L}(w,b,\\alpha) = w - \\sum\\limits_{i=1}^{N}\\alpha_i y_i x_i = 0 \\Rightarrow w = \\sum\\limits_{i=1}^{N}\\alpha_i y_i x_i \\\\\n\\nabla_b \\mathcal{L}(w,b,\\alpha) = -\\sum\\limits \\alpha_iy_i = 0 \\Rightarrow \\sum\\limits_{i=1}{N} \\alpha_iy_i = 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n \n\n \n \n \n\n\n \n\n \n \n\n\n \n \n\n \n \n\n\n\n将这两个式子带回 \n\\mathcal{L}(x,w,b)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n，我们可以得到该条件下的函数值，即为最小值\n\n\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) - \\sum\\limits_{i=1}^{N}\\alpha_i y_i \\left( \\left( \\sum\\limits_{i=1}^{N}\\alpha_j y_j x_j \\right) \\cdot x_i + b \\right) + \\sum\\limits_{i=1}^{N} \\alpha_i \\\\\n\\min\\limits_{w,b} \\mathcal{L}(w,b,\\alpha) = - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) + \\sum\\limits_{i=1}^{N} \\alpha_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n\n \n \n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n\n求 \n\\min\\limits_{w,b} \\mathcal{L}(x,w,b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n 对 \n\\alpha\n\n\n\n\n \n\n 的最大值，即对偶问题\n\n\\max\\limits_{\\alpha} \\quad - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) + \\sum\\limits_{i=1}^{N} \\alpha_i \\\\\ns.t. \\sum\\limits_{i=1}^{N} \\alpha_iy_i = 0 \\\\\n\\alpha_i \\geq 0, \\quad i = 1,2,...,N\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n对上面的目标函数取反，可以转化成最小值的问题，这里就不重复写式子了。注意对偶问题的下标：原始问题是求解目标函数关于 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 的最大值，而对偶问题是我们如何求解参数 \n\\alpha\n\n\n\n\n \n\n 的最优化取值。原始问题满足前一节定理2,的条件，所以我们求解对偶问题的解就和求见原始问题变成了等价的。我们有下面这个定理\n\n定理 4. 设 \n\\alpha^\\star  =  (\\alpha_1^\\star, \\alpha_2^\\star, ... , \\alpha_N^\\star)^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n\n \n \n\n\n 是对偶问题的解，则存在下标 \nj\n\n\n\n\n \n\n，使得 \n\\alpha_j^\\star &gt; 0\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，并求得 \nw^\\star\n\n\n\n\n\n \n \n\n 、\nb^\\star\n\n\n\n\n\n \n \n\n：\n\n&gt; w^\\star = \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i x_i \\\\\n&gt; b^\\star = y_j - \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i (x_i \\cdot x_j)\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n利用下面的KKT条件，我们可以证明这个定理\n\n\\nabla_w \\mathcal{L}(w,b,\\alpha) = w^\\star - \\sum\\limits_{i=1}^{N} \\alpha^\\star_iy_ix_i = 0 \\\\\n\\nabla_b \\mathcal{L}(w,b,\\alpha) = - \\sum\\limits_{i=1}^{N}\\alpha_i^\\star y_i = 0 \\\\\n\\alpha^\\star_i(y_i(w^\\star \\cdot x_i + b^\\star) - 1) = 0 \\\\\ny_i(w^\\star \\cdot x_i + b^\\star) - 1 \\geq 0 \\\\\n\\alpha^\\star \\geq 0, \\quad i = 1,2,...,N\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n通过KKT条件1，我们可以直接得到 \nw^*\n\n\n\n\n\n \n \n\n 的解。至少有一个 \n\\alpha_j &gt; 0\n\n\n\n\n\n\n\n \n \n \n \n\n，对于此 \n\\alpha_j\n\n\n\n\n\n \n \n\n 有\n\ny_j (w^\\star \\cdot x_j + b^\\star) - 1 = 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n\n将 \nw^\\star\n\n\n\n\n\n \n \n\n 的表达式代入后就可以得到 \nb^\\star\n\n\n\n\n\n \n \n\n 的表达式。从而得到超平面的表达式。\n\n线性SVM or 软间隔如果我们的数据集实际上是线性不可分的，我们应该如何将线性可分SVM扩展到可以解决该问题的样子呢？在很多情况下，在剔除了数据集中的一部分outlier之后，剩下大部分数据是线性可分的，这就是所说的近似线性可分。在这种情况下，线性SVM的推导中引入了一个松弛变量 \n\\xi_i\n\n\n\n\n\n \n \n\n，则原有的不等式约束转化为\n\ny_i(w\\cdot x_i + b) \\geq 1 - \\xi_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n原先优化问题中的目标函数也要因此改变，需要对每个松弛函数支付一定的代价。所以新的目标函数和新的二次规划问题就是\n\n\\min\\limits_{w,b,\\xi} \\quad \\frac{1}{2}||w||^2 + C\\sum\\limits_{i=1}^{N}\\xi_i \\\\\ns.t. \\quad y_i(w\\cdot x_i + b) \\geq 1 - \\xi_i, \\quad i=1,2,...,N \\\\\n\\xi_i \\geq 0, \\quad i=1,2,...,N\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n其中 \nC&gt;0\n\n\n\n\n\n\n \n \n \n\n 被称为惩罚参数，一般是根据具体问题设定的一个值，\nC\n\n\n\n\n \n\n 的制约的惩罚越大，惩罚项的目的是在优化目标函数时尽可能事误分类最少。 这个原始最优化对应的拉格朗日函数是\n\n\\mathcal{L}(w,b,\\xi,\\alpha,\\mu) = \\frac{1}{2}||w||^2 + C\\sum\\limits_{i=1}^{N}\\xi_i -\\sum\\limits_{i=1}^{N}\\alpha_i(y_i(w\\cdot x_i+b)-1+\\xi_i) - \\sum\\limits_{i=1}^{N}\\mu_i\\xi_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n\n我们的对偶问题即为极大极小问题：\n\\max\\limits_{\\alpha, \\mu} \\min\\limits_{w,b,\\xi} \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n \n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n。在先解决极小部分时，通过求鞍点，即偏导的等于0，我们有\n\n\\nabla_w \\mathcal{L}(w,b,\\xi,\\alpha,\\mu) = w - \\sum\\limits_{i=1}^{N}\\alpha_iy_ix_i = 0 \\\\\n\\nabla_b \\mathcal{L}(w,b,\\xi,\\alpha,\\mu) = - \\sum\\limits_{i=1}^{N}\\alpha_iy_i = 0\\\\\n\\nabla_{\\xi_i} \\mathcal{L}(w,b,\\xi,\\alpha,\\mu) = C - \\alpha_i - \\mu_i = 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n将以上三个结果带回到原式，我们可以得出对偶问题\n\n\\max\\limits_{\\alpha} \\quad - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) + \\sum\\limits_{i=1}^{N} \\alpha_i \\\\\n\\begin{aligned}\ns.t. \\quad &amp; \\sum\\limits_{i=1}^{N} \\alpha_iy_i = 0 \\\\\n&amp; C - \\alpha_i - \\mu_i = 0 \\\\\n&amp; \\alpha_i \\geq 0 \\\\\n&amp; \\mu_i \\geq 0, \\quad i = 1,2,...,N\n&amp; \\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n\n\n \n \n \n \n\n\n\n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n利用限制条件的第二条等式，我们可以消去 \n\\mu_i\n\n\n\n\n\n \n \n\n 只保留 \n\\alpha_i\n\n\n\n\n\n \n \n\n，后面三条约束条件可以写成\n\n0 \\leq \\alpha_i \\leq C\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n\n到这里我们其实可以看出，软间隔情况下的SVM和之前硬间隔情况下得对偶问题在数学表达上其实是差不多的，不同的是约束条件。它的解依然可以用下面的式子得到\n\nw^\\star = \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i x_i \\\\\nb^\\star = y_j - \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i (x_i \\cdot x_j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n\n\n这里下标为 \nj\n\n\n\n\n \n\n 的样本满足 \n0 \\leq \\alpha^\\star_j \\leq C\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n 的条件。\n\nReference《统计学习方法》附录C\n简书 - 一文理解拉格朗日对偶和KKT条件\n知乎 - 如何理解拉格朗日乘子法\n知乎 - 如何通俗地讲解对偶问题？尤其是拉格朗日对偶lagrangian duality\n知乎 - 线性空间的对偶空间和优化里的拉格朗日对偶有什么关系？\nSciPy Lectures - Mathematical optimization: finding minima of functions\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Mathematics/HD_lagrange_duality.png","plink":"https://magi003769.github.io/post/拉格朗日对偶/"},{"title":"机器学习模型——支持向量机（1）","date":"2019-06-25T07:00:00.000Z","date_formatted":{"ll":"Jun 25, 2019","L":"06/25/2019","MM-DD":"06-25"},"updated":"2020-05-10T23:01:26.000Z","content":"本篇是介绍支持向量机（SVM）的相关知识的其中一篇。因为SVM的相关内容很多，涵盖支持向量机的种类、软硬间隔以及核方法等相关概念，这一篇先讲解线性SVM以及软硬间隔的内容，主要是《统计学习方法》的搬运和一点个人理解。核方法和SMO算法会在另一篇中涉及。\n支持向量机的定义和分类\n支持向量机（support vector machine，SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。\n\n从上面的定义可以看出，SVM表面上还是一个通过函数 \nf(x)=wx + b\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n 来进行二分类的模型。能够将其与感知机区分开来的是间隔。感知机利用误差最小策略，求的分离超平面，不过这时的解有无穷多个。SVM（针对前面感知机的比较，这里一般说的是线性可分SVM）使用一定的策略，可以使得间隔最大化求出最优的超平面。\n\nModel / NameCaseMargin MaximizationKernel\n\n线性可分SVM数据集线性可分Hard margin:heavy_multiplication_x:\n\n线性 / 软间隔SVM数据集近似线性可分Soft margin:heavy_multiplication_x:\n\n非线性SVM数据集线性不可分Soft margin:heavy_check_mark:\n线性可分SVM与硬间隔假定我们的数据扔在一个而位的特征空间上，\nT=\\{(x_1, y_1), (x_2, y_2), ... (x_N, y_N)\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n。其中特征向量\nx_i \\in \\mathcal{X}=\\mathbb{R}^2\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n，标签\ny_i \\in \\{+1, -1\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n。我们的目标是找的一个可以划分数据的超平面\n\nw^*\\cdot x + b = 0\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n\n一般的，特征空间中的任意一点到超平面的距离为 \n|w\\cdot x+b|\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n，而 \nw\\cdot x + b\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的符号（signed distance）则表示分类的结果，其与标签\ny\n\n\n\n\n \n\n的乘积能够度量分类是否正确。因次我们可以用\n\n\\hat{\\gamma_i} = y_i(w\\cdot x_i + b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n\n来作为分类正确性与确信度的判断标准，也被称为函数间隔。而超平面 \n(w,b)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 关于数据集 \nT\n\n\n\n\n \n\n 的函数间隔即为所有样本函数间隔的最小值\n\n\\hat{\\gamma} = \\min\\limits_{i=1,2,...,N} \\hat{\\gamma_i}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n \n\n\n\n为了消除参数\nw, b\n\n\n\n\n\n\n \n \n \n\n成比例缩放带来的影响，又引入了几何间隔的概念，即函数距离除以权重的二范数\n||w||\n\n\n\n\n\n \n \n \n \n \n\n\n\n\\gamma_i = y_i(\\frac{w}{||w||}\\cdot x_i + \\frac{b}{||w||})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n\n\n \n\n \n \n \n \n \n\n\n\n \n\n \n \n\n \n\n\n\n \n\n \n \n \n \n \n\n\n\n \n\n\n这个就类似于我们非常常用的点到直线距离公式：点 \n(x_0, y_0)\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 到直线 \nAx+By+C=0\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n的距离为\n\\frac{|Ax_0 + By_0 + C|}{\\sqrt{A^2 + B^2}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n\n \n\n\n \n \n \n\n \n \n\n\n\n\n\n，只不过我们保留了结算结果的符号来表示分类结果。那么我们接下来的问题就是如何使间隔最大化，注意我们讨论的是如何将数据集 \nT\n\n\n\n\n \n\n 的间隔最大化，即最大化这个最小值。问题可以用下面的式子描述\n\n\\max\\limits_{w,b} \\quad \\frac{\\hat{\\gamma}}{||w||} \\\\\ns.t. \\quad y_i(w \\cdot x_i + b) \\geq \\hat{\\gamma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n\n\n\n我们理论上哪个是要用几何间隔才能唯一确定我们想要的超平面的，但我们这里使用函数间隔的取值是不影响整个优化问题的。我们总有一组等比例缩放的 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 与函数距离对应，因此为了方便计算，就把函数间隔 \n\\hat{\\gamma}\n\n\n\n\n\n \n \n\n 的值直接设为1。然后可以等价为优化问题\n\n\\min\\limits_{w,b} \\quad \\frac{1}{2} ||w||^2\\\\\ns.t. \\quad y_i(w \\cdot x_i + b) - 1\\geq 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n\n这样我们最后的模型可以通过解决这个最优化问题得到，解的存在性和唯一性就不在这里展开讲了，具体参考李航老师的蓝皮书。为了求得最优化问题的解 \nw^\\star\n\n\n\n\n\n \n \n\n 和 \nb^\\star\n\n\n\n\n\n \n \n\n，我们引入拉格朗日乘子 \n\\alpha^\\star  =  (\\alpha_1^\\star, \\alpha_2^\\star, ... , \\alpha_N^\\star)^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n\n \n \n\n\n，并使用拉格朗日对偶将问题转化成求 \n\\alpha^\\star\n\n\n\n\n\n \n \n\n。（具体推导可以参考我另一篇专门讲拉格朗日对偶的post）\n\n设 \n\\alpha^\\star  =  (\\alpha_1^\\star, \\alpha_2^\\star, ... , \\alpha_N^\\star)^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n\n \n \n\n\n 是对偶问题的解，则存在下标 \nj\n\n\n\n\n \n\n，使得 \n\\alpha_j^\\star &gt; 0\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，并求得 \nw^\\star\n\n\n\n\n\n \n \n\n 、\nb^\\star\n\n\n\n\n\n \n \n\n：\n\n&gt; w^\\star = \\sum\\limits_{i=1}{N} \\alpha_i^\\star y_i x_i \\\\\n&gt; b^\\star = y_j - \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i (x_i \\cdot x_j)\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n \n\n\n \n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n在这个定理里面我们看到存在一个特殊的样本点 \n(x_j, y_j)\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 使得 \n\\alpha^\\star_j &gt; 0\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n。这样根据KKT条件 \n\\alpha^\\star(y_j(w^\\star\\cdot x_j + b) - 1) = 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n\n 可知（同上，详见另一篇关于对偶的post）\n\ny_j(w^\\star\\cdot x_j + b) - 1 = 0 \\\\\n\\mathbf{or} \\quad w^\\star\\cdot x_j + b = \\pm 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n\n\n这个样本点就被称为支持向量。下图就可视化了一个线性可分SVM，临界的两个样本点就是支持向量，对应的拉格朗日乘子数值大于零，间隔平面两侧的样本对应的 \n\\alpha_i\n\n\n\n\n\n \n \n\n 则都是等于0的。这里因为是可以用我们学习到的百分之百分离的，且所以被称为硬间隔。\n\n线性SVM与软间隔如果我们的数据集实际上是线性不可分的，我们应该如何将线性可分SVM扩展到可以解决该问题的样子呢？在很多情况下，在剔除了数据集中的一部分outlier之后，剩下大部分数据是线性可分的，这就是所说的近似线性可分。在这种情况下，线性SVM的推导中引入了一个松弛变量 \n\\xi_i\n\n\n\n\n\n \n \n\n，则原有的不等式约束转化为\n\ny_i(w\\cdot x_i + b) \\geq 1 - \\xi_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n原先优化问题中的目标函数也要因此改变，需要对每个松弛函数支付一定的代价。所以新的目标函数和新的二次规划问题就是\n\n\\min\\limits_{w,b,\\xi} \\quad \\frac{1}{2}||w||^2 + C\\sum\\limits_{i=1}^{N}\\xi_i \\\\\ns.t. \\quad y_i(w\\cdot x_i + b) \\geq 1 - \\xi_i, \\quad i=1,2,...,N \\\\\n\\xi_i \\geq 0, \\quad i=1,2,...,N\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n其中 \nC&gt;0\n\n\n\n\n\n\n \n \n \n\n 被称为惩罚参数，一般是根据具体问题设定的一个值，\nC\n\n\n\n\n \n\n 的制约的惩罚越大，惩罚项的目的是在优化目标函数时尽可能事误分类最少。 这个原始最优化对应的拉格朗日函数是\n\n\\mathcal{L}(w,b,\\xi,\\alpha,\\mu) = \\frac{1}{2}||w||^2 + C\\sum\\limits_{i=1}^{N}\\xi_i -\\sum\\limits_{i=1}^{N}\\alpha_i(y_i(w\\cdot x_i+b)-1+\\xi_i) - \\sum\\limits_{i=1}^{N}\\mu_i\\xi_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n\n使用求偏导为0等一系列操作之后（同上，详见另一篇关于对偶的post），我们可以得到对偶问题\n\n\\max\\limits_{\\alpha} \\quad - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_j y_iy_j (x_i \\cdot x_j) + \\sum\\limits_{i=1}^{N} \\alpha_i \\\\\n\\begin{aligned}\ns.t. \\quad &amp; \\sum\\limits_{i=1}^{N} \\alpha_iy_i = 0 \\\\\n&amp; C - \\alpha_i - \\mu_i = 0 \\\\\n&amp; \\alpha_i \\geq 0 \\\\\n&amp; \\mu_i \\geq 0, \\quad i = 1,2,...,N\n&amp; \\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n\n\n \n \n \n \n\n\n\n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n利用限制条件的第二条等式，我们可以消去 \n\\mu_i\n\n\n\n\n\n \n \n\n 只保留 \n\\alpha_i\n\n\n\n\n\n \n \n\n，后面三条约束条件可以写成\n\n0 \\leq \\alpha_i \\leq C\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n\n到这里我们其实可以看出，软间隔情况下的SVM和之前硬间隔情况下得对偶问题在数学表达上其实是差不多的，不同的是约束条件。它的解依然可以用下面的式子得到\n\nw^\\star = \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i x_i \\\\\nb^\\star = y_j - \\sum\\limits_{i=1}^{N} \\alpha_i^\\star y_i (x_i \\cdot x_j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n\n\n这里下标为 \nj\n\n\n\n\n \n\n 的样本满足 \n0 \\leq \\alpha^\\star_j \\leq C\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n 的条件。从前面硬间隔的情况来看，我们的支持向量是定义为 \n\\alpha_j&gt;0\n\n\n\n\n\n\n\n \n \n \n \n\n 的样本。而软间隔的支持向量也是相同的，但是因为不能完全线性可分，所以是不太确定的：软间隔支持向量或者落在间隔边界上，或者再间隔边界与分离超平面之间，或者在分离超平面的误分类一侧。远离间隔边界且被正确分类的样本点，对应的参数 \n\\alpha_i = 0\n\n\n\n\n\n\n\n \n \n \n \n\n；在间隔边界上的样本点，对应的参数 \n0 &lt; \\alpha_i &lt; C\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n；而对于在间隔平面之间的样本点（包含正确分类的和误分类，两者的区别在松弛系数 \n\\xi_i\n\n\n\n\n\n \n \n\n 的大小），对应的参数 \n\\alpha_i = C\n\n\n\n\n\n\n\n \n \n \n \n\n。\n\nReference《统计学习方法》第7章：支持向量机\n\n知乎 - SVM教程：支持向量机的直观理解\n\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_SVM_1.jpg","plink":"https://magi003769.github.io/post/支持向量机（1）/"},{"title":"平衡二叉树","date":"2019-05-30T07:00:00.000Z","date_formatted":{"ll":"May 30, 2019","L":"05/30/2019","MM-DD":"05-30"},"updated":"2019-06-21T06:15:26.000Z","content":"在之前关于BST的讨论中，我们提到了简单的插入逻辑在某些特定条件下会使得BST从树结构退化成类似链表的线性结构，因此会导致搜索的复杂度从\nO(logn)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n上升为\nO(n)\n\n\n\n\n\n\n\n \n \n \n \n\n。为了防止这种情况出现，便有了平衡树概念的提出。\nAVL Tree\nAVL由俄罗斯科学家G.M.Adelson-Velsky E.M.Landis在1962年的论文首次提出，是最早的自平衡的自平衡二叉树\n\nAVL中的平衡二叉树并不像完全二叉树那样必须将一层铺满才能在下一层插入节点，它的定义相对宽松：左子树和右子树深度差不大于1。这样可以尽可能保持树的结构，为搜索带来便利。为了实现对高度的判别，除了节点的val和左右子节点的引用，我们需要给节点引入一个新的属性height。\n123456class __Node(object):    def __init__(self, val):        self.val = val        self.left = None        self.right = None        self.height = 1平衡因子那么如何判断一个节点代表的子树是否平衡呢？根据定义我们只需要对每个节点获取其左右子树的height，再用左子树高度减去减去右子树的高度。这样，平衡节点的平衡因子在\n[-1, 1]\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n的区间上，在这个区间范围外即可视为不平衡，需要一系列的操作来将其变为平衡的。这种的操作就是树的旋转。\n1234567891011def getHeight(self, root):    \"\"\" Returns height of the (sub)tree represented by root \"\"\"    if not root:        return 0    return root.height    def getBalance(self, root):    \"\"\" Get the balance factor of the (sub)tree represented by root \"\"\"    if not root:        return 0    return self.getHeight(root.left) - self.getHeight(root.right)树的旋转旋转操作在想树里面插入节点的过程中，我们会检测节点是否平衡。如果发现不平衡，则记首个不平衡点为z、该节点到插入新节点路径上的子节点为y、y在路径上的子节点为x。如下面的两幅图所示，我们有两种旋转方向：左旋转与右旋转。一般来说我们把第一个不平衡点z称为root，并且以y为pivot进行旋转。\n\n我们要将pivot新的root返回，原来的根节点 z作为y的左子树。从BST的性质中我们可以得出，T2所代表的子树位于节点z的右侧，所以T2中任意节点的值都比z大，所以在旋转中可以将T2直接转成z的右子树。这样向左旋转后的树结构就变得平衡了。需要注意的是，我们需要在旋转之后维护一下节点的高度self.height。\n1234567891011121314def leftRotate(self, z):    &quot;&quot;&quot; Implement left rotate operation on subtree z&quot;&quot;&quot;    y &#x3D; z.right    T2 &#x3D; y.left            # Perform rotation    y.left &#x3D; z    z.right &#x3D; T2            # Update height    z.height &#x3D; 1 + max(self.getHeight(z.left), self.getHeight(z.right))    y.height &#x3D; 1 + max(self.getHeight(y.left), self.getHeight(y.right))            return y因为在这个变化中我们只变动了z和y的位置，他们两个所代表的子树高度发生了改变，所以只需要将这两个节点的height属性进行一下维护。注意一下这里的顺序问题：要先维护z的高度，再去维护y的高度。在旋转之后，z变成了y的子节点，而我们在获取高度的时候实际上是直接调用这个属性的，如果我们先维护了y的高度，而没有去更新z的高度变化，那么得到的结果将会是出现错误。\n\n与左旋转相类似，右旋转也是将pivot的一个子树转移给原来的root，并将就得根节点作为pivot的一个子节点，再返回pivot作为新的根节点。\n1234567891011121314def rightRotate(self, z):    \"\"\" Implement right rotate operation on subtree z \"\"\"    y = z.left    T3 = y.right            # Perform rotation    y.right = z    z.left = T3            # Update height    z.height = 1 + max(self.getHeight(z.left), self.getHeight(z.right))    y.height = 1 + max(self.getHeight(y.left), self.getHeight(y.right))            return y旋转的不同情况Left-left and Right-right case这两种情况就如之前解释树的旋转操作时候所示，我们所考察的节点z、y、x全部分布在一侧。直接对不平衡节点进行一次left rotation或者right rotation即可。\nLeft-right case在该情况下，插入节点到第一个不平衡点的路径是有拐点的不能像上面两种情况通过一次树旋转来平衡。首先以节点y为根节点、节点x为轴，对这个子树进行左旋转使其变成所需节点全在一侧。然后以节点z为根节点再进行一次右旋转即可。\n\nRight-left case与上面的Left-right Case一样，进行两次旋转操作即可。\n\nAVL Tree的基本操作基于上面的分析，和之前一篇关于BST的Post，在这里简单梳理一下AVL Tree的三个基本操作的实现：插入、删减、搜索。搜索和BST基本一样就不在这里赘述了，基于比较大小决定查找方向的规则。变得麻烦的是插入和删减，因为要在这个过程中始终保持树的平衡。\n节点插入依然是一个递归的实现，需要注意的是判断当前子树形态的条件。GeeksforGeeks博客中的实现是判断插入值和root的某个子节点值的大小关系来判断。但很多时候会出现根节点只有一侧有子节点的情况，这样就会报错。所以我们在生成AVL Tree的时候使用root子节点的平衡因子来判断。\n12345678910111213141516171819202122232425262728293031323334def insert(self, root, val):    # Perform common BST insertion    if not root:        return self.__Node(val)    if val &lt; root.val:        root.left = self.insert(root.left, val)    else:        root.right = self.insert(root.right, val)            # Update Height    root.height = 1 + max(self.getHeight(root.left), self.getHeight(root.right))            # Get the balance factor and rebalance the tree    balance = self.getBalance(root)            # Left-left case    if balance &gt; 1 and self.getBalance(root.left) &gt;= 0:        return self.rightRotate(root)            # Right-right case    if balance &lt; -1 and self.getBalance(root.right) &lt;= 0:        return self.leftRotate(root)    # Left-right case    if balance &gt; 1 and self.getBalance(root.right) &lt;= 0:        root.left = self.leftRotate(root.left)        return self.rightRotate(root)            # Right-left case    if balance &lt; -1 and self.getBalance(root.left) &gt;= 0:        root.right = self.rightRotate(root.right)        return self.leftRotate(root)        return root节点删除先来回顾一下普通的BST是如何删除节点的：\n\n如果该节点没有子节点，直接删除\n\n如果该节点有一个子节点，用该子节点替换被删除的节点\n\n如果该节点有两个子节点，在其子树中寻找其在in-order排序下的前一个或后一个元素，进行替换并将找到的这个节点删除\n\n\n寻找节点的successor或者predecessor可以这样：在左子树中获取最大值，或者在右子树中获取最小值。一开始我们使用一般的BST逻辑进行节点删除。在这个实现中，如果找到需要删除的节点，在任意子节点缺失的情况下都可以直接返回另一子节点：具体的就是将root指向这一子节点。如果左右都存在子节点，就找它的predecessor来替代，并将predecessor删除（这个predecessor一般是叶节点，没有任何子节点，按照递归会直接返回None，不需要维护节点的height属性）。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def delete(self, root, val):    if not root:        return root    elif val &lt; root.val:        root.left = self.delete(root.left, val)    elif val &gt; root.val:        root.right = self.delete(root.right, val)    else:        if not root.left:            root = root.right        elif not root.right:            root = root.left        else:            predc = root.left            while predc.right:                predc = predc.right            root.val = predc.val            root.left = self.delete(root.left, predc.val)        # If the tree has only one node, no need to check height    if root is None:        return root        # Update height of node    root.height = 1 + max(self.getHeight(root.left), self.getHeight(root.right))            # Get balance factor    balance = self.getBalance(root)        # Left-left case    if balance &gt; 1 and self.getBalance(root.left) &gt;= 0:        return self.rightRotate(root)            # Right-right case    if balance &lt; -1 and self.getBalance(root.right) &lt;= 0:        return self.leftRotate(root)            # Left-right case    if balance &gt; 1 and self.getBalance(root.right) &lt;= 0:        root.left = self.leftRotate(root.left)        return self.rightRotate(root)            # Right-left case    if balance &lt; -1 and self.getBalance(root.left) &gt;= 0:        root.right = self.rightRotate(root.right)        return self.leftRotate(root)            return rootReferenceBaike - 平衡二叉树\nWiki - 平衡树\n简书 - AVL Tree的Java实现\nGeeksforGeeks - AVL Tree\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Data%20Structure%20and%20Algorithm/HD_AVL.jpg","plink":"https://magi003769.github.io/post/平衡二叉树/"},{"title":"Binary Search Tree","date":"2019-05-25T07:00:00.000Z","date_formatted":{"ll":"May 25, 2019","L":"05/25/2019","MM-DD":"05-25"},"updated":"2019-06-21T06:15:34.000Z","content":"在学习kNN的算法实现的时候看到了KD Tree，它是二叉搜索树（Binary Search Tree）的一种。所以在学KD Tree构建的同时也把BST学一下。和以前一样，内容还是LeetCode相关Card的搬运和一些题目的实现。\nBinary Search Tree二叉搜索树（BST）其实是二叉树的一种特殊形式。它特殊在树的构建时，需要遵循以下的一些规则\n\n某一节点的value要大于等于其左侧子树所有节点的value\n某一节点的value要小于等于其右侧子树所有节点的value\n\n上面这两个性质是非常重要的，在后续进一步讨论平衡树时会用到。和一般的二叉树一样BST也可以进行pre-order、inorder和post-order的树遍历，比较独特的一点是，BST的inorder遍历结果是升序的。因此inorder遍历是BST最常用的便利方式。\nBST的验证根据之前描述的BST性质，我们需要在递归的过程中，用当前当前节点的值更新比较边界：对于左侧子树更新上界upper，对右侧子树更新下界lower。\n123456789101112class Solution(object):    def isValidBST(self, root, upper=float('inf'), lower=float('-inf')):        \"\"\"        :type root: TreeNode        :rtype: bool        \"\"\"        if not root:            return True        if root.val &gt;= upper or root.val &lt;= lower:            return False        return self.isValidBST(root.left, min(upper, root.val), lower) and \\               self.isValidBST(root.right, upper, max(lower, root.val))二叉搜索树的迭代器173. Binary Search Tree Iterator我本来的想法是直接用inorder的遍历方法，将整个树转成一个升序数组。然而这个题实际上是将这个遍历方法拆开来。__init__将栈直接推到第一个最底层的左节点。调用next将堆最顶端的节点pop出来，再以这个节点的右子节点为起点继续往堆里添加节点。hasNext直接检查堆的长度就可以实现。\n123456789101112131415161718192021222324252627282930class BSTIterator(object):    def __init__(self, root):        \"\"\"        :type root: TreeNode        \"\"\"        self.stack = []        while root:            self.stack.append(root)            root = root.left                    def next(self):        \"\"\"        @return the next smallest number        :rtype: int        \"\"\"        node = self.stack.pop()        val = node.val        node = node.right        while node:            self.stack.append(node)            node = node.left        return val    def hasNext(self):        \"\"\"        @return whether we have a next smallest number        :rtype: bool        \"\"\"        return len(self.stack) &gt; 0BST的基本操作我们先梳理一下二叉树的基本操作有哪些\n\n插入（增加）节点\n删除节点\n搜索节点\n\nBST的搜索基于上面BST的性质，就可以很轻易的在树里面找到节点。目标小于当前节点值就继续向左侧搜索，大于就向右搜索，知道相等或到达最深。\n12345678910111213class Solution(object):    def searchBST(self, root, val):        \"\"\"        :type root: TreeNode        :type val: int        :rtype: TreeNode        \"\"\"        if not root or root.val == val:            return root        if val &lt; root.val:            return self.searchBST(root.left, val)        if val &gt; root.val:            return self.searchBST(root.right, val)在BST中插入新节点这就是一个非常简单的按照BST逻辑进行插入的方法。这个简单逻辑并不能保证生成的BST是Balanced Tree，平衡树概念的相关介绍会在后面一节介绍。\n1234567891011121314class Solution(object):    def insertIntoBST(self, root, val):        \"\"\"        :type root: TreeNode        :type val: int        :rtype: TreeNode        \"\"\"        if not root:            return TreeNode(val)        if val &gt; root.val:            root.right = self.insertIntoBST(root.right, val)        else:            root.left = self.insertIntoBST(root.left, val)        return root删除BST中的节点这个操作比之前的都稍微麻烦一点，以为被删除节点的子树可能有以下三种不同的情况：\n\n如果该节点没有子节点，直接删除\n如果该节点有一个子节点，用该子节点替换被删除的节点\n如果该节点有两个子节点，在其子树中寻找其在in-order排序下的前一个或后一个元素，进行替换并将找到的这个节点删除\n\n\n12345678910111213141516171819202122232425class Solution(object):    def deleteNode(self, root, key):        \"\"\"        :type root: TreeNode        :type key: int        :rtype: TreeNode        \"\"\"        if not root:            return root        if key &lt; root.val:            root.left = self.deleteNode(root.left, key)        elif key &gt; root.val:            root.right = self.deleteNode(root.right, key)        else:            if not root.left:                return root.right            elif not root.right:                return root.left            else:                predec = root.left                while predec.right:                    predec = predec.right                root.val = predec.val                root.left = self.deleteNode(root.left, predec.val)        return rootHeight-balanced Tree\n平衡树是计算机科学中的一类改进的二叉查找树。一般的二叉查找树的查询复杂度是跟目标结点到树根的距离（即深度）有关，因此当结点的深度普遍较大时，查询的均摊复杂度会上升，为了更高效的查询，平衡树应运而生了。在这里，平衡指所有叶子的深度趋于平衡，更广义的是指在树上所有可能查找的均摊复杂度偏低。\n\n这是Wiki上面对平衡二叉树的一种描述。我们可以想象一下在构建BST的时候，如果传入的序列是一个排好序的，那么节点就会不断出现在节点的同一侧。这样，树就退化成了一个类似链表的结构。那么操作的复杂度就有可能从\nO(log_2n)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n\n退化成\nO(n)\n\n\n\n\n\n\n\n \n \n \n \n\n的线性情况。因此就出现了使用Balanced Tree的概念。\n用sorted array构建BST先给一个非常直观的递归解决，每次都选择中间的元素创造新的节点，再将其两侧的序列nums[:mid]和nums[mid+1:]作为参数去构建该节点的子树。\n12345678910111213class Solution(object):    def sortedArrayToBST(self, nums):        \"\"\"        :type nums: List[int]        :rtype: TreeNode        \"\"\"        if not nums:            return None        mid = len(nums) // 2        root = TreeNode(nums[mid])        root.left = self.sortedArrayToBST(nums[:mid])        root.right = self.sortedArrayToBST(nums[mid+1:])        return root在给定数组的情况下，我们可以用上面这样的方法来构造BST。但如果我们的输入不是整个数组，而是需要从一个数据流中构建BST呢？不管这个数据流是否有序，我们都无法保证BST在搜索时的高效性。这就是我们之前所说，BST结构退化成线性结构的问题。关于平衡二叉树的讨论会在一篇专门的post里面讨论。\nReferenceBaike - 平衡二叉树\n\nWiki - 平衡树\n\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Data%20Structure%20and%20Algorithm/HD_BST.jpeg","plink":"https://magi003769.github.io/post/Binary_Search_Tree/"},{"title":"链表的双指针","date":"2019-05-05T07:00:00.000Z","date_formatted":{"ll":"May 5, 2019","L":"05/05/2019","MM-DD":"05-05"},"updated":"2020-05-10T22:56:00.000Z","content":"链表存在的一个最大的问题就是不支持下标索引，且只能通过遍历获取整个链表的长度。这样，在array中经常使用的双指针法看上去似乎很难有用武之地。但实际上，链表中也有它独特的双指针使用方法。\nSlow and Fast相比array中双指针可以通过改变下标实现任意元素的访问，链表中的指针是不可能实现这样的效果的。因此，链表中的双指针有比较特殊的用法：使用两个移动速度不同的指针，slow和fast，来实现一些有趣的解决方法。在每一次迭代中，fast指针都会多移动一步。\n123456789101112131415// Initialize slow &amp; fast pointersListNode slow = head;ListNode fast = head;/** * Change this condition to fit specific problem. * Attention: remember to avoid null-pointer error **/while (slow != null &amp;&amp; fast != null &amp;&amp; fast.next != null) &#123;    slow = slow.next;           // move slow pointer one step each time    fast = fast.next.next;      // move fast pointer two steps each time    if (slow == fast) &#123;         // change this condition to fit specific problem        return true;    &#125;&#125;return false;   // change return value to fit specific problemLinked List Cycle141. Linked List Cycle要求判断链表里面优美与出现循环的情况。使用双指针的话就很巧妙的解决了这个问题。如果没有cycle，那么fast将会最先到达链表的尾端这时即可返回false；而如果是有循环的情况，fast将会逐渐逼近slow，有点类似赛跑时的“套圈”。\n12345678910111213141516171819class Solution(object):    def hasCycle(self, head):        \"\"\"        :type head: ListNode        :rtype: bool        \"\"\"        if head == None:            return False        slow = head        fast = head        while True:            slow = slow.next            fast = fast.next            if fast != None and fast.next != None:                fast = fast.next            else:                return False            if slow == fast:                return True142. Linked List Cycle II是之前这道题的升级版，不仅是要判断有误循环，还要找到形成cycle的那个节点，记为E。从下面的代码中可以看出，我们还是利用快慢两个指针得到他们相遇的节点，记为M。那么如果设链表的起点为H、cycle的长度为C。\nH到E的距离为L1\nE到M的距离为L2\n沿链表方向M到E距离为C - L2\nslow走过的长度为L1+L2\nfast走过的长度为l1+L2+n*C\n由上面的关系我们可以得出两个指针经过的路径长度是二倍关系，所以就能得出：L1 = (n-1)*C + (C - L2)。这说明（可能经过几个cycle之后又回到M）相遇点M到E的距离与起点H到E的距离是相等的。我们可以复用slow，并令指针head同步向前，两者最终会在E相遇。\n123456789101112131415161718class Solution(object):    def detectCycle(self, head):        \"\"\"        :type head: ListNode        :rtype: ListNode        \"\"\"        slow = fast = head        while fast and fast.next:            slow = slow.next            fast = fast.next.next            if slow == fast:                break        else:            return None        while head != slow:            slow = slow.next            head = head.next        return headPalindrome Linked List234. Palindrome Linked List也是链表的一个经典问题，如果我们能直接获取链表的长度和利用下标索引访问元素（就像array一样），那这个问题其实是很好办的。然而链表蛋疼之处就在于这两个都不行。而链表双指针却恰好可以通过这个二倍的速度，来帮助获取链表的“中点”。我们需要比较注意的是在实现fast步长为2时的条件。我们需要fast.next.next去迭代这个指针，所以要保证fast.next也不是空。\n1234567891011121314151617181920212223242526class Solution(object):    def isPalindrome(self, head):        \"\"\"        :type head: ListNode        :rtype: bool        \"\"\"        # Get slow pointer to the middle of list        fast, slow = head, head        while fast and fast.next: # this condition!            fast = fast.next.next            slow = slow.next        # reverse second half of list        new_head = None        while slow:            cur = slow            slow = slow.next            cur.next = new_head            new_head = cur        # check        while new_head:            print(new_head.val, head.val)            if new_head.val != head.val:                return False            head = head.next            new_head = new_head.next        return TrueFast Goes First这一类的双指针方法就如其名：先迭代指针fast，再开始同时迭代两个指针。这样，在后续的迭代中两个指针就会保持一个固定的距离，两个指针也就覆盖了链表中的一个确定的范围。链表相关的题目中如果涉及到倒数第N各元素或者需要指定一部分链表进行操作时，就可以利用这种类型的双指针来解题目。\nRemove Nth Node from End of List解决19. Remove Nth Node From End of List就可以这样，让fast领先于slow指针N个节点，这样两个指针就合计覆盖了长度为N+1的链表范围。到此开始同步移动两个指针直到fast到达链表尾端。此时slow指向的就是链表的倒数第N+1个节点，将其next的指向修改即可。\n123456789101112class Solution:    def removeNthFromEnd(self, head: ListNode, n: int) -&gt; ListNode:        dummy = ListNode('#')        dummy.next = head        fast = slow = dummy        for _ in range(n):            fast = fast.next        while fast.next:            fast = fast.next            slow = slow.next        slow.next = slow.next.next        return dummy.nextRotate List我已开始做这道题的思路，是用一个queue存一系列链表的节点。这个queue的长度固定为N+1。虽然这道题目也过了，但是内存占用实在是太大了。后来考虑了一下，其实我需要的只有这个queue里的第一以及最后一个元素，所以不如使用双指针。从下面实现的代码中可以看出，除去前面吧k取模数的步骤之外，剩下的就几乎和上一道去除倒数第N个节点的题目一样。\n123456789101112131415161718192021222324252627class Solution(object):    def rotateRight(self, head, k):        \"\"\"        :type head: ListNode        :type k: int        :rtype: ListNode        \"\"\"        if not head or not head.next:            return head        size = 0        count = head        while count:            size += 1            count = count.next        k = k % size        if k == 0:            return head        slow = fast = head        for _ in range(k):            fast = fast.next        while fast.next:            slow = slow.next            fast = fast.next        ans = slow.next        slow.next = None        fast.next = head        return ansWhat the Hell !?又是如标题所述，这个部分记录的就是一些非常巧妙的解法。看见的时候，第一反应都是：WTF？？？还有这种操作？？？这部分的题目可能以后还会再更新。\nIntersection of Two Linked List\n这道题就是对两个head表示的两个链表各自使用一个指针，当迭代到链表末端的时候，就回到另一个起始点继续开始遍历。由此，当两个指针再次到达末尾的时候，走过的步数是两个链表的长度之和。也就是说，他们会同时再次到达尾部。由上面的图我们可以得知，交点之后的链表实际上是完全一样的，逆推一下的话我们可以得出这样的结论：两个指针会同时抵达交点。\n1234567891011class Solution(object):    def getIntersectionNode(self, headA, headB):        \"\"\"        :type head1, head1: ListNode        :rtype: ListNode        \"\"\"        curA, curB = headA, headB        while curA is not curB:            curA = curA.next if curA else headB            curB = curB.next if curB else headA        return curAReferenceHow to solve Linked List Cycle 2\nLeetCode - Two-pointer technique\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/karsten-gohm-1556287-unsplash.jpg","plink":"https://magi003769.github.io/post/链表的双指针/"},{"title":"Linked List","date":"2019-05-01T07:00:00.000Z","date_formatted":{"ll":"May 1, 2019","L":"05/01/2019","MM-DD":"05-01"},"updated":"2019-05-26T23:34:16.000Z","content":"LeetCode Explore Linked List专题的搬运，主要介绍链表的概念和常见问题，概念部分夹杂一些参考书《数据结构预算法Python实现》上面的内容。\nLinked ListPython 中 array-based list class，虽然这些类已经进行了高度的优化而且是储存信息的不错选择，但是它有一系列的缺点：\n\ndynamic array的长度可能比实际上的元素个数要多，\n操作的 amortized bounds (???) 对于实际\n中间元素的插入或删减代价非常高\n\n使用链表可以避免以上的三个问题，但是链表无法高效的通过 index 访问元素。\nSingly Linked List\n链表的第一个和最后一个元素分别是链表的head和tail。我们可以遍历从头到尾的所有节点。Singly Linked Lists，即单项链表，是最基本的链表，每个节点只有指向下一个节点的 pointer。链表在内存中的表示依赖于许多对象的集合：每个节点是unique的对象，另外还有一个对象指向的是整个链表。一个链表实例至少要有 head 的 reference，而 tail 的 reference 则并不一定需要，我们可以从头遍历整个链表找到它。但是我们一般会保存它，来避免麻烦的遍历。出于同样的思考，一个链表实例也要储存它节点的个数以便于获得整个链表的长度size。\n然而在LeetCode刷题的过程中，往往tail和size在链表的实例中都是不存在的，这也给我们刷题带来了更大的挑战。比如一些提前预知链表长度就可以轻松解决的问题，在没有size的情况下就会变得很复杂，只能通过遍历和一些特殊方法解决，e.g. two pointer。\nCirularly Linked List循环链表tail的next指向链表的head，因此在实现循环链表的过程中，我们实际上只需要指定tail即可。\nDoubly Linked List\n前面利用 Singly linked list 实现的简单 stack 和 queue 里面，stack 只对 head 处的节点做删减操作，queue 只对 head 处的节点做删除，tail 处的节点做增加。这些都没有涉及到对链表尾部的删除操作，这也正是单向链表的一个重要缺陷：无法高效地删除尾部或者任意中间节点进行。即使单项链表中有 tail 的属性指向链表的尾部，我们也无法了解就得为不节点之前是什么，无法将它的 next 改为 Null。\n具体的双向链表实现可以参考707. Design Linked List中的submission。这里讲一下addAtIndex和deleteAtIndex这两个操作的实现。首先是addAtIndex，当链表的下标从0开始的时候，我们从head开始用循环来遍历节点。当循环的次数为k时，指针最终指向的就是第k个元素。当使用addAtIndex(k, val)时，我们实际上只需要遍历到第k-1个元素即可，剩下的就是修改涉及到的三个节点各自的前后关系就可以。当然不要忘记修改self.size，这个也同样关键。\n123456789101112131415161718192021222324def addAtIndex(self, index, val):        \"\"\"        Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted.        :type index: int        :type val: int        :rtype: None        \"\"\"        if index &gt; self.size:            return        if index &lt;= 0:            self.addAtHead(val)        elif index == self.size:            self.addAtTail(val)        else:            add = self._Node(val)            prev = self.head            for _ in range(index-1):                prev = prev.next            next_ = prev.next            add.prev = prev            add.next = next_            prev.next = add            next_.prev = add            self.size += 1稍微有点坑的是deleteAtIndex这个操作，会有一些额外的特殊的case需要处理。比如当链表只有一个节点的时候，self.head和self.tail都指向同一节点，且节点的prev和next都为空。如果不处理这个情况的话，后续的情况cur.prev.next这样的对象会报错。\n123456789101112131415161718192021222324def deleteAtIndex(self, index):        \"\"\"        Delete the index-th node in the linked list, if the index is valid.        :type index: int        :rtype: None        \"\"\"        if index &lt; 0 or index &gt;= self.size:            return        cur = self.head        for _ in range(index):            cur = cur.next        if self.size == 1:            self.head = None            self.tail = None        elif cur is self.head:            cur.next.prev = cur.prev            self.head = cur.next        elif cur is self.tail:            cur.prev.next = cur.next            self.tail = cur.prev        else:            cur.prev.next = cur.next            cur.next.prev = cur.prev        self.size -= 1Classic Problem这一届记录几个非常有趣的链表算法题。都是很常用的操作，比如reverse和delete。这些操作都可以在array中非常轻易的实现（准确的说是各种各级语言已经提供了相关的方法，还有方便的index访问机制），这里就通过时先来感受链表的蛋疼所在。\nReverse Linked List解决206. Reverse Linked List，除了初始的head，还需要而外的两个指针curr和prev。head用来遍历链表中的节点， curr用来确定当前需要操作的节点，prev则是该节点在新链表中所指向的节点。\n\n下面是Python实现，其中的指针等价关系为：cur == curr、new_head == prev。操作顺序和指针使用上和上面的gif虽然有区别，但是基本的逻辑都大致相同。\n12345678910111213class Solution(object):    def reverseList(self, head):        \"\"\"        :type head: ListNode        :rtype: ListNode        \"\"\"        new_head = None        while head:            cur = head            head = head.next            cur.next = new_head            new_head = cur        return new_headPalindrome Linked List234. Palindrome Linked List遍历整个链表，将元素的值存到一个list里面再[::-1]一下就非常容易做，但是这样我们就有\nO(n)\n\n\n\n\n\n\n\n \n \n \n \n\n的空间复杂度。而题目要求我们做到\nO(n)\n\n\n\n\n\n\n\n \n \n \n \n\n的时间复杂度和\nO(1)\n\n\n\n\n\n\n\n \n \n \n \n\n的空间复杂度。由于无法像array一样使用index的方式来访问元素且链表示单向的，我们就需要一个办法找到链表的中间位置。这个问题可以利用链表中的Tow Pointer 方法来解决：slow指针步长为1，fast指针步长为2。这样在fast到达链表尾部的时候，slow就位于链表的中间位置。当节点个数为奇数时，slow停在正中间的节点；当节点个数为偶数时，则停在n/2+1处。这个已经在之前链表的双指针方法里面讨论过了。\n1234567891011121314151617181920212223242526class Solution(object):    def isPalindrome(self, head):        \"\"\"        :type head: ListNode        :rtype: bool        \"\"\"        # Get slow pointer to the middle of list        fast, slow = head, head        while fast and fast.next:            fast = fast.next.next            slow = slow.next        # reverse second half of list        new_head = None        while slow:            cur = slow            slow = slow.next            cur.next = new_head            new_head = cur        # check        while new_head:            print(new_head.val, head.val)            if new_head.val != head.val:                return False            head = head.next            new_head = new_head.next        return True具体的实现分成几个步骤：获取中间位置、将后半部分链表反向、从head和中间位置new_head开始进行比较。这里面会用到之前的反向链表的方法。slow相当于原始链表的起始位置，这个起始位置仍然是可以由head开始遍历到。也就是说，整个链表实际上并没有被切分，只是后面的部分被反转了而已。因此不用考虑当元素个数为奇数的时候，中间元素是否head和new_head是否都能遍历到。\nSummary再来总结一下链表（单项和双向）的一些性质：\n无法在constant time内访问链表的任意位置\n在某已确定节点后或者链表开头处添加元素的复杂度是O(1)\n删除第一个节点的复杂度是O(1)\n但是在删除节点（包括最后一个节点）时会有一些不同的地方需要注意：单项链表因为节点没有prev信息，再删除时复杂度会很高，是O(N)；而双向链表则是O(1)。\n\n上面的这个表格是集中数据结构操作复杂度的一个对比，不难看出链表除了在按下表访问元素上劣势，在删减操作上还是很有优势的。因此在选择时我们可以使用这样的原则：\n\nIf you need to add or delete a node frequently, a linked list could be a good choice.\nIf you need to access an element by index often, an array might be a better choice than a linked list.\n\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Data%20Structure%20and%20Algorithm/HD_linked-list.jpeg","plink":"https://magi003769.github.io/post/linked-list/"},{"title":"Queue and BFS","date":"2019-04-25T07:00:00.000Z","date_formatted":{"ll":"Apr 25, 2019","L":"04/25/2019","MM-DD":"04-25"},"updated":"2019-05-26T23:34:28.000Z","content":"LeetCode上面 Queue &amp; Stack 专题的搬运：Queue部分。（可能也有那么一点自己的理解，存疑）\nQueue\nQueue 是一个 FIFO 的数据结构。在这个类似数组的结构里，我们无法像一般的array一样通过index去访问其中的元素，且我们一般只允许在末尾增加元素，在头部删减元素。这两个操作分别被称为 enqueue 和 dequeue。\nQueue and BFSBFS: Breadth-first Search (BFS) 的常见应用是寻找最短路径。很多时候 BFS 是用在 Tree 或者 Graph 之类的数据结构中，经常用来做遍历或找最短路径。不过它也可以用在其他数据结构中。\n\nIt will be important to determine the nodes and the edges before doing BFS in a specific question. Typically, the node will be an actual node or a status while the edge will be an actual edge or a possible transition\n\n\n以上面这副图为例子，我们想要从节点 a 到达节点 h。创建一个空的 queue，将根节点先添加进去。进入循环，将 queue 前部的节点 pop 出来，访问这个节点的子节点，并添加进 queue 里。如此循环下去，直到 queue 为空。每一次循环中，BFS都会更新一个深度变量 step，这也是算法最终的结果。下面是一个 BFS 的一个模板。\n1234567891011121314151617181920212223242526272829/** * Return the length of the shortest path between root and target node. */int BFS(Node root, Node target) &#123;    Queue&lt;Node&gt; queue;  // store all nodes which are waiting to be processed    Set&lt;Node&gt; visited;  // store all the nodes that we've visited    int step = 0;       // number of steps neeeded from root to current node    // initialize    add root to queue;    add root to visited;    // BFS    while (queue is not empty) &#123;        step = step + 1;        // iterate the nodes which are already in the queue        int size = queue.size();        for (int i = 0; i &lt; size; ++i) &#123;            Node cur = the first node in queue;            return step if cur is target;            for (Node next : the neighbors of cur) &#123;                if (next is not in used) &#123;                    add next to queue;                    add next to visited;                &#125;                remove the first node from queue;               &#125;        &#125;    &#125;    return -1;          // there is no path from root to target&#125;值得注意的是这里面有一个变量是 visited，在有向图中，如果没有 cyclic 的情况出现，这个变量其实是没有必要的。但是有些问题中，我们必须保证同一节点不会被访问两次，否则会陷入死循环。这是有一个变量或者方法实现 visited 的效果就很重要了。下面用一个题目来展示一下 Queue 的实际应用：200. Number of Islands。\n12345678910111213141516171819202122232425262728293031323334class Solution(object):    def numIslands(self, grid):        \"\"\"        :type grid: List[List[str]]        :rtype: int        \"\"\"        num_island = 0        for i in range(len(grid)):            for j in range(len(grid[0])):                if grid[i][j] == \"1\":                    num_island += 1                    self.bfs((i,j), grid)        return num_island        def bfs(self, coord, grid):        \"\"\"        This function receives a seed points and         convert the \"1\" to \"0\" to eliminate current island        \"\"\"        w, h = len(grid), len(grid[0])        # construct queue        queue = []        # Add pixel to queue and set to zero (used)        queue.append(coord)        grid[coord[0]][coord[1]] = \"0\"        while len(queue):            cur = queue.pop(0)            for neib in [(max(cur[0]-1, 0), cur[1]),                        (min(cur[0]+1, w-1), cur[1]),                        (cur[0], max(cur[1]-1, 0)),                        (cur[0], min(cur[1]+1, h-1))]:                if grid[neib[0]][neib[1]] == \"1\":                    queue.append(neib)                    grid[neib[0]][neib[1]] = \"0\" # key point主方法 numIsland()，遍历所给 grid 里是 &quot;1&quot; 的元素。在某一次循环中，以改点为种子点，找到它所在的岛屿。每次调用 bfs()就会创建一个记录岛屿所包含点的 queue，每次 pop 出最早加入点，在判断其四邻域是否是也属于岛屿，如果属于则添加进 queue。如此循环，直到 queue 为空。\n这个场景中，每个节点的子节点是他的四邻域，从图的角度来说，会出现 cyclic 的情况。另外一点就是主函数是遍历所有为 &quot;1&quot; 的点的，如果没有 visited 这个机制的话，一定会出现重复。所以在每次调用 bfs() 处理一个岛屿的时候，可以想象到是把这个岛屿从图中抹去的一个过程。\nReferenceLeetCode Explore: Queue &amp; Stack\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Data%20Structure%20and%20Algorithm/HD_queue.jpg","plink":"https://magi003769.github.io/post/Queue/"},{"title":"Stack and DFS","date":"2019-04-25T07:00:00.000Z","date_formatted":{"ll":"Apr 25, 2019","L":"04/25/2019","MM-DD":"04-25"},"updated":"2019-05-26T23:34:38.000Z","content":"LeetCode上面 Queue &amp; Stack 专题的搬运：Stack部分。（可能也有那么一点自己的理解，存疑）\nStack使用 stack 经常是这样的场景：当前的某一步的操作需要之前的一些信息或者结果，或者说对节点和元素的处理与访问它们的顺序是相反的。 比如说 Reverse Polish Calculator，我们需要保留之前的数字，在遇到计算符的时候，使用最近的两个数字进行计算。\n1234infix      reverse-Polish-------    --------------a+b*c      a b c * +(a+b)*c    a b + c *这个时候就可以使用 stack，在遇到数字把数字 push 进去，遇到计算符把 stack 最顶端的两个元素 pop 出来，并将计算结果 push 回去。\nStack and DFS与 BFS 很相似，Depeth-First Search 也可以找到从根节点到目标节点的路径，但是这个路径并不一定是最短的。总的来说，DFS 方法中，我们只有在到达最深的节点时，才会开始回溯。这也是这一方法名字的由来。\n\nAs a result, the first path you found in DFS is not always the shortest path. For instance, in the example above, we successfully found a path A-&gt;C-&gt;F-&gt;G and stop the DFS. But this is not the shortest path from A to G.\n\n\n我们对节点的处理顺序（回溯）和我们添加节点的顺序是完全相反的，所以我们才会选择 stack 来实现 DFS。下面是 DFS 的一个模版。\n12345678910111213/* * Return true if there is a path from cur to target. */boolean DFS(Node cur, Node target, Set&lt;Node&gt; visited) &#123;    return true if cur is target;    for (next : each neighbor of cur) &#123;        if (next is not in visited) &#123;            add next to visted;            return true if DFS(next, target, visited) == true;        &#125;    &#125;    return false;&#125;我们可以发现，这是一个递归方法。当时用这种递归方法时，我们看上去没有用任何的 stack。然而实际上，我们仍然用了系统提供的隐式的 stack，称之为 Call Stack。下面的代码，以200. Number of Islands作为例子，说明 DFS 的使用。\n12345678910111213141516171819202122class Solution(object):    def numIslands(self, grid):        \"\"\"        :type grid: List[List[str]]        :rtype: int        \"\"\"        num_island = 0        for i in range(len(grid)):            for j in range(len(grid[0])):                if grid[i][j] == \"1\":                    num_island += 1                    self.bfs((i,j), grid)        return num_island        def dfs(self, x, y, grid):        if x &gt;= len(grid) or y &gt;= len(grid[0]) or x &lt; 0 or y &lt; 0 or grid[x][y] != \"1\":            return        grid[x][y] = \"0\"        self.dfs(x-1, y, grid)        self.dfs(x+1, y, grid)        self.dfs(x, y-1, grid)        self.dfs(x, y+1, grid)Stack Overflow使用递归方法的一个优点就是容易实现，但是当递归的深度太大时，会出现 stack overflow。这个时候我们其实可以使用 BFS 或者使用 explicit stack。\n1234567891011121314151617181920/* * Return true if there is a path from cur to target. */boolean DFS(int root, int target) &#123;    Set&lt;Node&gt; visited;    Stack&lt;Node&gt; stack;    add root to stack;    while (s is not empty) &#123;        Node cur = the top element in stack;        remove the cur from the stack;        return true if cur is target;        for (Node next : the neighbors of cur) &#123;            if (next is not in visited) &#123;                add next to visited;                add next to stack;            &#125;        &#125;    &#125;    return false;&#125;这里我们用一个 while 循环和 stack 来模拟递归中的 system call stack。以之前GIF里面找路径的问题为例子，从根节点出发，在第一个循环结束后，stack 里面是根节点的子节点。随后再继续进行迭代，将子节点的子节点加入 stack。在到达最深处的时候，不再增加 neibhbor 进 stack，并开始慢慢从中 pop 节点出来知道 stack 变空。\n我们可以注意到 visited 这个 set，它其实是为了防止这样一个情况出现：在回溯的过程中，我们会遇到有其它分支的节点。它的 neighbor 里面其实是包含有之前从 stack 里pop出去的元素。如果再次将它push到 stack 里面，就会出现这样的 \nSeveral Examples以下几个题目是刷 LeetCode 上 Queue &amp; Stack 这个 card 里出现的题目，有几个还挺有趣的。也用到了 stack，就放在这里略微讲解一下\nDecode String394.Decode String 这道题就有点巧妙，其实很早之前在水某学校录取群时就见过这个题，要求解析 &quot;3[a2[b]c]de&quot; 这种形式的字符串，其结果是 &quot;abbcabbcabbcde&quot;。其实很容易理解，数字代表中括号内字符串的循环次数。比较恶心的地方在于如何去update这个字符串。基本思路大概就下面三点：\nnum_stack 和 alpha_stack 存储较为外层的循环次数和字符串\n只有在遇到 &quot;]&quot; 的时候才会更新返回的字符串，先把当前的字符串做重复，在把前一段字符串pop出来做prefix\ntmp_num 和 tmp_char 用来处理多位的循环次数和连续的字符串\n下面是具体实现的代码\n123456789101112131415161718192021222324252627class Solution(object):    def decodeString(self, s):        \"\"\"        :type s: str        :rtype: str        \"\"\"        num_stack = []        alpha_stack = []        tmp_str = \"\"        tmp_num = \"\"        prev = \"\"        for char in s:            if char.isdigit():                tmp_num += char            elif char.isalpha():                tmp_str += char            elif char == \"[\":                alpha_stack.append(tmp_str)                num_stack.append(int(tmp_num))                tmp_str = \"\"                tmp_num = \"\"            elif char == \"]\":                prev = alpha_stack.pop()                tmp_str = tmp_str * num_stack.pop()                tmp_str = prev + tmp_str            #print(tmp_str, num_stack)        return tmp_strKeys and Rooms841.Keys and Rooms 这也是一道很有意思的题目，可以把每一个room当成是树里面的一个节点，他所有钥匙的房间是这个节点的子节点。stack 用来储存房间包含的 keys。需要注意的是，stack 中的元素是有重复的，也就是说我们有可能走很多重复的道路。所以已经在 keys 这个set里出现过的就可以直接忽略了。\n1234567891011121314151617class Solution(object):    def canVisitAllRooms(self, rooms):        \"\"\"        :type rooms: List[List[int]]        :rtype: bool        \"\"\"        stack = [0]        keys = set(stack)        while stack:            i = stack.pop()            for j in rooms[i]:                if j not in keys:                    stack.append(j)                    keys.add(j)                    if len(rooms) == len(keys):                        return True        return len(rooms) == len(keys) 这里插播一条关于 set 这个数据结构的小知识，set 其实是基于哈希的一种数据结构，查找起来比内建的 list 更加快捷，且在使用方法 .add() 添加元素时，如果添加的内部已有的元素，将不再重复添加，非常方便。这种基于哈希的数据结构在 Java 和 C++ 里面其实都是有的：\n1HashSet&lt;Integer&gt; seen = new HashSet&lt;Integer&gt;(); seen.add(0);1unordered_set&lt;int&gt; seen = &#123;0&#125;;ReferenceMonash - Stack\nstackoverflow - Call Stack\nLeetCode Explore: Queue &amp; Stack\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Data%20Structure%20and%20Algorithm/HD_stack.jpeg","plink":"https://magi003769.github.io/post/Stack/"},{"title":"Binary Tree（2）","date":"2019-04-15T07:00:00.000Z","date_formatted":{"ll":"Apr 15, 2019","L":"04/15/2019","MM-DD":"04-15"},"updated":"2019-05-19T22:46:26.000Z","content":"这一篇post是之前Binary Tree的后续，主要记录一些Binary Tree相关的题目。其实LeetCode上有很多题是相同或者存在极大关联的，基本思想都差不多，一道解决了，其他的也就都解决了。解法的分析也尽量将递归和迭代两种方法都实现一下（当然我也只是尽量，迭代方法还真不一定看得懂）。\nFind Path这类题目基本上就是按要求找从 root 出发到leaf的路径。257. Binary Tree Paths、Path Sum II和129. Sum Root to Leaf Numbers这几个题目思路基本思路几乎是一模一样的，所以就挑其中的129讲一下.\n1234567891011121314151617181920class Solution(object):    def sumNumbers(self, root):        \"\"\"        :type root: TreeNode        :rtype: int        \"\"\"        if not root:            return 0        ans = []        self._pathSum(root, root.val, ans)        return sum(ans)        def _pathSum(self, node, sum_, ans):        if node.left == None and node.right == None:            ans.append(sum_)            return        if node.left:            self._pathSum(node.left, sum_ * 10 + node.left.val, ans)        if node.right:            self._pathSum(node.right, sum_ * 10 + node.right.val, ans)Depth of Tree这个题型实际上已经在之前的一篇post里讨论过了，这里有拿出来是因为遇到了一个会用到这个的相关题目。这也是和前面的路径寻找相类似的常见问题。\nMaximum Depth of Binary Tree我们依然用递归的方法，当目前节点为Null时，返回0,。因此对于二叉树最深处的某个节点，它所返回的深度应该是1。以此类推，即可获得二叉树的最大深度\n123456789class Solution(object):        def maxDepth_btop(self, root):        \"\"\"        :type root: TreeNode        :rtype: int        \"\"\"        if root == None:            return 0        return max(self.maxDepth(root.left), self.maxDepth(root.right)) + 1Diameter of Binary Tree543. Diameter of Binary Tree这道题虽然是一个easy，思路也没有很难，但是我还是绕了半天。下面是一个错误答案，我的思路开始是把 root 的左右最大的深度加在一起，这样就是diameter了。但是我忽略了左右非常不平衡的情况，比如左边的substree很深而且有很多节点，而右边的subtree很浅，甚至更极端的情况只有一个节点。这样，使用root两侧的深度相加的想法就是不行的了。\n123456789101112131415161718class Solution(object):    def diameterOfBinaryTree(self, root):        \"\"\"        :type root: TreeNode        :rtype: int        \"\"\"        if not root:            return 0        if root.left == None and root.right == None:            return 0        left = self._getDepth(root.left)        right = self._getDepth(root.right)        return left + right            def _getDepth(self, node):        if not node:            return 0        return max(self._getDepth(node.left), self._getDepth(node.right)) + 1由上面的分析可以得出，我们需要在subtree的部分也进行遍历，对每个节点的subtree进行直径计算，来寻找最大。我们依然使用某一节点两侧相加为最大，但是我们会在每次计算之后比较大小，如果需要则更新结果。下面是一个正确答案。\n1234567891011121314151617class Solution(object):    def diameterOfBinaryTree(self, root):        \"\"\"        :type root: TreeNode        :rtype: int        \"\"\"        self.max = 0        self._getDepth(root)        return self.max            def _getDepth(self, node):        if not node:            return 0        left = self._getDepth(node.left)        right = self._getDepth(node.right)        self.max = max(self.max, left + right)        return max(left, right) + 1Construct Tree这部分我是真的迷。等我再研究研究，希望能把坑填上。答案先贴上，一般都是从inorder中访问元素并创建节点，将preorder或者postorder作为迭代顺序的依据。inorder的根节点是在中间的，不论对于整个树还是对任一节点为根节点的suntree\nFrom inorder and postorder12345678910111213class Solution(object):    def buildTree(self, inorder, postorder):        \"\"\"        :type inorder: List[int]        :type postorder: List[int]        :rtype: TreeNode        \"\"\"        if inorder:            idx = inorder.index(postorder.pop())            root = TreeNode(inorder[idx])            root.right = self.buildTree(inorder[idx+1:], postorder)            root.left = self.buildTree(inorder[:idx], postorder)            return rootFrom inorder and preorder12345678910111213class Solution(object):    def buildTree(self, preorder, inorder):        \"\"\"        :type preorder: List[int]        :type inorder: List[int]        :rtype: TreeNode        \"\"\"        if inorder:            idx = inorder.index(preorder.pop(0))            root = TreeNode(inorder[idx])            root.left = self.buildTree(preorder, inorder[:idx])            root.right = self.buildTree(preorder, inorder[idx+1:])            return root","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/alice-donovan-rouse-177313-unsplash.jpg","plink":"https://magi003769.github.io/post/Tree-02/"},{"title":"Binary Tree（1）","date":"2019-04-05T07:00:00.000Z","date_formatted":{"ll":"Apr 5, 2019","L":"04/05/2019","MM-DD":"04-05"},"updated":"2019-05-17T08:10:06.000Z","content":"LeetCode 上面 Binary Tree 专题的搬运。介绍二叉树的基本概念和一些 LeetCode 相关题目。\n二叉树Tree 是非常常用的数据结构，用来模拟一种分级的树结构。树的每一个 Node 都有它的 value 和若干个 child node。同图的角度来看，树结构其实是非对称的有向图，他拥有 \nN\n\n\n\n\n \n\n nodes 和 \nN-1\n\n\n\n\n\n\n \n \n \n\n edges。Binary Tree 则是对典型的一种树结构，其中的每个节点都拥有至多2个子节点，以 left 和 right 作区分。\n二叉树遍历这部分介绍的内容是二叉树的遍历方法，分别有： Pre-order，In-order 和 Post-oder 三种。这三种名字的由来于三个操作的顺序：遍历左分支，遍历右分支，访问该节点的值。每种方法都可以通过迭代和递归两种方法实现，下面会做展示。\nPreorderPre-oder 顾名思义就是在便利左右分支之前，先访问当前节点的 value，并写入最终返回的数组中。比如从根节点 1 起始，先push到 stack 里面去作为初始化。下面进入循环流程：pop出 stack 最顶端的元素，依次为当前节点 curNode，先将其值写入最红返回的结果，再先后访问其右节点和左节点，并把值push到 stack 里面去。当前循环结束，此时 stack 里面是 curNode 的两个子节点。如此循环下去，直到 stack 为空。\n\n接口和上面的文字描述并结合图片，可以注意到，我们在这个迭代过程中使用 stack 这一数据结构，是FILO的。也就是说在我们采取先右后左的push顺序下，左分支上的内容总是会先被访问。这正符合 root -&gt; left -&gt; right也就是 Pre-order 的顺序。下面的代码块是具体的 Python 实现。\n123456789101112131415161718192021222324252627282930313233class Solution(object):    def preorderTraversal_iter(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        stack, ans = [root], []        while stack:            curNode = stack.pop()            if curNode:                ans.append(curNode.val)                stack.append(curNode.right)                stack.append(curNode.left)        return ans            def _helper(self, node, ans):        '''        This function will Fisrtly go through the left side        '''        if node:            ans.append(node.val)            self._helper(node.left, ans)            self._helper(node.right, ans)            def preorderTraversal(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        ans = []        self._helper(root, ans)        return ans上面的代码片段里，preorderTraversal_iter 是迭代方法，而 preorderTraversal 和 _helper 则共同构成了递归的方法。可以看出迭代的方法相对复杂一些，而递归的方法则更加的 intuitive。其内部的顺序直接就是 root -&gt; left -&gt; right。\nIn-orderIn-oder 是一种相对比较蛋疼的遍历顺序：left -&gt; root -&gt; right。只有在遍历了左分支之后，才开始访问值并写入最终结果。首先初始化空的 stack，进入外层循环，遍历左分支到尽头（只部分是内层循环），这个过程中把节点push入 stack。该过程结束后，pop出 stack 最顶层的节点进行访问写入结果并转向其右节点一侧开始下一次外层循环。如果有节点不是空，则开始内层循环，遍历这个右节点以下部分的左分支，以此类推。\n\n只有在遍历左分支的过程中，才向 stack 里面 push 节点，这样就保证了 left 分支的优先，有在分叉有右分支的时候，先解决右边的这一部分。\n1234567891011121314151617181920212223242526272829303132class Solution(object):    def inorderTraversal_iter(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        stack, ans = [], []        while True:            while root:                stack.append(root)                root = root.left            if not stack:                return ans            node = stack.pop()            ans.append(node.val)            root = node.right            def _helper(self, node, ans):        if node:            self._helper(node.left, ans)            ans.append(node.val)            self._helper(node.right, ans)        def inorderTraversal(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        ans = []        self._helper(root, ans)        return ansPost-orderPost-order 则是在遍历过整个树之后（左侧的节点更优先），再进行值的访问。从根节点 &quot;1&quot; 开始，将stack初始化为 [root]。循环流程：将 stack1最顶端的节点pop出来作为curNode，并访问它的值写入最终结果的stack2里面将它的两个子节点全部push到stack里面，直到分支尽头。\n这里的操作其实和 Pre-order 几乎是一样的，唯一的区别是添加子节点的顺序。我们需要最后的结果是左侧的子节点在前。那么在使用stack1和stack2这样FILO的数据结构时，我们需要左侧子节点在结果stack2顶部，则需要将左侧节点优先于右侧push到stack1里面。这样才能是左侧节点更晚的被转移进stack2，即保持在相对顶部的位置。\n\n1234567891011121314151617181920212223242526272829class Solution(object):    def postorderTraversal_iter(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        stack, ans = [root], []        while stack:            curNode = stack.pop()            if curNode:                stack.append(curNode.left)                stack.append(curNode.right)                ans.append(curNode.val)        return ans[::-1]        def _helper(self, node, ans):        if node:            self._helper(node.left, ans)            self._helper(node.right, ans)            ans.append(node.val)                def postorderTraversal(self, root):        \"\"\"        :type root: TreeNode        :rtype: List[int]        \"\"\"        ans = []        self._helper(root, ans)        return ans二叉树中的递归递归是树结构的一个特性，因此递归是解决树相关问题的有效又常用的方法。对每个递归层级，我们可以只关注该问题针对一个节点并递归地去解决他的子节点。\nTop-downTop-down 意味着，在每一次递归中，我们访问一个节点，得到某些值，并将这些值传递给子节点递归调用的函数。这个方法就与之前的 pre-order 类似。\n12345return specific value for null nodeupdate the answer if needed                      &#x2F;&#x2F; answer &lt;-- paramsleft_ans &#x3D; top_down(root.left, left_params)      &#x2F;&#x2F; left_params &lt;-- root.val, paramsright_ans &#x3D; top_down(root.right, right_params)   &#x2F;&#x2F; right_params &lt;-- root.val, params return the answer if needed                      &#x2F;&#x2F; answer &lt;-- left_ans, right_ans以寻找树的最大深度为例，我们创建一个变量 depth 来记录树的深度。解决问题的函数为 maxDepth(node, depth)。这个函数的伪代码就是：\n12345return if root is nullif root is a leaf node:\tanswer &#x3D; max(answer, depth)         &#x2F;&#x2F; update the answer if neededmaximum_depth(root.left, depth + 1)      &#x2F;&#x2F; call the function recursively for left childmaximum_depth(root.right, depth + 1)     &#x2F;&#x2F; call the function recursively for right child具体的Python实现如下面的代码片段。因为作用域的关系且Python里面是无法规定pass by value还是pass by reference，我们无法直接对answer做直接的修改（Python中对于数值e.g.int、float和一些较为复杂的类e.g.list，它们的引用方式是有一点区别的。观察感觉是前者传值，后者传引用）。因此需要一个global variable，而在类里面可以用一个attribute代替。\n12345678910111213class Solution(object):    def maxDepth(self, root):        self.ans = 0        self._helper(root, 1)        return self.ans        def _helper(self, node, depth):        if not node:            return        if node.left == node.right == None:            self.ans = max(self.ans, depth)        self._helper(node.left, depth+1)        self._helper(node.right, depth+1)Bottom-upBottom-up就是另外一种递归思路了，我们递归地对每一个子节点调用函数，然后通过这些值获得返回给父级节点的结果。这样从最尽头的子节点开始的方式就很类似Post-order。\n1234return specific value for null nodeleft_ans = bottom_up(root.left)          // call function recursively for left childright_ans = bottom_up(root.right)        // call function recursively for right childreturn answers                           // answer &lt;-- left_ans, right_ans, root.val依然用求树的最大深度为例，对于一个根节点，其左节点为l有节点为r。这两个节点，各自递归获得它们子树（subtree）的深度。则对于根节点来讲，深度为x = max(l, r)+1。\n1234return 0 if root is null                 &#x2F;&#x2F; return 0 for null nodeleft_depth &#x3D; maximum_depth(root.left)right_depth &#x3D; maximum_depth(root.right)return max(left_depth, right_depth) + 1  &#x2F;&#x2F; return depth of the subtree rooted at root之所以会出现+1，是因为对于最深处的节点来讲，左右都是 Null，他自己本身这一level的深度其实是1。另外，对于某些只有一侧节点的情况也需要注意一下。下面是具体的Python代码实现，看上去就简洁很多。\n123456789class Solution(object):        def maxDepth(self, root):        \"\"\"        :type root: TreeNode        :rtype: int        \"\"\"        if root == None:            return 0        return max(self.maxDepth(root.left), self.maxDepth(root.right)) + 1总结非常透彻的理解和找到合适的递归方法是很不容易的。需要根据具体情况来决定使用top-down还是bottom-top。（这不是废话吗）\nReferenceYu’s Coding Garden - Tree Traversal\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/hello-i-m-nik-1399233-unsplash.jpg","plink":"https://magi003769.github.io/post/Tree-01/"},{"title":"牛顿法","date":"2019-04-05T07:00:00.000Z","date_formatted":{"ll":"Apr 5, 2019","L":"04/05/2019","MM-DD":"04-05"},"updated":"2020-05-10T22:57:16.000Z","content":"最早听说这个方法是在知乎的一个回答里，大佬手写 sqrt(x) 没有用 binary search 而是用了牛顿法去迭代，把面试官都懵逼了。后面刷题遇到了 69. Sqrt(x) 和 367.Valid Perfect Square，就对这个牛顿法有了一些兴趣，本文大部分是搬运 references 中的东西。\n牛顿-拉弗森法五次及以上多项式方程没有根式解（就是没有像二次方程那样的万能公式），这个是被伽罗瓦用群论做出的最著名的结论。牛顿迭代法就是一个用来解多项式零点的方法。\n\n牛顿法的一个重要基础是：切线是曲线的线性逼近。越靠近方程的解，曲线与切线的差距越小。\n\n基于这个想法，求解曲线的根的时候，要不然咱就研究一下切线的根？于是就发现了下面的一个现象，在曲线上随便找一个点 \n(x_0, f(x_0))\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n (其实应该在根的附近，之所以随便，因为我们并不知道根在哪里)，做一条切线。该切线有一个零点 \nx_1\n\n\n\n\n\n \n \n\n ，并在曲线上 \n(x_1, f(x_1))\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n 重复前面做切线的工作，得到新切线的根 \nx_2\n\n\n\n\n\n \n \n\n。 \n\n随着迭代的继续，这个切线的零点，会逐渐逼近曲线的零点。这就是牛顿-拉弗森法的求近似根过程。已知曲线为 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n，在 \nx_n\n\n\n\n\n\n \n \n\n 处做切线求出 \nx_{n+1}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n。由此利用点斜式，我们很容易得出切线方程 \ng(x)\n\n\n\n\n\n\n\n \n \n \n \n\n\n\ng(x) = f(x_n) + f'(x_n)(x-x_n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n\n于是可由方程 \ng(x)=0\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 得出新的迭代结果 \nx_{n+1}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n\n\ng(x) = 0 \\Rightarrow x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n\n\n\n \n \n\n \n \n\n \n\n\n \n \n \n\n \n \n\n \n\n\n\n\n\n这就是牛顿法迭代的代数表达式，也是后面程序实现的一个基础。\n\n方法的收敛性使用条件从上面的分析来看，牛顿法来自于比较靠谱的直觉，但是这种方法是否总是可以得到足够近似的根呢？下面是牛顿法收敛的一个充分条件\n\n若 \nf\n\n\n\n\n \n\n 二阶可导，那么在待求的零点 \nx\n\n\n\n\n \n\n 周围存在一个区域，只要起始点 \nx_0\n\n\n\n\n\n \n \n\n 位于这个邻近区域内，那么牛顿-拉弗森方法必定收敛。\n\n这个描述其实有点玄学的感觉。因为在迭代的过程中，我们压根儿就不知道这个根在哪里。如果起始点没有选在这个区域，这个方法估计就要gg了。下面就是一些比较特殊的情况，牛顿法并不适用。\n不收敛的情况Case 1：驻点\n这种情况下倒数为零，切线不存在零点，迭代的式子没有意义。\nCase 2：迭代的根越来越远如果我们要求 \nf(x) = x^{\\frac{1}{3}}\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n\n\n\n\n 的根，根据牛顿法的迭代表达式可以得到\n\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{x_n^{\\frac{1}{3}}}{\\frac{1}{3}x_n^{-\\frac{2}{3}}} = -2x_n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n\n\n\n \n \n\n \n \n\n \n\n\n \n \n \n\n \n \n\n \n\n\n\n \n\n \n \n\n \n\n\n\n\n \n\n\n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n\n \n\n\n\n \n \n\n\n\n \n\n\n\n\n \n \n \n\n \n \n\n\n\n很明显，这里的根是0，得到的新的迭代结果，比之前的结果离得更远。\n\nCase 3: 循环震荡不收敛这是一种更加蛋疼的情况，比如求 \nf(x) = |x|^{\\frac{1}{2}}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n\n\n \n \n\n\n\n\n 的根。在迭代过程中 \nx_{n+1} = -x_{n}\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n \n\n\n（这个结果你可以分类讨论得到），就是个非常神奇的现象了。这样迭代结果正负来回跳，想收敛？不存在的。\n\n牛顿-拉弗森法的一些缺陷通过上面的一些分析，我们可以知道牛顿法实际上是存在很大问题的，主要是以下几点：\n\n初值的选择对问题求解挺像太大，可能直接导致解不出来，或者效率低下\n无法得知有几个根，即便知道也不一定能全部得出\n迭代过程中，我们并不知道这次迭代的值和根的误差是多少（压根不知道跟在哪）\n\n利用牛顿法求平方根设函数输入为 \na\n\n\n\n\n \n\n ，函数输出为 sqrt(a)，及下面式子中的 \nx\n\n\n\n\n \n\n。问题可以转化成解方程，然后转化成前面讲解牛顿法的例子：求多项式 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 的零点。\n\nx^2 = a \\Rightarrow f(x)=x^2 - a\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n\n\n于是我们可以得到 \nf'(x) = 2x\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n，并待入之前牛顿法迭代的式子中得到\n\nx_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)} = x_{n} - \\frac{x_n^2 - n}{2x_n} = \\left(x_n+\\frac{a}{x_n}\\right)/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n\n\n\n \n \n\n \n \n\n \n\n\n \n \n \n\n \n \n\n \n\n\n\n \n\n \n \n\n \n\n\n\n\n \n \n \n \n \n\n\n \n\n \n \n\n\n\n\n \n\n \n\n \n \n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n这里直接用输入作为一个起始点，这样牛顿法可以收敛。根据题目要求，只需要找到平方根的整数部分就可以，那么就在这个根的平方刚刚小于输入时停止迭代，输出结果。\n123456789class Solution:    def mySqrt(self, x: int) -&gt; int:        '''        Newton's Method        '''        r = x        while r * r &gt; x:            r = (r + x / r) // 2        return int(r)References如何通俗易懂地讲解牛顿迭代法？\n如何用牛顿法求一个数的平方根\n知乎 - 牛顿法怎么理解？还有什么其他方法进行高次方程的数值求解？\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Mathematics/HD_blackboard.jpg","plink":"https://magi003769.github.io/post/牛顿法/"},{"title":"Array and String","date":"2019-03-25T07:00:00.000Z","date_formatted":{"ll":"Mar 25, 2019","L":"03/25/2019","MM-DD":"03-25"},"updated":"2019-05-26T23:09:08.000Z","content":"LeetCode 上面 Array and String 专题的搬运。其实这两个没什么可讲的，主要就是记一下这个Card下面比较难刷的题目，和偶然发现的一些坑。\nArray刷这部分的题目时，遇到问题比较多的是2D-array的题目。下面两个就是做题过程中花了很长时间的两道题。\n498. Diagonal Traverse\n498. Diagonal Traverse 题目要求我们使用上图所示的模式来遍历给定的二维数组\n1234567891011121314151617181920212223242526272829303132333435class Solution(object):    def findDiagonalOrder(self, matrix):        \"\"\"        :type matrix: List[List[int]]        :rtype: List[int]        \"\"\"        if matrix == [] or matrix[0] == []:            return []        row, col = len(matrix), len(matrix[0])        count = 0        ans = [0] * (row * col)        i, j = 0, 0        #direct = [-1, 1]        for n in range(len(ans)):            ans[n] = matrix[i][j]            up = (i+j) % 2 == 0            if up:                if j == col - 1:                    i += 1                elif i == 0:                    j += 1                else:                    i -= 1                    j += 1            else:                if i == row - 1:                    j += 1                elif j == 0:                    i += 1                else:                    i += 1                    j -= 1            #print(ans, (i, j))            count += 1        return ans59. Spiral Matrix II59. Spiral Matrix II 这道题依然是一个2D-Array的题目。解题的思路在与4个动态的boundary和周期变化的方向。\n12345678910111213141516171819202122232425262728293031323334353637class Solution(object):    def generateMatrix(self, n):        \"\"\"        :type n: int        :rtype: List[List[int]]        \"\"\"        ans = [[0] * n for i in range(n)]        nums = [i for i in range(1, n*n+1)]        # constrain of 4 sides (index)        left, up = 0, 0        right, bottom = n - 1, n - 1        # direction        direct = 0        idx = 0        while idx &lt; n*n:            if direct == 0:                for i in range(left, right+1):                    ans[up][i] = nums[idx]                    idx += 1                up += 1            elif direct == 1:                for i in range(up, bottom+1):                    ans[i][right] = nums[idx]                    idx += 1                right -= 1            elif direct == 2:                for i in range(right, left-1, -1):                    ans[bottom][i] = nums[idx]                    idx += 1                bottom -= 1            elif direct == 3:                for i in range(bottom, up-1, -1):                    ans[i][left] = nums[idx]                    idx += 1                left += 1            direct = (direct+1) % 4        return ans这里插播一条关于Python的坑。这个solution的开头我们初始化要返回的矩阵时使用ans = [[0] * n for i in range(n)] ，不能用ans=[[0]*n]*n。\nTwo-PointerTwo-Pointer是一种十分常用的方法。看上去很简单，但有的时候使用得当的话会有非常好的效果，一个经典的例子就是对sorted的序列做搜索，也就是使用binary search。下面是几个非常之有趣的two-pointer方法。\n209. Minimum Size Subarray Sum12345678910111213141516class Solution(object):    def minSubArrayLen(self, s, nums):        \"\"\"        :type s: int        :type nums: List[int]        :rtype: int        \"\"\"        sum_ = head = 0        ans = len(nums) + 1        for tail in range(len(nums)):            sum_ += nums[tail]            while sum_ &gt;= s:                ans = min(ans, tail - head + 1)                sum_ -= nums[head]                head += 1        return ans if ans &lt;= len(nums) else 0StringString也是很常用的数据结构，程序员基本也无法避免处理字符串对象。大部分的高级语言里面都有相关的实现，使用起来其实还是挺方便的。Python里面有一个值得注意的点，它的str对象支持下标访问，但是不支持利用下标去单独对某一字符或者片段赋值。\n205. Isomorphic Strings这道题205. Isomorphic Strings算是一个字符串的match问题，890. Find and Replace Pattern可以用几乎一模一样的方法解决。题目要求比较两个字符串s和t是否有相同的pattern。\n123456789101112131415161718192021class Solution(object):    def isIsomorphic(self, s, t):        \"\"\"        :type s: str        :type t: str        :rtype: bool        \"\"\"        '''        map_s, map_t = &#123;&#125;, &#123;&#125;        code_s, code_t = '', ''        '''        def check(s, t):            mapping = &#123;&#125;            for i in range(len(s)):                try:                    if t[i] != mapping[s[i]]:                        return False                except KeyError:                    mapping[s[i]] = t[i]            return True        return check(s, t) and check(t, s)一个简单人直接想法就是使用dict建立起两个字符串的对应关系。但是进行但相比较的时候出现了错误：s = &quot;ab&quot;与t = &quot;aa&quot;。从s映射到t的单向比较，会使得&#39;a&#39;、&#39;b&#39;均对应&#39;a&#39;而没有后续的字符进行比较发现这个错误。再加一个反相比较，就可以了。\n796. Rotate String796. Rotate String这道题虽然是个easy但是很有趣，有点脑筋急转弯。\n12345678class Solution(object):    def rotateString(self, A, B):        \"\"\"        :type A: str        :type B: str        :rtype: bool        \"\"\"        return len(A) == len(B) and B in A+A","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/home-3.jpg","plink":"https://magi003769.github.io/post/Array_String/"},{"title":"Batch Normalization：定义与实现","date":"2018-10-01T07:00:00.000Z","date_formatted":{"ll":"Oct 1, 2018","L":"10/01/2018","MM-DD":"10-01"},"updated":"2020-05-10T23:04:40.000Z","content":"现在BN基本上是深度学习或者说CNN的标配，他可以帮助提升训练速度，提升模型的效果。强烈推荐这两篇专栏文章：Batch Normalization原理与实战、详解深度学习中的Normalization，BN/LN/WN。本篇也基本就是这两篇的搬运和拼凑。\nBackground and Why BN?深度学习的困扰与Internal Covariant Shift我们知道随着Deep Learning的发展，网络变得越来越深、越来越复杂。这样输入信息或者浅层网络的输出稍有变动，就会随着后续网络的深入而被不断放大，深层的输入变化就会变得非常剧烈。因此，我们需要尝试不同的参数配置甚至精心设计每一层的参数初始化，才能达到理想的效果。然而这个过程是极为困难和复杂的，这也是深度网络难以训练的一个重要原因：层与层之间高度的关联和耦合性。深层网络的参数需要不断改变去适应浅层输入的变化，哪怕这些变化十分微小。BN的原作者把这个现象称作：Internal Covariant Shift，并起将其定义为：\n\n在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化\n\nInternal Covariant Shift带来的问题\n深层的网络需要不断调整来适应输入数据分布的变化，这会导致网络学习速度的下降。\n\n网络输入的变化被放大，深层的输入趋向于更小或更大，这样输入极易进入饱和区\n\n\n第一条问题我们已经在之前讨论了，第二条主要讲的是一个梯度消失的问题。当的那个神经网络采用saturated activation function的时候，例如sigmoid和tanh，趋向极端的输入变化会使得模型训练进入饱和区。这些区域的梯度为0，反向传播无法更新参数，整个模型的学习也就陷入停滞。\n\n对于激活函数的梯度饱和问题，其实有两种思路：(1) 使用非饱和的激活函数，比如人见人爱花见花开的ReLU，他可以一定程度上解决进入饱和区问题。(2) 另一种就是尽可能使输入落在不饱和的区域。如上图所示，BN就属于这种思路的一个解决方案。\nBatch NormalizationBN定义与算法步骤要使得数据都落在指定范围内或者尽可能符合某个指定的分布，比较常见的思路是使用白化（whitening）。它将数据投射到一个标准正态上，均值为0方差为1。白化的基本公式如下所示，非常基础\n\n\\hat{x_i} = \\frac{x_i - \\mu}{\\sigma}\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n \n \n\n \n\n\n\n\n其目的主要有两个：(1) 使得输入的特征符合统一分布（同均值方差），(2) 去除特征之间的相关性。理论上来说，这里实际上已经可以解决之前说的Internal Covariant Shift的问题了，但实际上它也存在两个问题：(1) 计算成本相当高，(2) 当输入被映射到标准正态上之后，其表征能力被很大程度的削弱了。后者显然是一个更加棘手的问题，也是BN和白化的最大的区别。\n下面是BN的伪代码，可以看到我们先做了简单的白化操作得到\n\\hat{x_i}\n\n\n\n\n\n\n \n \n \n\n。为了恢复数据的表征能力，就索性在增加一个线性操作，引入参数\n\\gamma\n\n\n\n\n \n\n (re-scale factor)和\n\\beta\n\n\n\n\n \n\n (re-shift factor)，这两个参数都是可训练参数。\n\n推测阶段的BN在之前一节我们已经讨论了BN的实际计算和作用，这些都是基于mini-batch来做的，每个batch里面都包含很多样本。这样我们根据当前batch计算出的均值和方差偏差是比较小的。然而在推测（inference）或者说测试阶段，我们的batch size很小甚至是1，此时再用同样方法计算出的结果一定是偏估计。那我们应该如何计算呢？\n\n原文提供的方法是在我们利用诸多mini-batch的过程中，我们保留每一组的均值和方差：\n\\mu_B\n\n\n\n\n\n \n \n\n 和 \n\\sigma^2_B\n\n\n\n\n\n\n \n \n \n\n，并对他们取期望来作为测试阶段所使用的均值 \n\\mu_{test}\n\n\n\n\n\n\n\n \n\n \n \n \n \n\n\n 和方差 \n\\sigma^2_{test}\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n，以实现无偏估计。这是一个非常关键的地方，会直接影响BN的效果。由此我们可以得出：BN在训练和测试阶段的操作差别很大。\n\n可训练参数 \n\\gamma\n\n\n\n\n \n\n 和 \n\\beta\n\n\n\n\n \n\n 在训练阶段会更新，测试阶段固定\n训练阶段会记录mini-batch的均值方差，从而得到样本整体的估计，并在过程中不断更新\n\n在具体的实现中，我们是利用Exponential Moving Average在训练中对\n\\mu_{test}\n\n\n\n\n\n\n\n \n\n \n \n \n \n\n\n 和方差 \n\\sigma^2_{test}\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n进行更新的方法。\nBN的优势在哪里（1）BN是的网络为一层的分布相对稳定，加快模型学习速度\nBN通过对输入进行标准化再进行线性操作增强表征，使得每层网络的输入数据的均值和方差在一定的范围内。这样一定程度上是对各个层之间的解耦，弱化层与层之间的联系，深层的网络不需要不断去适应浅层输入的变化，增加学习速度。\n（2）弱化模型对参数的敏感程度，是学习过程更加稳定\n之前提到随着网络的深入，微小的变化会逐渐被放大。如果我们使用一个scale factor \na\n\n\n\n\n \n\n 来讨论输入的数值对后面的影响（推导过程见链接），会发现BN可以抹去权值缩放带来的影响。这样就抑制了浅层参数变化带来的后续巨大干扰，让训练过程更稳定。\n（3）BN允许使用饱和激活函数：signoid、tanh\n我觉得这一点其实也没什么了，现在ReLU基本已经变成了标配。之前也讨论过了，BN可以让输入的值尽量落在非饱和区，解决梯度消失的问题。\n（4）BN具有一定的正则化效果\n这一点是因为我们利用mini-batch去估计整个训练数据集的均值和方差。尽管每一个mini-batch都是随机抽样得到的，但每一次迭代都是有偏差估计，这也就是这个随机噪声的来源，类似于Dropout随即关闭神经元的正则化方式。虽然之前有人说BN和Dropout有其一即可，但是最近似乎有新的研究证明而这其实是可以共存的。\nTensorFlow 中 BN 的接口tf.nn.batch_normalization这个接口的文档可以在后面的链接查到：tf.nn.batch_normalization，是一个非常底层的接口。通过阅读它的源代码我们可以发现他实际上只进行了标准正态投影以及最后线性变化的计算，并不涉及均值方差的更新，如果单纯的使用这个接口，在测试时会出现问题。因此需要我们自己管理它们的更新。具体的代码如下（其实我也是从stackoverflow上面搬的）：\n12345678910111213141516171819def BN(x, phase_train, name):    x_shape = x.get_shape().as_list()    param_shape = x_shape[-1:]    batch_mean, batch_var = tf.nn.moments(x, axes=[0 ,1, 2])    beta = tf.get_variable(name+'/beta', param_shape, tf.float32,                            initializer=tf.constant_initializer(0.0, tf.float32))    gamma = tf.get_variable(name+'/gamma', param_shape, tf.float32,                            initializer=tf.constant_initializer(1.0, tf.float32))    ema = tf.train.ExponentialMovingAverage(decay=0.99)    def mean_var_with_update():        ema_apply_op = ema.apply([batch_mean, batch_var])        with tf.control_dependencies([ema_apply_op]):            return tf.identity(batch_mean), tf.identity(batch_var)    mean, var = tf.cond(phase_train,                        mean_var_with_update,                        lambda: (ema.average(batch_mean), ema.average(batch_var)))    return tf.nn.batch_normalization(x, mean, var, beta, gamma, BN_EPSILON)tf.layers.batch_normalizationtf.layers.batch_normalization是一个相对高级的接口，除了算符部分要求的计算，在调用它的时候它也会自己管理用于test的均值和方差的更新。\n\nNote: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be executed alongside the train_op. Also, be sure to add any batch_normalization ops before getting the update_ops collection. Otherwise, update_ops will be empty, and training/inference will not work properly. For example:\n\n接口本身会调用一些UPDATE_OPS，在训练的过程中我们需要把他们和优化loss的optimizer一起拎出来，只有这样才能对相应的值进行更新，和反向传导更新参数一个道理：不执行这个op就不更新。\n1234567x_norm = tf.layers.batch_normalization(x, training=training)# ...update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)train_op = optimizer.minimize(loss)train_op = tf.group([train_op, update_ops])Summary总的来说，BN就是通过标准化保证输入的均值和方差在一定的范围内，减少网络的Internal Covariate Shift。让每一层的值在更加有效的范围内传递，降低网络对参数和激活函数等设置的依赖。\nReferenceWhat is right batch normalization function in Tensorflow?\nUnderstanding the backward pass through Batch Normalization Layer\nImplementing Batch Normalization in Tensorflow\nDeeper Understanding of Batch Normalization with Interactive Code in Tensorflow (Manual Back Propagation)\nPitfalls of Batch Norm in TensorFlow and Sanity Checks for Training Networks\nHow could I use batch normalization in TensorFlow?\n知乎 - 什么是批标准化\nBatch Normalization原理与实战\n知乎 - 深度学习中 Batch Normalization为什么效果好？\nUsing TensorFlow’s Batch Normalization Correctly\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_BN.png","plink":"https://magi003769.github.io/post/Batch norm/"},{"title":"经典网络结构——DenseNet","date":"2018-09-07T07:00:00.000Z","date_formatted":{"ll":"Sep 7, 2018","L":"09/07/2018","MM-DD":"09-07"},"updated":"2019-05-07T06:28:06.000Z","content":"DenseNet在引入 skip connection 的 ResNet 在训练更深层的网络取得成功之后，一种更加大胆的网络结构被提出了，这就是 DenseNet。相比于一般的残差网络每两层或者 bottleneck 中每三层做一次连接，DenseNet 中的模块则是将当前卷积层的输出与之前所有卷积层的输出拼接，并作为下一卷积层的输入。下图就说明了这样一个包含四个卷积层以及多个 skip connection 的模块。假设每个模块中有 \nL\n\n\n\n\n \n\n 个卷积层，则一共有 \n\\frac{L(L+1)}{2}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n\n\n 个跨层连接。\n\n\nFor each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. \n\n整体的模型则包含了多个这样的 DenseBlock，以及中间由 \n1\\times1\n\n\n\n\n\n \n \n \n\n 卷积 （前面还有BN）和池化层组成的 Transition Layer。在这些模块的内部，通道数量是变化的，而经过 dense block 不会引起特征图尺寸的变化(feature-map sizes match within each block)。block 中每一个 layer 的输出空间都是固定的，在文中被称为 growth rate，记作 \nk\n\n\n\n\n \n\n。\n假设一个有着 \nL\n\n\n\n\n \n\n 个卷积层的 dense block 输入特征图维度为 \nW \\times H \\times C\n\n\n\n\n\n\n\n \n \n \n \n \n\n，那么这个模块之后，输出特征图的通道数就会变成 \nC+kL\n\n\n\n\n\n\n\n \n \n \n \n\n。这样输出的通道数就增加了，如果这样保持下去，最后的输出空间维度肯定是一个非常大的数字，而且中间卷积层参数也会因为输出维度的增加而增加。因此，为了更有效地减少参数，作者引入了被称为 compression factor 的超参数，记作 \n\\theta\n\n\n\n\n \n\n。它限制了 transition layer 中卷积层的输出维度，进而相对减少后续积层所需的参数。取值上 \n0&lt;\\theta\\leq1\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，当 \n\\theta=1\n\n\n\n\n\n\n \n \n \n\n 即输出空间维度在 transition layer 中不改变时，就是最原始的 DenseNet，其他取值的时候则被称为 DenseNet-C\n\n特征的复用\nDenseNet 的几个有点优点\n输出与输入有更直接的连接，一定程度上结局了梯度消失的问题\n通过通道连接多个特征图，加强了特征的传导和重复利用\n减少了参数的使用，在较少参数的情况下保证效果\n\n论文中作者认为卷积神经网络是一种与状态 (state) 有关的算法，传统的模型是将前一层的状态传递到一层中并进行处理产生新的状态。而这个过程中，有一些有用的信息并没有得到完整的保存。ResNet 正式通过 skip connection 解决了这一问题。不过因为其每一层都有各自独立的权值且输出特征图的宽度也随着深度增加，这样 ResNet 的参数其实是非常多的。作者设计出 DenseNet 的结构也是为了能够减少参数。\n它最大的一个特点就是将不同深度的特征图沿着通道方将进行拼接，block 中卷积层的输出空间维度都是相同的（growth rate），这样一定程度上控制了参数随深度不断增加的问题。而经过几层的之后，最终经过叠加之后的输出特征维度也是相对较高的。\n实现的注意点目前还在实现这个网络结构，中间遇到了一些让人比较困惑的地方，在这里写出来。代码等到过几天写好了训练完能有不错的结果再放。\n论文中的一个卷积操作包含了三部分（BN-ReLU-Conv2d），这个顺序似乎和一般的不太一样。因此，最后一个 dense block 的输出后面是否要加BN和激活函数呢？目前的一种解释是，我们在每个卷积层之前都拼接了新的特征，所以需要从新做 BN 来进行平衡。（那为什么 Resnet 不用呢，虽然是做加和）\n除了针对 ImageNet 的模型以外，其他数据集的模型都是有3个 dense block，每个模块卷积层数字相同。\n除了针对 ImageNet 的模型以外，其他数据集的模型在进入 dense block 之前只经过一个卷积。\n注意 compression factor， 有一部分开源的实现中直接使用 growth rate 作为 transition layer 的输出维度。这样明显是错误的。每经过一次 dense block 好不容易提取了这么多维度特征，又强行压缩这么多回去。\nReferencearXiv - Densenet\nCVPR 2017最佳论文作者解读：DenseNet 的“what”、“why”和“how”\nGitHub - taki0112/Densenet-Tensorflow\n","plink":"https://magi003769.github.io/post/DenseNet/"},{"title":"经典网络结构——残差网络","date":"2018-09-01T07:00:00.000Z","date_formatted":{"ll":"Sep 1, 2018","L":"09/01/2018","MM-DD":"09-01"},"updated":"2019-05-07T06:08:40.000Z","content":"Residual Learning从2012年的 AlexNet 开始，深度学习模型在计算机视觉中得到了广泛的应用。2014的 GoogleNet 和牛津大学视觉组的 VGG，也都是非常成功的深度学习模型。这些模型都利用了深度卷积网络，后两者在深度和复杂度上的提升使得其性能在 AlexNet 的基础上又提升了数个百分点。在这样的背景下，就产生了一个假设：\n\nDeeper networks have better performance than its shallower counterpart.\n\n然而，诸多试验表明单纯的堆叠更多的隐层来扩展深度并不能使网络的表现更好，甚至出现了退化（degradation）的情况。ReLU和Batch normalization一定程度缓解梯度爆炸和梯度消失的问题，因此这种情况比非完全由梯度问题导致。实验中Train error的上升说明，并不是过拟合带来的问题。那么，怎样才能让网络更深，而不损失性能呢？如果在一个shallow的网络之上增加更多的层，而这些层均为单位映射（identity mapping）或是增加了些许扰动的单位映射，那么这个更深层的网络在表现上应至少不会比原来差。\n\n\nOriginal Mapping: \n\\mathcal{H}(\\mathbf{x})= \\mathcal{F}(\\mathbf{x}) + 1\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\nResidual Mapping: \n\\mathcal{F}(\\mathbf{x}) =  \\mathcal{H}(\\mathbf{x}) -1\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n其中，\n\\mathcal{H}(\\mathbf{x})\n\n\n\n\n\n\n\n \n \n \n \n\n 是整个residual block的映射，也被称为original mapping。而\n\\mathcal{F}(\\mathbf{x}) \n\n\n\n\n\n\n\n \n \n \n \n\n 是在plain网络之上增加的层的映射。\n\\mathcal{H}(\\mathbf{x})\n\n\n\n\n\n\n\n \n \n \n \n\n 才是我们想要得到的映射。而我们实际调参优化的过程是针对隐层的，也就是拟合一个理想的\n\\mathcal{F}(\\mathbf{x}) \n\n\n\n\n\n\n\n \n \n \n \n\n 的过程。\n\nHypothesis：相较于拟合一个\n\\mathcal{H}(\\mathbf{x})\n\n\n\n\n\n\n\n \n \n \n \n\n ，拟合残差映射更加容易。\n\nResNet基于上面的残差学习，这才有了 ResNet。这是一个深的多的模型，被广泛地应用在许多 CV 任务中。不同的模型在描述时都会使用不同的属于去描述结构，所以还是有点烦人的。一个比较通用的描述是这样：每一个 skip connection 对应一个残差单元（residual unit），多个连续的残差单元组成一个 block。这里的 block 经常和前面描述中的 residual building block 搞混。\n\nBottleneck\n通常的 ResNet Building Block 都是连续两个 \n3\\times 3\n\n\n\n\n\n \n \n \n\n 卷积，但是论文提出了更具有实践性Bottelneck。 之所以称为 Bottleneck，原因主要在于特征图的输出空间的维度。左边普通的 block 维度始终不变，而右边则是有维度变化的 256--&gt;64--&gt;64--&gt;256 ，中间维度会缩小。这一结构的使用可以在参数减少和复杂度几乎不变的情况下达到几乎同等的效果。\nReferenceUnderstand deep residual learning\n知乎：Deep Residual Network 深度残差网络\n知乎：到底 ResNet 解决了什么问题呢？\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/ryoji-iwata-697773-unsplash.jpg","plink":"https://magi003769.github.io/post/ResNet/"},{"title":"图像语义分割入门：FCN与U-Net","date":"2018-08-17T07:00:00.000Z","date_formatted":{"ll":"Aug 17, 2018","L":"08/17/2018","MM-DD":"08-17"},"updated":"2020-05-10T23:12:24.000Z","content":"实习的第二天，台风中的七夕，承受着暴雨和暴击，开始读 paper。因为任务大致是检测出影像中的病灶区域，所以大致的方向就是语义分割这一块。老板说让读一读 U-Net 相关的的论文来了解一下，今天权当是语义分割入个门吧。\nFCN\n在 U-Net 之前，就已经有了 FCN (Fully convolutional networks) 用于图像的语义分割领域。所以，U-Net也多少受到了 FCN 工作的一些启发。因此，在讨论U-Net之前，先对 FCN 有一个简单的了解。\n从结构上可以看出，FCN的大部分结构还是与一般进行分类的 CNN 网络差别不大，都是数个卷积层 + 一个池化层的组合构成不同的 convolutional stage，从而产生多级的特征图。\n不同的地方在于后面的输出部分。一般用于分类的 CNN 模型，都是学习到 \n1\\times1\\times N\n\n\n\n\n\n\n \n \n \n \n \n\n 的特征图输出，再将其延展（flatten）成一维的特征向量，通过若干全连接层和最后的 Softmax 输出一个置信向量进行分类。而 FCN 则没有延展的过程，并将尺寸较小的特征图坐上采样或者将不同的特征图融合后上采样，来输出分割的结果。因此，上面的框图结构是并不特别准确的。通道数为21的最小特征图到最终的分割预测，中间其实经过了如下图所示的过程，其实还是有点复杂的。\n\n上采样可以是插值 (Interpolation) 也可以是反卷积 (Deconvolution)，FCN使用的则是反卷积。它实际上也是一种卷积操作，只不过会根据输出尺寸、kernel 的大小以及步长等来改变 padding 部分的大小。个人感觉（我乱讲的）这种情况实际上引入了一定的噪声，并且会有一些信息损失。从下面作者给出的效果来说，融合了更多的特征图的预测效果更好。\n\nU-Net分割模型我们大概都可以理解为一个 Encoder-Decoder 结果，先将原始图片中的特征进行编码，在想这些特征做解码，解码为分割结果。编码的通常是尺寸逐渐收缩的卷积层和池化层，而解码部分则是翻卷积或者有些简单粗暴的会直接上采样。这一点上 FCN 和 U-Net 都是一样的。U-Net 则是引入了一个类似 skip connection 的结构。\nU-Net 网络结构\nU-Net 的卷积层还是很常见的那种：\n3\\times3\n\n\n\n\n\n \n \n \n\n 卷积 + ReLU。每一个 convolutional stage 之间都是步长为2的 \n2\\times2\n\n\n\n\n\n \n \n \n\n 卷积。作者在文中专门提到了，要非常认真的选择输入的尺寸，并且保证每一次 pooling 层的输入尺寸都是偶数，以保证分割的准确。\n除此之外，这个网络另一个有点神奇的地方就是：输入和输出的尺寸竟然是不一样的。 这一点也非常值得注意，原图和目标分割结果应该都适合输出尺寸一致的。所以在输入时我们需要对图片进行预处理，这样就出现了一个叫 Overlap-tile的方法。\n解决的问题可以分割（localization）\n需要相对少的数据训练，仍能达到不错的效果\n对目标部分形变的鲁棒\n分离相互接触的区域（cell segmentation）\n一些策略Overlap-tile Strategy: 在补全输入图像的时候，对边缘 padding 时间上是做镜像。\nData augmentation: 增加数据量，模拟现实中的病灶形变\nWeighted Loss：相接触的部分中间的背景像素在 loss 中有更大的权重\nReference图像语义分割入门+FCN/U-Net网络解析\n图像语义分割之FCN和CRF\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_segmentation.jpeg","plink":"https://magi003769.github.io/post/U-Net/"},{"title":"机器学习模型——决策树","date":"2018-07-25T07:00:00.000Z","date_formatted":{"ll":"Jul 25, 2018","L":"07/25/2018","MM-DD":"07-25"},"updated":"2020-05-10T22:55:16.000Z","content":"这一篇post来讲一讲决策树，内容杂糅了李航的《统计学习方法》和周志华的“西瓜书”。内容包括决策树的基本算法、信息熵和信息增益、简单的算法实现。\n基本算法决策树是一种基本的分类与回归方法（对决策树也可以用来作回归，本篇主要讨论它的分类应用）。它可以认为是if-then的规则集合，也可以认为是定义在特征空间和类空间上的条件概率分布。决策树的学习通常包括三个步骤：特征选择、决策树生成和决策树剪枝。\n本篇暂时不涉及剪枝部分，这个坑以后再填。主要讲一下特征选择时利用的信息增益以及决策树的建立。下面的这段伪代码说明了决策树的建立过程，通过比较划分前后的信息增益来选择最佳的划分方式，并一递归的方式建立整个树。\n\n划分选择从上面的基本算法可以看出，决策树学习的关键是第8行：如何选择最优的分类属性。最优的分类属性要使得决策树的分直节点所包含的样本尽可能属于同一类别，即“纯度”越来越高。而信息论中的信息上可以描述样本集的uncertainty，所以可以作为衡量属性决策能力的一个标准。\n信息增益 (Information Gain)假设离散属性\na\n\n\n\n\n \n\n有\nV\n\n\n\n\n \n\n个可取值\n\\{a^1, a^2, ... , a^V\\}\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n, 若使用\na\n\n\n\n\n \n\n对样本集合\nD\n\n\n\n\n \n\n进行划分，则会有\nV\n\n\n\n\n \n\n个分支节点，其中第\nv\n\n\n\n\n \n\n个节点包含了\nD\n\n\n\n\n \n\n中属性a取值\na^v\n\n\n\n\n\n \n \n\n的所有样本，记为\nD^v\n\n\n\n\n\n \n \n\n。这样我们可以得到样本结合\nD\n\n\n\n\n \n\n在经过属性\na\n\n\n\n\n \n\n划分之后的信息增益  (Information Gain)为\n\nGain(D, a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n\n\n\n \n\n \n \n\n \n\n\n \n \n \n\n\n\n \n \n \n \n\n \n \n\n \n\n\n从计算式上理解，整个数据集在经过属性\na\n\n\n\n\n \n\n的划分后成为\nV\n\n\n\n\n \n\n个子集，并对这些子集求一个加权平均的熵，就成为了我们衡量数据集混乱程度的新标准。这样就定义了一个划分前和划分后整体样本集的信息熵变化，一般来说，这个增益越大，表示经过属性\na\n\n\n\n\n \n\n的划分纯度提升越大，效果越好。ID3决策树算法就是用这一标准来选择属性的优先级。\n然而信息增益具有一定的偏好，其更倾向于选择划分\nV\n\n\n\n\n \n\n取值更大，即分类更为详细的属性为优先。若每个样本经过划分后都属于不同的分支节点（按编号划分），则其信息增益一般大于其他属性，这显然是不科学的。\n信息增益率 (Information Gain Ratio)为了避免上述信息增益偏好所带来的不利影响，C4.5决策树算法使用信息增益率 (Information Gain Ratio)作为判断优先的准则。\n\nGainRatio(D, a)  = \\frac{Gain(D,a)}{IV(a)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n\n\n\nIV(a)=-\\sum_{v=1}{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n\n\n\n\n \n\n \n \n\n \n\n\n \n \n \n\n\n\n \n \n\n \n \n\n\n\n\n\n \n\n \n \n\n \n\n\n \n \n \n\n\n\n\n\n\nIV(a)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n称为属性\na\n\n\n\n\n \n\n的“固有值” (intrinsic value)。属性\na\n\n\n\n\n \n\n的可能取值越多（即\nV\n\n\n\n\n \n\n越大），则\nIV(a)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n的值通常越大。需要注意的是，增益率准则对所取值较少的属性有所偏好，因此C4.5并不直接使用增益率最大的属性作为最优，而是使用一种启发式：先从候选属性中找出增益高于平均水平的属性，再从中选取增益率最高的。\n基尼指数CART决策树使用基尼指数来选择划分属性，与熵的概念类似，基尼指数也是用来描述当前集合的“混乱程度”或者“纯度”的标准（英文里面一般用impurity）。对于给定的数据集合\n\\mathcal{D}\n\n\n\n\n \n\n，其标签集合为\n\\mathcal{Y}\n\n\n\n\n \n\n，共包含\n|\\mathcal{Y}|\n\n\n\n\n\n \n \n \n\n类标签，则其基尼指数为\n\nGini(D) = \\sum_{k=1}^{|\\mathcal{Y}|}\\sum_{k'\\neq k}p_k p_{k'} = 1 - \\sum_{k=1}^{|\\mathcal{Y}|} p_k^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n\n \n\n \n \n \n \n\n\n\n \n \n\n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n\n \n \n \n\n\n\n直观上来讲，基尼系数反映了从数据集\n\\mathcal{D}\n\n\n\n\n \n\n中随机抽取两个样本期类别标记是否一致的概率。因此，基尼指数越小，数据集的纯度越高。接着我们讨论在使用某个属性\na\n\n\n\n\n \n\n对数据集进行划分后的基尼指数\n\nGini(D, a) = \\sum_{v=1}^{V} \\frac{|D^v|}{|D|}Gini(D^v)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n\n\n\n \n\n \n \n\n \n\n\n \n \n \n\n\n\n \n \n \n \n \n\n \n \n\n \n\n\n简单的决策树实现这里的决策树实现参考Google在YouTube上的一个教程。整个实现并不是很好地OOP设计，且不涉及剪枝。首先先明确，这个简单的决策树实现中，会按照某一维特征的取值来进行划分，因此每次划分结果只有两个分支。每一次划分都像是在提出一个问题，因为我们就设计了一个Question类来进行对数据归类：column定义是哪一位度的特征，value是该维度上进行数据划分的取值。函数partition就会以一个Question对象为参数进行数据的分割。\n12345678910111213141516171819202122class Question(object):        def __init__(self, column, value):        self.column = column        self.value = value        def match(self, sample):        val = sample[self.column]        return val &gt;= self.value        def __repr__(self):        condition = \"&gt;=\"        return \"Is %s %s %s?\"%(header[self.column], condition, str(self.value))def partition(samples, question):    true_part, false_part = [], []    for one in samples:        if question.match(one):            true_part.append(one)        else:            false_part.append(one)    return np.array(true_part), np.array(false_part)对于每个分割节点，我们需要找到可以获得最大信息增益的分割方式。我们利用gini函数来获得当前数据基尼指数，并在一组(col, val)的分割组合之后，使用info_gain来计算信息增益来实现最有效的分割。\n123456789101112131415161718def find_best_split(data):    best_gain = 0    best_question = None    # get gini index before split    current = gini(data)    for col in range(len(data[0]) - 1):        values = set(data[:, col])        for val in values:            question = Question(col, val)            true_part, false_part = partition(data, question)            # No split, just ahead to next question iteration            if len(true_part) == 0 or len(false_part) == 0:                continue            gain = info_gain(true_part, false_part, current)            if gain &gt;= best_gain:                best_gain, best_question = gain, question    # print(best_gain, best_question)    return best_gain, best_question我们将每一次划分定义为内部的DecisionNode，主要包含这一节点用于划分的Question对象，和其左右分支true_branch和false_branch。不能再进行划分的这部分定义为Leaf，包含训练集的划分结果，在测试阶段会返回这个predictions属性作为预测结果。\n1234567891011121314151617class DecisionNode(object):    def __init__(self, question, true_branch, false_branch):        self.question = question        self.true_branch = true_branch        self.false_branch = false_branchclass Leaf(object):    def __init__(self, samples):        self.predictions = class_counts(samples)        def predict_prob(self):        \"\"\"A nicer way to print the predictions at a leaf.\"\"\"        total = sum(self.predictions.values()) * 1.0        probs = &#123;&#125;        for lbl in self.predictions.keys():            probs[lbl] = str(int(self.predictions[lbl] / total * 100)) + \"%\"        return probs和一般的树一样，树的构建是通过递归的方式。\n12345678def build_tree(samples):    gain, question = find_best_split(samples)    if gain == 0:        return Leaf(samples)    true_samples, false_samples = partition(samples, question)    true_branch = build_tree(true_samples)    false_branch = build_tree(false_samples)    return DecisionNode(question, true_branch, false_branch)Reference知乎 - 西瓜书配套\n简书 - 决策树\n决策树：Pseudocode\nYouTube - Let’s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes\nTutorial Code\nKaggle - Decision Tree from Scratch\nDecision-tree-post-pruning\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_Decision_tree.png","plink":"https://magi003769.github.io/post/决策树/"},{"title":"机器学习模型——朴素贝叶斯法","date":"2018-07-15T07:00:00.000Z","date_formatted":{"ll":"Jul 15, 2018","L":"07/15/2018","MM-DD":"07-15"},"updated":"2020-05-10T23:00:12.000Z","content":"\nToo young. Too simple. Sometimes Naive. —— The Elder\n\n朴素贝叶斯的几个相关概念和公式条件概率、联合分布与贝叶斯定理讨论贝叶斯定理之前，我们先来确定一下条件概率：条件概率可由联合分布以及作为条件的随机变量的分布决定。所以，已知条件对调时，条件概率的计算也将随之发生变化。\n\nP(A|B) = \\frac{P(A\\cap B)}{P(B)}  \\\\\nP(B|A) = \\frac{P(A\\cap B)}{P(A)}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\n从上面两个式子可以看出，不管从那个条件出发，我们总有办法用表示联合概率的分布。是两种表示方法相等，我们就能得到贝叶斯定理的表达式\n\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n先验概率与后验概率先验概率 (prior) 和后验概率 (posterior)，是贝叶斯方法中的一个重要概念。简单一点的解释，先验概率其实就是一种分布已知的情况，比如一个 fair 的硬币我们就有 \np(Head) = 0.5, p(Tail) = 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n \n\n\n。因此先验概率是一个观测结果，而不是有什么已知推断出来的分布结果。我们对导致这一分布的条件一无所知，only god knows。然而在实际生活中，我们也会对条件进行观测。这就使得我们有了更多的观测，获取了更多的信息。基于对原因变量的观测，我们对结果变量做出估计，这就是后验概率了。由此条件概率表述为\n\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\Rightarrow 后验(posterior)=\\frac{似然函数(likelihood)\\times 先验(prior)}{边缘概率(marginal\\ probability/evidence)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n后\n\n\n验\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n似\n\n然\n\n\n函\n\n\n数\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n先\n\n\n验\n\n \n \n \n \n \n \n \n\n\n边\n\n缘\n\n\n概\n\n\n率\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n而在一般情况下，后验概率是没有办法直接观察得出的，我们需要根据已有的数据进行计算。这就是为什么我么需要贝叶斯定理，以为我们需要用它来计算这个以输入的特征向量为条件的后验概率 \np(Y=c_k|X=x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n。\n全概率公式若事件 \nB_1, B_2, ..., B_n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n\n 构成一个完整事件且都有正概率，则对任意事件 \nA\n\n\n\n\n \n\n 都有\n\nP(A) = \\sum_{i=1}^n P(B_i) P(A|B_i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n\n这一定理将用于后面的分母处理。\n条件独立性假设前面讨论条件概率和联合分布的公式中，条件都是单一的属性随机变量 \nB\n\n\n\n\n \n\n。而我们的输入向量往往是多个属性集合，也就是多个随机变量的集合。这些随机变量都将成为模型作出判断的条件，因此条件概率的计算就需要做一定的改变了。朴素贝叶斯中的一个重要假设就是：这些”属性条件”都是相互独立的。假设条件 \nB\n\n\n\n\n \n\n 由 \nn\n\n\n\n\n \n\n 个属性，于是，多属性的贝叶斯定理就可以表示为\n\np(A|B) = \\frac{p(A)}{p(B)}\\prod_{i=1}^np(B_i|A)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n \n\n\n朴素贝叶斯法之所以 naive 就是因为这一非常强的假设。所以在应用的时候，基于这样一个假设，在设计和提取我们所需要的特征时，就需要尽可能保证各个维度代表的属性是不相关的。这样才能是朴素贝叶斯比较有效。\n朴素贝叶斯方法仍然假定我们有一个 \nn\n\n\n\n\n \n\n 维输入空间 \n\\mathcal{X} \\subseteq \\mathbf{R}^n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，一个输入向量的实例为 \n\\mathcal{x}_i = (x_i^1, x_i^2, ..., x_i^n)^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n\n \n \n\n\n。输出空间为一个类标记集合 \n\\mathcal{Y} = \\{c_1, ..., c_k\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n。作为一个多属性的情况并基于条件独立性假设，我们可以得到后验概率 (即根据输入对输出做出的推断或者预测)。根据贝叶斯定理，最原始的式子应该是\n\np(Y=c_k|X=x) = \\frac{p(X=x|Y=c_k)p(Y=c_k)}{P(X=x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n\n\n\n\n\n之后通过全概率公式，我们可以将分母改写，进而得到\n\n\\begin{aligned}\np(Y=c_k|X=x) &amp;= \\frac{p(X=x|Y=c_k)p(Y=c_k)}{\\sum_kp(X=x|Y=c_k)p(Y=c_k)} \\\\   \\\\\n                        &amp;= \\frac{p(Y=c_k)\\prod_{i=1}^np(X^{(i)}=x^{(i)}|Y=c_k)}{\\sum_kp(X=x|Y=c_k)p(Y=c_k)} \\\\  \\\\\n                        &amp;= \\frac{p(Y=c_k)\\prod_{i=1}^np(X^{(i)}=x^{(i)}|Y=c_k)}{\\sum_kp(Y=c_k) \\prod_{i=1}^{n}p(X^{(i)}=x^{(i)}|Y=c_k)}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n \n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n\n\n\n\n \n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n\n\n\n\n \n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n\n\n\n\n\n\n这就是朴素贝叶斯方法的基本公式。于是，朴素贝叶斯分类器可以表示为\n\ny = f(x) = \\mathop{\\arg\\max}_{c_k} \\frac{p(Y=c_k)\\prod_{i=1}^np(X^{(i)}=x^{(i)}|Y=c_k)}{\\sum_kp(Y=c_k) \\prod_{i=1}^{n}p(X^{(i)}=x^{(i)}|Y=c_k)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n\n\n\n可以看到，分母部分是在所有类别样本中符合输入 \nx\n\n\n\n\n \n\n 的样本概率之和。当数据集固定的时候，不论类别 \nc_k\n\n\n\n\n\n \n \n\n 如何变化，这个分子也就固定下来了。所以，分类器的表达式可简化为\n\ny =  \\mathop{\\arg \\max}_{c_k}p(Y=c_k)\\prod_{j=1}^np(X^{(j)}=x^{(j)}|Y=c_k)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n\n对于整个模型来讲，\np(Y=c_k)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n 和 \nP(X^{(j)}=x^{(j)}|Y=c_k)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n 是我们用来做估计的参数，而这些参数则是由数据集和极大似然估计得来的。\np(Y=c_k)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n是一个先验概率，可以直接通过统计数量得出；而\nP(X^{(j)}=x^{(j)}|Y=c_k)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n则需要根据一定的方法做估计，方法可以是贝叶斯估计也可以是对特征分布的假设。接下来介绍一下这两种对这个likelihood进行估计的方法。\n贝叶斯估计以上的部分就是朴素贝叶斯法的原理和基本公式，这一部分的内容就涉及到具体实现中可能遇到的问题。用极大似然估计，可能会出现所求概率为0的情况，这样所得的后验概率也就为0了，可能造成分类误差。这一问题的解决方法就是贝叶斯估计，\n\nP_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl}, y_i = c_k)+\\lambda}{\\sum_{i=1}^NI(y_i = c_k)+S_j\\lambda}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n\n\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n\n\n\n\n引入的参数\n\\lambda\n\n\n\n\n \n\n就是为了应对新出现的特征值，被称为拉普拉斯平滑。\nS_j\n\n\n\n\n\n \n \n\n则表示给定数据中某一维度特征的不同取值个数。利用上面这个式子得出似然结果的模型称为Multinomial Naive Bayes。\n假设特征分布与之前的贝叶斯估计不同，这种方法里，我们假定数据在各个特征维度上都服从某一种分布。我们通过训练数据获取确定该分布必要的参数，比如均值和方差。这样我们就在应对测试集中新的特征值时计算出其对应的似然值。数据越多，对应计算出的参数肯定是更加逼近真实情况。这就是为什么朴素贝叶斯模型中会出现诸如：Gaussian、Bernoulli等以随机变量分布命名的模型。\n简单的实现基于之前对朴素贝叶斯方法的讲解，我们这里就来实现一个简单的Gaussian Naive Bayes。依然选用sklearn.dataset里面的iris数据集做演示。150个样本中120个做训练、30个做测试。模型fit的过程就是通过对训练数据进行统计，得到各个类别的先验概率以及各个特征维度的均值方差，为后面计算似然值和后验概率做准备。\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class GaussianNB(object):        def __init__(self, num_class):        self.num_class = num_class        self.class_prior = []        self.mean = []        self.var = []            def __get_class_prior(self, y):        ''' method to obtain class prior P(Y=ck) '''        sample_num = float(len(y))        for label in range(self.num_class):            c_count = np.sum(y == label)            self.class_prior.append(c_count / sample_num)        def __get_mean_var(self, X, y):        ''' method to get mean and variance for each feature'''        num_features = X.shape[1]        for c in range(self.num_class):            class_mean = []            class_var = []            class_samples = X[y == c]            for i in range(num_features):                mean = np.mean(class_samples[:, i])                var = np.var(class_samples[:, i])                class_mean.append(mean)                class_var.append(var)            self.mean.append(class_mean)            self.var.append(class_var)        def fit(self, X, y):        ''' method to fit the naive bayes model '''        self.__get_class_prior(y)        self.__get_mean_var(X, y)                def __cal_gaussian_prob(self, xi, mean, var):        ''' metod to calculate likelihood '''        inside = (xi - mean) ** 2 / (2 * var)        numeric = np.sqrt(2 * np.pi * var)        return np.exp(-inside) / numeric        def __predict_one(self, x):        class_probs = np.zeros(self.num_class)        for label in range(self.num_class):            class_prior = self.class_prior[label]            likelihood = 1.0            for i in range(len(x)):                likelihood *= self.__cal_gaussian_prob(x[i], self.mean[label][i], self.var[label][i])            prob = likelihood * class_prior            class_probs[label] = prob        return np.argmax(class_probs)        def predict(self, X):        if X.shape[0] == 1:            return self.__predict_one(X)        else:            predict = np.zeros(X.shape[0], dtype=np.int)            for i in range(X.shape[0]):                predict[i] = self.__predict_one(X[i])            return predict下面是预测结果和真实值对比生成的confusion matrix。在这个简单的数据集合上面效果感觉一般吧，30个测试样本中有2个错误。数据及的特征是思维的，1和2在前两个维度上甚至是线性不可分的，这也是为什么对0的分类效果要好一些，而1和2则容易混淆。\n\nReferenceCSDN-先验概率与后验概率、贝叶斯区别与联系\n贝叶斯公式的直观理解(先验概率/后验概率)\n知乎-如何简单理解贝叶斯决策理论\nwepon - 贝叶斯理论推导与三种常见模型\nscikit-learn Naive Bayes\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_naive-bayes-classifier.jpg","plink":"https://magi003769.github.io/post/朴素贝叶斯/"},{"title":"Python中的装饰器与类中的property","date":"2018-06-30T07:00:00.000Z","date_formatted":{"ll":"Jun 30, 2018","L":"06/30/2018","MM-DD":"06-30"},"updated":"2019-05-07T06:10:06.000Z","content":"原来读 python 代码的时候，经常遇到 @ 这个符号却并不知道是个啥意思。它经常出现在某一个类的函数定义前。这就是 python 的一个语法糖 —— 装饰器（Decorator）。考虑到模块化的设计，装饰器帮助我们在不改变函数定义的情况下，增加需要的功能，比如输出日志，性能测试，事务处理、缓存、权限校验等。\n123456789101112import functoolsdef log(func):\t@functools.wraps(func)\tdef wrapper(*arg, **kwargs):\t\tprint('\\ncall %s()' % func.__name__)\t\treturn func(*arg, **kwargs)\treturn wrapper@logdef slogan():\tprint('Don\\'t stop learning')装饰器本质上是能够返回一个函数的高阶函数。上面是一个装饰器的实现，是一个二层嵌套结构，基本的功能是在调用函数的时候产生日志。在定义函数 slogan() 时，相当于执行了语句 slogan=log(slogan)。log()  是一个装饰器，返回一个函数，所以最原始的函数 slogan() 依然存在，而现在同名的 slogan 变量则指向一个经过 log() 处理的新函数。 所以在主程序中调用 slogan() 将会执行新的函数，即 log() 中返回的 wrapper() 函数。\n如果我们需要给装饰器本身传入参数，在定义时就需要在 wrapper() 外在封装一层，原先的最外层就用来区别装饰器的名称和传参。这样，需要传参的装饰器就是一个三层嵌套结构。定义函数时相当于执行了 slogan = log2(&#39;execute&#39;)(slogan)。\n123456789101112def log2(text):\tdef decorator(func):\t\t@functools.wraps(func)\t\tdef wrapper(*arg, **kwargs):\t\t\tprint('\\n%s %s()' % (text, func.__name__))\t\t\treturn func(*arg, **kwargs)\t\treturn wrapper\treturn decorator@log2('execute')def slogan():\tprint('Don\\'t stop learning')上面的两个装饰器有一个值得注意的地方：@functools.wraps(func)。它是为了解决关于函数名统一的问题。在 python 中，函数作为对象，也有一个属性 __name__。上面我们也讨论了，装饰器是返回了一个函数，那么他最终返回的就是嵌套在最内层的 wrapper()。因此，在没有 @functools.wraps(func) 这个指令的情况下，我们获取函数名时 slogan.__name__ 会输出 &#39;wrapper&#39;，而不是 &#39;slogan&#39;。@functools.wraps(func) 相当于在装饰器内执行 wrapper.__name__=func.__name__。这一命令是使用在 wrapper函数之前的。\n小练习：如何定义一个在调用函数前后都输出日志的装饰器？\n只要在最内层的 wrapper() 中调用被修饰的函数，将其赋值给一个变量。这样就在调用的同时又可以返回该函数。只要在调用前后进行日志输出即可。单纯地将调用后的日志输出设定在wrapper外，decorator之内是不行的。\n123456789101112131415161718192021222324'''Exercise'''# Following funciton is WRONGdef log3(text=''):\tdef decorator(func):\t\t@functools.wraps(func)\t\tdef wrapper(*arg, **kwargs):\t\t\tprint('\\nBegin call: %s%s' % (text, func.__name__))\t\t\treturn call         print('End call: %s%s' % (text, func.__name__))\t\treturn wrapper\treturn decorator# Following funciton is RIGHTdef log3(text=''):\tdef decorator(func):\t\t@functools.wraps(func)\t\tdef wrapper(*arg, **kwargs):\t\t\tprint('\\nBegin call: %s%s' % (text, func.__name__))\t\t\tcall = func(*arg, **kwargs)\t\t\tprint('End call: %s%s' % (text, func.__name__))\t\t\treturn call\t\treturn wrapper\treturn decorator一个 Python 内置的装饰器：@property这里介绍一个修饰器在 Python 的一个应用：@property。它非常广泛的应用在 OOP 中。通常情况下定义一个类，我们需要如下的设计：get_score()来获取成绩，set_score() 来设定、修改成绩，这样就可以在设置时检查成绩是否合法。\n1234567891011class Student(object):    def get_score(self):        return self._score    def set_score(self, value):        if not isinstance(value, int):            raise ValueError('score must be an integer!')        if value &lt; 0 or value &gt; 100:            raise ValueError('score must between 0 ~ 100!')        self._score = value但是为了程序的简单易用性，我们想要使用类似于属性的简单方式来访问类的变量，同时又可以对参数的合法性进行检查。于是 Python 就为我们提供了一个内置的装饰器 @property 来达到这个目的。将一个同名的函数定义时加上 @property 就可以了。与此同时，它有自动生成了另一个装饰器 @score.setter，用来把一个 setter 变成属性变量，于是就产生了一个可以设置检查条件的简单方法。\n12345678910111213class Student(object):\t@property\tdef score(self):\t\treturn self.__score\t@score.setter\tdef score(self, value):\t\tif not isinstance(value, int):\t\t\traise ValueError('score must be integer!')\t\tif value &lt; 0 or value &gt; 100:\t\t\traise ValueError('score must between 0 ~ 100')\t\tself.__score = value一点想法：关于私有变量。C++ 中的私有变量是不能直接访问的，需要通过特定的方法方法才能访问。上面这样的设计就意味着设计了一个访问方法。那么 python 中又是如何保证私有变量不被外部非友元访问的呢？\nReference廖雪峰 - 装饰器\n廖雪峰 - @property 使用\npython programming - property\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/post-bg.jpg","plink":"https://magi003769.github.io/post/decorator/"},{"title":"机器学习模型——逻辑斯谛回归","date":"2018-06-28T07:00:00.000Z","date_formatted":{"ll":"Jun 28, 2018","L":"06/28/2018","MM-DD":"06-28"},"updated":"2020-05-10T08:03:22.000Z","content":"逻辑斯谛回归（Logistic Regression）是统计学习中的一个经典方法。最大熵是概率模型学习中的一种准则，将其推广到分类问题得到最大熵模型（Maximum Entropy Model）。二者都属于对数线性模型模型。\nLogistic Distribution对一个连续随机变量 \nX\n\n\n\n\n \n\n ，若其具有下列分布函数 \nF(x)\n\n\n\n\n\n\n\n \n \n \n \n\n（CDF）和概率密度函数 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n（PDF），则称 \nX\n\n\n\n\n \n\n 服从逻辑斯谛分布\n\nF(x)=P(X\\leq  x) = \\frac{1}{1+e^{-(x-\\mu)/\\gamma}} \\\\\nf(x)=F^\\prime(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n\n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n\n\n\n\n\n其中\n\\mu\n\n\n\n\n \n\n 是位置参数，\n\\gamma&gt;0\n\n\n\n\n\n\n \n \n \n\n是形状参数。形状参数越小，下图的 S 形曲线在中心附近增长越快。这个 S 形曲线是 CDF 的曲线，也被称为 Sigmoid Curve，关于 \n(\\mu, \\frac{1}{2})\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n \n \n\n\n \n\n。右边的一组是 PDF 曲线，形状参数越小，峰值越大。PDF 是 CDF 的导数，这也就等同于在中心附近，增长得更快。\n\n二项逻辑思谛回归模型模型定义Binomial Logistic Regression 是一种分类模型，由条件概率 \nP(Y|X)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 表示，形式为参数化的逻辑斯谛分布。该模型有如下的条件概率分布\n\nP(Y=1|X) = \\frac{exp(w \\cdot x + b)}{1 + exp(w \\cdot x + b)} \\\\\nP(Y=0|X) = \\frac{1}{1 + exp(w \\cdot x + b)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n在模型进行决策时，通过比较两个值的大小来将实例进行归类：分入概率值较大的那一类中。有时为了方便，会将权值向量和输入空间进行扩充，讨论时省去 \nb\n\n\n\n\n \n\n。在二分类的情况下，我们可以只计算 \nY=0\n\n\n\n\n\n\n \n \n \n\n 这一概率以简化计算。\n12def __sigmoid(self, z):        return 1 / (1 + np.exp(-z))模型特点\nQ：几率和概率究竟有何区别？\n\n现在对模型的特点进行考察。一个事件的几率（odds）是指事件发生的概率和该事件不发生的概率之间的比值。某一件事情发生的概率为 \nP\n\n\n\n\n \n\n，那么这个时间的 几率即为 \n\\frac{P}{1-P}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n\n，则其对数几率（log odds）或 logits 函数为\n\nlogit(p) = log\\frac{p}{1-p}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n\n\n\n\n\n将前面所定义的逻辑斯谛回归代入，可以得到\n\nlogit\\frac{P(Y=1|X)}{1-P(Y=1|X)} = w\\cdot x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n\n\n\n这意味着，在逻辑思谛回归模型中，输出 \nY=1\n\n\n\n\n\n\n \n \n \n\n 的对数几率是输入 \nx\n\n\n\n\n \n\n 的线性函数。或者说这一对数几率是由输入 \nx\n\n\n\n\n \n\n 的线性函数表示的模型。这就是逻辑斯谛模型。\n\n换一个角度，直接考虑输入 \nx\n\n\n\n\n \n\n 的线性函数 \nw\\cdot x\n\n\n\n\n\n\n \n \n \n\n，将这一线性函数转化为概率\n\nP(Y=1|X) = \\frac{exp(w \\cdot x + b)}{1 + exp(w \\cdot x + b)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n可以看到，这个内积的值越趋向于正无穷，概率越趋向于1。相对的，越趋向于负无穷，概率越趋向于0。因此我个人的理解就是，越极端的线性结果（易于分类），意味着更大的几率，同时也意味着更大的（或者说更有偏向的）条件概率。\n模型参数估计利用逻辑斯谛回归时，对于给定数据集 \nT,={(x_1,y_1), ...., (x_N, y_N)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n，其中 \nx_i \\in \\mathbf{R}^n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，\ny_i \\in \\{0, 1\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n。应用极大似然法估计模型参数，设 \nP(Y=1|x)=\\pi(x), \\quad P(Y=0|x)=1-\\pi(x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n。则似然函数为\n\n\\prod^{N}_{i=1} [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n \n \n\n \n\n \n\n \n \n\n\n \n \n \n \n \n\n \n \n\n \n\n \n\n \n \n\n \n \n\n\n\n\n\n对数似然函数为\n\n\\begin{aligned}\nL(w) &amp;= \\sum_{i=i}^{N}[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))] \\\\\n        &amp;= \\sum_{i=1}^{N}[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))] \\\\\n        &amp;= \\sum_{i=1}^{N}[y_i(w\\cdot x) - log(1+exp(w\\cdot x))]\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n \n \n\n\n\n\n \n \n\n \n \n\n \n\n\n \n \n \n \n\n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n这个似然函数即为我们的损失函数，由此，问题即转化为对数似然函数的最优化问题。对 \nL(w)\n\n\n\n\n\n\n\n \n \n \n \n\n 求最大值，或者等价求\n-L(w)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n的最小值，得到 \nw\n\n\n\n\n \n\n 。\n12def __loss(self, h, y):        return (- y * np.log(h) - (1-y) * np.log(1-h)).mean()通常采用梯度下降及拟牛顿法。梯度下降的方法，就是求偏导来更新参数。这是一个简单的线性模型，假设我们每次迭代有\nm\n\n\n\n\n \n\n个样本，那么我们更新参数的偏导可以表示为：\n\n\\frac{\\partial L(w)}{\\partial w_j} = \\frac{1}{m} X^T(\\pi(X\\cdot w) - Y_{label})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n \n\n \n \n\n\n\n \n\n\n\n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n\n其中\n\\pi(x)\n\n\n\n\n\n\n\n \n \n \n \n\n在这个逻辑思谛回归模型中代表的就是sigmoid函数，再具体的实现中可以是\n1gradient = np.dot(X.T, (h - y)) / y.size  # h为sigmoid输出，即预测结果为什么可以用做分类下面的这张图（出自Determined22），是一个简单的一维特征空间的二分类。它说明了，为什么线性规划不适用于分类。线性回归优化均方误差，在标签值极少的情况下（二分类），十分容易受到 outlier 的影响，从而出现许多的误分类情况。\n\n\n模型的简单实现下面使用sklearn.dataset里面的iris数据集来做一个简单的逻辑斯谛回归。这个数据集共有150个样本分属于不同的三类，其中标记为1和2的两类线性不可分。为了简化问题，我们将这两类合并为一类从而转化成二分类问题，并且只选用四维特征中的前两个维度作为输入空间以方便可视化。\n\n模型的实现主要在训练部分，loss的定义、梯度求解和参数更新都在前面提到过一些。下面代码片段中的类把思路整合一下。\n1234567891011121314151617181920212223242526272829303132333435363738394041424344class LogisticRegression(object):    def __init__(self, lr=0.1, num_iter=100000, add_bias=True, verbose=True):        self.lr = lr        self.num_iter = num_iter        self.add_bias = add_bias        self.verbose= verbose        def __add_constant(self, X):        b = np.ones((X.shape[0], 1))        return np.concatenate((X, b), axis=1)        def __sigmoid(self, z):        return 1 / (1 + np.exp(-z))        def __loss(self, h, y):        return (- y * np.log(h) - (1-y) * np.log(1-h)).mean()        def fit(self, X, y):        # add constant term to features        if self.add_bias:            X = self.__add_constant(X)                # parameters initialization        self.theta = np.zeros(X.shape[1])                # iterations to update parameter        for i in range(self.num_iter):            z = np.dot(X, self.theta)            h = self.__sigmoid(z)            gradient = np.dot(X.T, (h - y)) / y.size            self.theta -= self.lr * gradient                        if self.verbose and i % 10000 == 0:                z = np.dot(X, self.theta)                h = self.__sigmoid(z)                print('Loss: &#123;&#125; \\t'.format(self.__loss(h, y)))        def predict_prob(self, X):        if self.add_bias:            X = self.__add_constant(X)        return self.__sigmoid(np.dot(X, self.theta))        def predict(self, X, thresh):        return self.predict_prob(X) &gt;= thresh通过上面的逻辑思谛回归模型的训练，我们可以得到下图所示的结果。直线为可视化的超平面，可以看出基本所有的样本都得到了正确分类。\n\n多项逻辑斯谛回归单纯的逻辑斯谛只能做二分类的case，面对多分类情况时所用的 softmax 就是逻辑斯谛在多分类问题中的一个推广。在原来的二分类中，扩展后的特征空间是 \nx\\in \\mathbf{R}^{n+1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n。我们所需要调整的权值 \nw\\in \\mathbf{R}^{n+1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n，是一个列向量。而现在，当多分类时，\ny_i \\in \\{1,2,3,...,K\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n，我们需要调整的权值就是一个 \n(n+1)\\times K\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的矩阵了，每一列代表计算该实例是否属于某一类时所用的向量，一共有 \nK\n\n\n\n\n \n\n 列。这样，多分类可以转化为一个二分类问题：属于or不属于这一类。于是我们有了如下的式子计算属于第 \nj\n\n\n\n\n \n\n 类的概率\n\nP(Y=K|x) = \\frac{1}{1+\\sum_{k=1}^{K-1} exp(w_k\\cdot x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n\n \n \n\n \n \n \n\n\n\n\n\n区别几个概念下面是几个概念我们需要加以区分，可能不全都属于逻辑斯谛，但是多少有点关系。尤其是 Softmax，形式和逻辑斯谛的多分类很像\n逻辑斯谛回归（多分类）\nSoftmax\nSoftmax Loss / Cross Entropy Loss\nReference逻辑回归Logistic Regression，Softmax Regression\nsoftmax, softmax loss and cross entropy\nLogistic Regression from Scratch\nBuild a Logistic Regression in Python\nUnderstanding Logistic Regression\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/linear_vs_logistic_regression.jpg","plink":"https://magi003769.github.io/post/逻辑斯谛/"},{"title":"PyTorch Note (2) —— Autograd","date":"2018-06-22T07:00:00.000Z","date_formatted":{"ll":"Jun 22, 2018","L":"06/22/2018","MM-DD":"06-22"},"updated":"2020-05-10T23:09:36.000Z","content":"autograd 包是 pytorch 构建神经网络的核心。他为所有的张量运算提供了自动求导的方法。而这是一个 define-by-run 的框架，这意味着反向传播的过程取决于你的代码如何运行。甚至每一次迭代都可以有所不同。\nTensor 和 Functiontorch.tensor 是整个包的核心类。设置它的 attribute .require_grad 为   Ture，这个对象就会开始追踪所有的运算。当计算过程完成后，可以调用 .backward() 来自动计算梯度。一个张量的梯度将会被储存在其 .grad 这一 attribute 中。\n如果想要终止对张量的追踪，可以调用 .detech()。这样这两就和运算追踪分离，且将来的运算也不会再追踪。除此之外，还可以用 with torch.no_grad(): 来包含一整块代码。这个方法在评价模型的时候就非常有帮助，因为模型中包含了许多可训练的参数require_grad=True，但此时我们并不需要这些梯度信息。\n12345678# 初始化一个 tensorx = torch.ones(2,2,requires_grad=True)# 对tensor进行一次运算, y 是一个运算的结果，所以它有 grad_fny = x + 2print(y.grad_fn) # 输出一个实例的信息print(x.grad_fn) # 输出 None对于梯的实现来讲，另一个非常重要的类就是 Funciton 了。Tensor 和 Function 是相互关联的，它们组成了一个 acyclic graph (???) 用来编码和记录完整的运算记录 (history of computation)。每一个变量都有 .grad_fn 这一 attribute 用来索引生成张量的Function。我们可以理解为，整个图是由张量和运算这两个最重要的元素所组成的，Funciton 可以简单地对应到运算上去，通过运算得到的变量就回具有 .grad_fn。\n当计算倒数时，对一个张量对象调用 .backward()。如果该张量是一个标量，则不需要对 .backward() 指定任何的参数。然而，如果它包含了其他很多元素，那么他就需要指定和张量匹配维度的参数。\nGradients123456789101112131415# Simple gradient: output is scalara1 = torch.tensor([2,3], requires_grad=True, dtype=torch.float)a2 = torch.tensor([2,2], requires_grad=True, dtype=torch.float)b = a1 + 3print(b.requires_grad)c = b * b * 3 + a2out = c.mean()out.backward()print(\"\\n======== Simple gradient =========\")print('\\nInput:\\n', a)print('\\nComputation result:\\n', out)print('\\nInput gradient: \\n', a1.grad)print('\\nInput gradient: \\n', a2.grad)print('\\nInput gradient: \\n', b.grad)第一个 case 适当最后的张量是一个标量时，可以直接调用 .backward() ，而不需要传参。最终结果的表达式以及偏导数为。于是我最终应该得到的目标是一个 \n[15, 18]\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 的向量。\n\nout = \\frac{1}{2}\\sum_{i=0}^{1}3\\times(x_i + 3)^2  \\\\\n\\frac{\\partial out}{\\partial x_1}|_{x_1=2} = \\frac{1}{2}\\times6(x_1+3)^2\\times1=15 \\\\\n\\frac{\\partial out}{\\partial x_1}|_{x_2=3} = \\frac{1}{2}\\times6(x_2+3)^2\\times1=18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n\n \n \n\n \n \n\n\n\n\n\n\n \n \n \n \n\n\n \n\n \n \n\n\n\n\n \n\n \n \n \n \n\n\n \n\n\n\n \n \n\n\n \n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n\n\n\n\n \n \n \n \n\n\n \n\n \n \n\n\n\n\n \n\n \n \n \n \n\n\n \n\n\n\n \n \n\n\n \n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n\n\n在所实验的时候，出现了一些有趣的现象。为了验证链式法则，我尝试着得到中间张量的梯度信息，然而发现并不能够成功。感觉 pytorch 的计算图实现中，类似于这样的树状结构，只能对包含 requires_flag=True 的叶节点的计算图求梯度。\n1234567891011121314151617# Complex case: output is NOT scalarm = torch.tensor([2,3], requires_grad=True)n = torch.zeros(2) # torch.tensor([0,0], requires_grad=True)l = torch.zeros(2)n[0] = 2 * m[0]n[1] = 3 * m[1]l[0] = n[0] ** 2l[1] = n[1] ** 3l.backward(torch.tensor([1,1])) # !!!!!print(\"\\n======== Complex gradient =========\")print('\\nInput:\\n', m)print('\\nComputation result:\\n', l)print('\\nInput gradient: \\n', n.grad)print('\\nInput gradient: \\n', m.grad)第二个 case 是最终的结果是一个 vector，这时我们就需要给 .backward() 一个参数。这个参数的大小 (shape) 必须和求导的 vector 一致。这个参数的含义实际上给每个偏导的一个权值。这里权值都是 1，所以对球到结果并不产生任何影响。\n其实，在实验里还有一些问题\n\n\\frac{\\partial l}{\\partial x_i}\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n 应该有几个值 4/2？\n第二个 case 中，如果张量 n 的生成为注释部分，就会最终只得到 n 的梯度信息，而 m 的就会消失。为什么？明明两个都是 requires_grad=True。\nReferenceCSDN - PyTorch中的backward\nAutograd mechanics\nPytorch的backward()相关理解\n","plink":"https://magi003769.github.io/post/pytorch_autograd/"},{"title":"经典的降维方法——PCA","date":"2018-06-20T07:00:00.000Z","date_formatted":{"ll":"Jun 20, 2018","L":"06/20/2018","MM-DD":"06-20"},"updated":"2019-05-07T06:17:24.000Z","content":"在数据处理和一部分特征工程中，降维一直都是非常重要的一步。这篇 post 就简单介绍一下主成分分析法（PCA）的计算过程和实现。\nPCA降维的计算过程PCA降维的计算过程有两种实现方法\n\n通过计算协方差矩阵并求特征值/特征向量\n\n第二种方法是用数据矩阵的奇异值分解（singular value decomposition） \n\n\n这里我们来介绍第一种。特征向量和特征值只能由方阵得出，且并非所有方阵都有特征向量和特征值。如果一个矩阵有特征向量和特征值，那么它的每个维度都有一对特征向量和特征值。矩阵的主成分是由其协方差矩阵的特征向量，按照对应的特征值大小排序得到的。最大的特征值对应第一主成分，第二大的特征值对应第二主成分，以此类推。  \n\nPCA的一种计算方法\n计算协方差矩阵\n得到协方差矩阵的特征值和特征向量\n对特征值进行排序，选取最大的特征值作为优先的成分\n\n我们来理一理这中间的维度变化。对于原始数据，我们假设有 \nN\n\n\n\n\n \n\n 个实例，每个实例的特征个数，即维度都是 \nD_0\n\n\n\n\n\n \n \n\n 。将每一个实例都作为一个行向量，那么原始数据的矩阵维度就是 \n[N, D_0]\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n。对于这样一个维度为 \nD_0\n\n\n\n\n\n \n \n\n 的特征空间来讲，其对应的协方差矩阵 \n\\mathbf{A}\n\n\n\n\n \n\n 维度为 \n[D_0, D_0]\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n ，则我们能够写出的特征方程维度上就是这样的\n\n\\mathbf{A}\\overrightarrow{v} = \\lambda \\overrightarrow{v} \\\\ \n[D_0, D_0] * [D_0, 1]=  \\lambda[D_0, 1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n此处 \n\\overrightarrow{v}\n\n\n\n\n\n\n \n \n\n 表示特征向量。由于特征值 \n\\lambda\n\n\n\n\n \n\n 的个数一般为 \nD_0\n\n\n\n\n\n \n \n\n ，因此全部的特征向量可以组成一个新的 \n[D_0, D_0]\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 矩阵。当然这些向量会根据特征值的大小进行排序，而我们最终需要确定他的列数，这是一个将为的重要参数。我们选择特征值最大的主成分，即前 \nk\n\n\n\n\n \n\n 列。由此得到一个 \n[D_0, k]\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n 的矩阵，利用这一矩阵对原始数据的矩阵进行降维得到\n\n[N, D_0] * [D_0, k]=  [N, k]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n这样，原始数据的维度就由 \nD_0\n\n\n\n\n\n \n \n\n 变成了 \nk\n\n\n\n\n \n\n，当然 \nk &lt; D_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，不然这降维降的是个锤子。\n实现前面应该提到了，PCA的降维过程实际上有两种计算过程。这里我们也只实现第一种计算过程，现成的 sklearn 里面 PCA 的实现使用的是第二种奇异值的方法。目前还没研究，等以后再把这个坑填上。\n1234567def my_PCA(raw_data, output_dim):\t# x = np.transpose(x)\tT = raw_data - np.mean(raw_data, axis=0)\tcov_mat = np.cov(T.transpose()) # 计算协方差矩阵\tw, v = np.linalg.eig(cov_mat)   # 返回特征值和特征向量\treduced_x = np.matmul(raw_data, v[:, :output_dim])\treturn reduced_x使用 sklearn.dataset 中的 load_iris 获取数据。共150个实例，每个实例包含4种特征属性。我们不能将四维度空间，但通过 PCA 降维就可以。下面是结果，我对比了自己的实现和 sklearn 中的实现，结果还是有很大区别的，数据点聚合的形式其实差不太多：一类相对较远，另外两类有重叠。具体区别，以后填奇异值分解这个坑的时候，在研究吧。\n\nP.S. 疯狂安利参考资料中的第一个链接。\nReference通俗理解PCA降维\nPCA降维的原理及步骤\nPCA数据降维：从代码到原理的深入解析\n","thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/pixel-water.jpg","plink":"https://magi003769.github.io/post/PCA/"},{"title":"PyTorch Note (1) —— Tensors and Operations","date":"2018-06-15T07:00:00.000Z","date_formatted":{"ll":"Jun 15, 2018","L":"06/15/2018","MM-DD":"06-15"},"updated":"2019-05-04T03:50:20.000Z","content":"最近听说 PyTorch 很好用，所以就开始看一下。起码不用每次想看 tensor 维度的时候都必须要 feed 然后 run session。PyTorch 中的张量实现非常类似于 Numpy 中的 ndarray，因此在使用时有很多地方很类似于 Numpy。这里就记录一些 PyTorch 的常用接口。\n张量生成张量 (tensor)，是运算的基本单位，几乎所有的操作都是基于 tensor 来进行的。对于一个 tensor，我们需要以下几个要素来确定它\n\n维度\n数据类型\n初始值\n\n维度和数据类型自不必说，通常都作为参数传入接口来生成一定大小的 tensor。而对于初始值，我们可以选取零初始、随机数或者一个指定的概率分布。类似于 Numpy，PyTorch 提供了很多生成 tensor 的接口。\n1234567891011import torch# 按照维度生成空的/随机的tensorx = torch.empty(5, 3)y = torch.rand(5, 3)# 指定数据类型z = torch.zeros(5, 3, dtype=torch.long)# 直接由数据生成tensor，类似于np.array()的用法k = torch.tensor([5.5, 3])除了上述的的生成方法，PyTorch 还支持通过已有的 tensor 来生成新的 tensor。已有的 tensor 可以作为一个对象，调用一些方法来改变值，也可以作为参数传入一个方法中来返回一个想要的 tensor。在没有指定的情况下，PyTorch 会保留原有张量的性质。\n1234567# 直接改变已有tensor的维度、数值以及类型# 得到一个 5*3 的全1张量k = k.new_ones(5, 3, dtype=torch.double)# 改变数值，全1变成随机，并改变数据类型# 但是不改变维度k = torch.randn_like另外，在 debug 的时候，Numpy 为我们提供了 ndarray.shape 这样的 attribute 来方便的查看矩阵的维度。而 PyTorch 中的 tensor 就相当于 Numpy 中的 ndarray，也有这样的接口来方便的查看 tensor 的维度 —— k.size()。这个方法会返回一个 torch.Size 对象，实际上是一个 tuple。 \n张量运算对于 tensor 的运算操作，有非常多的形式。以相加为例，就有一下几种形式\n12345678x = torch.rand(5, 3)y = torch.rand(5, 3)# 使用运算符print(x+y)# 使用PyTorch内建函数print(torch.add(x, y))以上的两种方法均是产生一个右值，我们可以将结果直接赋值给一个变量。而下面的两种方法，将使得计算结果改写一个已有的tensor\n1234567# 使用作为输出的tensor当做参数result = torch.empty(5, 3)torch.add(x, y, out=result)# 像一个tensor的值加到另一个上面# 改写参与运算的其中一个tensory.add_(x)需要改变的tensor作为一个对象，调用对应的运算方法来改变自身的值：y &lt;- y + x。任何替换 tensor 自身数值的操作都是以 _ 所谓后缀的。\n选取张量中的指定元素PyTorch 支持所有标准Numpy 下标索引语法。一般的索引都是用 slice 对象实现，格式为 start:stop:step。基于这种格式，下面是几个例子\n123456789101112import numpy as np&gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5, 6])&gt;&gt;&gt; x[3:]array([4, 5, 6])&gt;&gt;&gt; x[3::]array([4, 5, 6])&gt;&gt;&gt; x[::2]array([1, 3, 5])可以看到，: 或者:: 单独出现在一个数字后面的时候，就表示将这一维度上的所有选出。可以视作对终止位置和步长的省略（默认为最后一个元素和1步长）。值得注意的是最后一个操作，省略的起始和终止，只对步长做了说明，这样就会在原矩阵中的某一维度上等距的选取元素。\n张量的 resize如果想要 resize 一个 tensor，就需要 torch.view。就是这个张量的名字，起的实在是不怎么直观。参数是各个维度的数值，而设定为 -1 的维度则会由其他维度的大小以及总元素的个数推断出来。\n12345678910111213141516x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)  # the size -1 is inferred from other dimensionsprint(x)print(z)print(x.size(), y.size(), z.size())tensor([[-1.1173, -1.2871, -0.0458, -0.2737],        [-0.4851,  1.3128, -0.6147,  0.2434],        [ 0.6655,  1.2270,  1.1213, -0.1862],        [-0.6330,  0.0598,  0.3160,  0.4627]])tensor([[-1.1173, -1.2871, -0.0458, -0.2737, -0.4851,  1.3128, -0.6147,          0.2434],        [ 0.6655,  1.2270,  1.1213, -0.1862, -0.6330,  0.0598,  0.3160,          0.4627]])torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])Torch tensor 和 Numpy array 的转换这二者之间的转换在这一框架下是很容易的，它们会共享底层的内存，一旦其中一个改变，另一个就会跟着改变。不管谁作为原始数据，改变都会同时发生\n1234567891011a = torch.ones(5)# Convert pytorch tnesor to numpy arrayb = a.numpy()# ========================== #a = np.ones(5)# Convert numpy array to pytorch tensorb = torch.from_numpy(a)CUDA TensorsPyTorch 支持将 tensor 移动到任意的硬件上进行计算，使用方法 .to 即可实现。不过在这之前我们需要检查 CUDA 是否可用。\n123456789# let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available():    device = torch.device(\"cuda\")          # a CUDA device object    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``    z = x + y    print(z)    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!ReferencePyTorch tutorial: What is PyTorch\nNumpy indexing\n","plink":"https://magi003769.github.io/post/pytorch_tensor_operation/"},{"title":"Morphological Operation","date":"2018-05-20T07:00:00.000Z","date_formatted":{"ll":"May 20, 2018","L":"05/20/2018","MM-DD":"05-20"},"updated":"2019-05-04T04:20:58.000Z","content":"Morphology (形态学) 大概是图像处理这门课最后一部分的内容了，最后一次作业包括了数硬币和车牌识别两个部分。虽然说形态学是图像处理的一个重要分支，但是总感觉这个方法并不是那么得高明，感觉像是一种 pixel level 的匹配，也谈不上什么特征不特征的。不过作为计算机视觉中的经典方法，也算是在早期起到了一定的作用。这一篇博客就简单的谈一下这方面的话题。\nMorphological Operation首先，形态学的各种操作都是基于 Binary image 的，这是应用这些方法的前提。在 Morphological Operation 中，一个重要的东西就是 Structure Element。数值上来讲，其内部均为1。如下图所示，structure element在输入图片上移动，与它所覆盖的区域对应做乘法，所得出的结果经过特定的逻辑运算的输出结果。\n\n根据逻辑运算的种类不同，morphological operation 也可以由许多不同的分类。最基本的就是 Erosion 和 Dilation。前者使用 Logical AND，而后者使用 Logical OR。这样就会导致非常直观的结果，如下图所示。Erosion 的与门操作使得能够输出数值的条件变得非常的苛刻，而 Dilation 的或门则使得输出数值相对容易。于是，原有的形状就被 “膨胀”，或者被 “侵蚀”。\n\n另外，需要注意的是，structure element 在原图形上是以其中心为基准的。中心在原图上对应一个像素点，并与之周边的像素做操作。因此在原图像边缘的位置，就需要做 padding 来保持输入输出图片尺寸的一致性。然而，貌似在 Matlab 的实现中，padding 的值默认为1，这就会在一定的情况下引出一些 bug，所以是一个比较值得注意的地方。\nRobustnessMorphological operation 能够有效的条件实际上是比较苛刻的，我们需要被检测的 pattern 和被用作 structure element 的模板一模一样，甚至一个像素点都不能差，这样才能出现理想的检测结果。因此，morphological operation 的鲁棒性实际上是挺差的，那该只能用来检测类似印刷字或者工业产品这种有标准模板的东西。\n一个常规的办法就是将模板腐蚀掉一部分，这样不至于什么都检测不出来。但这样就会带来如下图的问题。‘K’ 和 ‘1’ 都将会检测出 ‘I’ 的结果。‘I’ 的模式确实也出现在 ‘K’ 和 ‘1’ 里面。\n\nHit-missHit-miss filter 是上面问题的一种解决方法。他通过检测字符本身，以及字符周边的背景轮廓来使得结果更加理想。下面的两张图是对这一方法的一个说明，白色表示需要检测的前景，黑色为背景。我们的任务是要检测左上角稍大的正方形区域，因此 structure element 就选取与之对应的形态。最终我们得到了如 \nA\\ominus H\n\n\n\n\n\n\n \n \n \n\n 的结果。下面矩形区域产生的结果并不是我们想要的。\n\n下面一步的操作就是，取原图像的全反 —— 前景变背景，背景变前景。相对应的，我们将上面的 structure element 取反并 padding 一些前景作为检测目标。这样就相当一是对背景轮廓的检测了。于是我们就得到了另一组结果 \nA^C \\ominus M\n\n\n\n\n\n\n\n \n \n \n \n\n。这两组结果取交集就是我们的最终结果了。\n\n下面是在车牌检测中使用的模板，前景和背景。将原模板侵蚀一下作为前景，膨胀后取反作为背景。\n\n\n\n","plink":"https://magi003769.github.io/post/morphologic_operation/"},{"title":"聊一聊物体检测的Metrics与Detection Theory","date":"2018-04-27T07:00:00.000Z","date_formatted":{"ll":"Apr 27, 2018","L":"04/27/2018","MM-DD":"04-27"},"updated":"2020-05-10T23:07:56.000Z","content":"最近在做 FYP 的收尾工作了，需要将模型的精度用一个量化的形式去衡量。正巧 Data Communication 的课上也涉及到了检测的相关内容，于是就在这里做个整理，也帮马上就要写的论文整理一下思路。虽然我个人觉得有的时候只用这种精度来衡量模型的好坏并不能非常全面的衡量，但毕竟这些标准是有理论支持的、相对客观的。而且真正在工业实践中比较模型好坏时，肯定不能用人眼的主观感受去判断，人们更愿意用数据说话。\nDetection Theory吐槽完了，现在进入这篇 post 的正题： Detection Theory。它有很多的应用场景，比如雷达、癌症检测还有文本检索等等。我们以癌症检测为例，输入和输出都是二值化的：即阳性或者阴性（Positive or Negative）。模型会依照输入的信息做出判断，判断有对有错（True or False），因此中体上来讲就会出现如下图中所示4种情况：True Positive (TP), False Positive (NP), False Negative (FP) 和 True Negative (TN)。\n\n下面的图是用来说明检测的过程。假设原先的随机噪声信号服从一个正态分布，有癌症信号出现之后，则整体平移。Critical Threshold 是检测门线，左侧检测结果为癌症不存在，右侧为癌症存在。门线偏左代表一个较为宽松的检测标准，可以有效减少FN的数量，而FP将会变多。\n\n准确率与查全率分类问题的衡量标准相对简单，就是预测的标签和实际的是否一致。而在物体检测任务中，事情就会变得相对复杂。我们需要两个量来衡量模型的好坏准确率（Precision）和查全率（Recall）。\n\nP = \\frac{TP}{TP+FP} \\\\\nR = \\frac{TP}{TP+FN}\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n\n准确率相对容易理解，就是模型做出物体存在的判断中，有多少是正确的。它可以而查全率的概念就相对来说绕一点。直译的话 recall 是召回的意思，而这个意思并不能很好的反应这个量的意义。查全率反应的是模型是否能够讲目标尽可能的检测出来，而不漏掉。Precision 和 Recall 是两个需要 trade-off 的标准。当我们用相对严格的标准来让模型判断时，比如更高的 IOU 和置信度，最终的检测结果可能有很高的 precision，仅为选出来的基本上都是走有可能正确的。那些相对置信度没有那么高的检测结果却对应着某一实际结果，则会被这样一个过高的标准筛选掉，没有出现在检测结果中。这样就会产生大量的 FN 结果，从而导致很低的 recall。\n\nPR curve 可以非常直观的比较两个系统检测的性能：曲线左下方所围面积越大，说明检测的效果越好。上图中的水平线，是最优化的方案，不论在什么样的查全率下，或者说如何宽松的条件下，所检测出的物体都是正确预测。\nPASCAL VOC的评价标准在 PR 曲线中，每一组 (recall, precision) 值都对应着不同的检测尺度。相对严格的尺度会得到较好的准确率较差的查全率，相对宽松的尺度则会有较好的查全率较差的准确率。因此，真正产生 PR 曲线的是变化的是变化的尺度。而在物体检测的任务中，变化的则是检测结果的 confidence。这里推荐一个GitHub上的SSD实现：pierluigiferrari/ssd_keras，它的模型评估写得非常清楚，而且注释非常详尽，简直都赶上写小说了。\n对于检测测试集的某一类，我们产生了 \nX_{c}\n\n\n\n\n\n \n \n\n 个检测结果，并将这些检测结果根据 confidence 进行降序排序。其中下标 \nc\n\n\n\n\n \n\n 代表某一类别。在一个循环中（\ni\\in[1,X]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n），  对 confidence 最高的前 \ni\n\n\n\n\n \n\n 个检测结果进行匹配，判断是否是正确检测（匹配的过程不在这里赘述，也是一个比较麻烦的过程）。这里仅仅记录检测结果是 TP 或是 FP，暂时不涉及到准确率和查全率的计算。TP 和 FP分别存在两个 list 里面，长度均为 \nX_c\n\n\n\n\n\n \n \n\n。所以，对于整个数据集的 TP 和 FP 我们都有 [20, \nX_C\n\n\n\n\n\n \n \n\n] 的 2D list —— true_positive 和 false_positive。再对这些 list 做累积求和得到 cumulative_true_pos 和 cumulative_false_pos。\n这些工作都完成之后，就可以计算准确率和查全率了。cumulative_true_pos 和 cumulative_false_pos可以用来直接计算准确率，而查全率的计算则需要用到某一类 ground truth 的总个数 num_gt_per_class。到此，就得到了可以做出 PR 曲线的数据。\n最后就是 PASCAL VOC 的数值化指标：AP。官方标准是在查全率 [0, 1] 的范围里取等距的11个点，并在 PR 曲线上找出相应的准确率。所取得的11个准确率数值做平均，就得到了AP\n\nAP = \\frac{1}{11} \\sum_{r=\\{0, 0.1,...,1\\}} p(r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n\n \n \n \n \n\n\n总体的准确率就是对各个类别的 AP 去均值，即 mAP。以上就是 PASCAL VOC 对于整体准确的评价凡事。至于paper中的细节评判，如尺寸不同物体的检测效果，或者近似类别的评估，这里我就不讲了。并没有研究出是怎么做的，也不知道有什么样的工具或者现成的API可以做这个工作。以后更深入研究这个领域的时候，会再看再学。\n实现中的注意：我是智障以 SSD 在 PASCAL VOC 数据集上做物体检测为例，查全和查准都是在具体的某一类别上说的。具体计算时，需要针对具体某一张图片。比如，一张图片上有 \nN\n\n\n\n\n \n\n 属于人这一类别的 ground truth，而模型给出了 \nX\n\n\n\n\n \n\n 个人的检测结果，并且 \nX\n\n\n\n\n \n\n 个检测结果中有 \nT\n\n\n\n\n \n\n 个结果是可以有匹配的 ground truth。假 \nX&gt;N\n\n\n\n\n\n\n \n \n \n\n，那么我们就有\n\nP = \\frac{TP}{TP+FP} = \\frac{T}{T+(X-N)} \\\\\nR  = \\frac{TP}{TP+FN} = \\frac{T}{N}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n \n\n\n\n \n\n \n \n \n \n \n \n \n\n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n \n\n\n\n \n \n\n\n\n\n\n一开始的时候，我一直在疑惑这个 FN 的情况怎么处理， 以至于在查全率的地方困惑了很久。现在才发现，自己是真的蠢 \nTP+FN\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 不就是图片里 ground truth 的个数吗？预测到的，加上没预测到的。MDZZ\nReferenceCOCO Dataset\nPASCOL VOC style mAP\nUnderstanding the mAP Evaluation Metric for object detection\nOfficial document of PASCAL VOC\n","plink":"https://magi003769.github.io/post/Detection Theory/"},{"title":"DCT——离散余弦变换","date":"2018-04-20T07:00:00.000Z","date_formatted":{"ll":"Apr 20, 2018","L":"04/20/2018","MM-DD":"04-20"},"updated":"2020-05-10T07:59:18.000Z","content":"Discrete Cosine Transform定义和公式图像处理中可应用二维的 DFT 将图像转化到频域并做处理，然而 DFT 会产生虚部，就是个让人非常烦的事情。不管是模长还是相位都与图像的空间域重建有关系。而且 DFT 的频域信息可视化起来并不是很方便而且不容易理解。因此就引入了 DCT 来省去 DFT 带来的虚部。下面是一维 DCT 的公式：\n\nF(k) = \\alpha(k) \\sum_{n=0}^{N-1} x(n)cos[\\frac{(2n+1)k\\pi}{2N}] \\\\\nx(n) = \\alpha(k) \\sum_{k=0}^{N-1} F(k)cos[\\frac{(2n+1)k\\pi}{2N}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n\n\n\n \n\n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n\n\n\n \n\n\n\n其中的系数 \n\\alpha(k)\n\n\n\n\n\n\n\n \n \n \n \n\n 的作用在于 normalize，使得结果都落在0到1之间。\n\n\\alpha(k) = \\left\\{\n\\begin{aligned}\n\\sqrt{\\frac{1}{N}} \\quad &amp;, k=0 \\\\\n\\sqrt{\\frac{2}{N}} \\quad &amp;, k\\neq0\n\\end{aligned}\n\\right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n\n\n\n\n \n \n\n\n\n\n \n\n\n\n\n \n \n\n\n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\nDCT 的基​        以时域转换到余弦波的正向转换为例，\nF(k)\n\n\n\n\n\n\n\n \n \n \n \n\n 最终的长度应该和输入的时域信号长度相同，它的每一个元素都是固定 \nk\n\n\n\n\n \n\n 值，每个 \nk\n\n\n\n\n \n\n 值都对应一个余弦波的频率 \n\\frac{k}{2N}\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n **。再对 \nN\n\n\n\n\n \n\n 个该频率余弦波的采样点进行进行计算，然后叠加。所以当输入的信号长度固定时，我们计算的过程中这些余弦波的频率都是固定的。唯一的变化就是输入信号的值大小不同，而当我们固定振幅， \nx(n) = 1 , \\forall n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n ，那么我们就可以得到 DCT 的基。下面的表格中是一个长度为8的时域信号所对应的 \n8\\times8\n\n\n\n\n\n \n \n \n\n 基矩阵，可以由 MATLAB 中的 dctmt(N) 得到。每一行对应一个 \nk\n\n\n\n\n \n\n 值，每一列对应一个 \nn\n\n\n\n\n \n\n 值计算出的余弦值。**\n\n从数值上，我们可以看到 \n0.3536\\approx\\sqrt{\\frac{1}{8}}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n\n\n\n \n \n\n\n\n\n ， 而余下我们的余弦波最大振幅应该为 \n0.5\n\n\n\n\n\n\n \n \n \n\n 。\n想要得到最终的 DCT 结果，只要用这个基矩阵与原时域信号的转置做矩阵乘法就可以了。\n\\mathbf{B}\\mathbf{x^T}\n\n\n\n\n\n\n \n\n \n \n\n\n 对应的维度是 \n[N, N][1, N]^T=[N, 1]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n，最终仍是一个长度为N的向量，只不过原先的输入信号是一个行向量，而输出是一个列向量。下图是可视化之后的基。一开始 DC 分量，频率为0，第二个是半周期，再是整周期，然后类推：某一频率下余弦波周期 \nT=\\frac{1}{f}=\\frac{2N}{k}\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n \n\n\n \n\n\n\n\n \n \n\n \n\n\n\n。\n\nSummaryDCT 将一个信号转化成一系列频率、振幅不同的余弦波的叠加。但它并不是 DFT 的实部，仅仅只是避免了变换之后复数的产生，将所有处理的数据都变成实数。 实际上，DCT将原有的信号转化成了可以由这些基的线性组合表示的结果。\n2D DCT在图像处理中，我们通常要处理的信号都是二维的，所以就引入了 2D-DCT。对图片的各行和各列分别做 1D-CDT 就可以得到 2D-DCT。以 \nN=4\n\n\n\n\n\n\n \n \n \n\n 为例，用上面相同的操作，我们可以得到一个 \n4\\times4\n\n\n\n\n\n \n \n \n\n 的基矩阵 \n\\mathbf{T}\n\n\n\n\n \n\n。每一行是一个我们需要的余弦波数值。需要注意的是，这个基矩阵仍然是针对一维情况的基矩阵，二维情况我们可以参看后面的可视化结果。做矩阵乘法，前行乘后列\n\n\\mathbf{F_{tmp} = TX}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n\n\n我们得到了原图 \n\\mathbf{X}\n\n\n\n\n \n\n 每一列应用 1D-DCT 的结果，然后我们需要继续对每一行做 DCT。将基矩阵转置，使得余弦波由原来的行向量变为列向量，在做矩阵乘法\n\n\\mathbf{F=F_{tmp}T^T}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n \n \n\n\n\n由此，我们完成了 2D-DCT 的整个过程，最终结果 \n\\mathbf{F}\n\n\n\n\n \n\n 和原始图像 \n\\mathbf{X}\n\n\n\n\n \n\n 的转化关系可以表示为\n\n\\mathbf{F=TXT^T}\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n\n对于 \n4\\times4\n\n\n\n\n\n \n \n \n\n 的图像来说，它们 2D-DCT 的基就有16个，每个基也都是 \n4\\times4\n\n\n\n\n\n \n \n \n\n 的大小，如下图所示。对 DC 分量，即全白的那一张，我们可以视作它的行列都应用了 \n\\mathbf{T(1,:)}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n ，也就是原来一维情况下的 DC 分量。而对于下标为 \n(i,j)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的基，则在原图的各行应用了 \n\\mathbf{T(j, :)}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n，而各列应用了 \n\\mathbf{T(i, :)}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n。\n\n","plink":"https://magi003769.github.io/post/DCT/"},{"title":"Linux命令行","date":"2018-04-02T07:00:00.000Z","date_formatted":{"ll":"Apr 2, 2018","L":"04/02/2018","MM-DD":"04-02"},"updated":"2019-05-04T03:47:10.000Z","content":"这几天总算是搞定了实验室服务器的账户，终于用上TITAN了。摆脱了一天到晚读玄学paper不知所以，开始了一脸懵逼的玄学实验胡乱调参。而在这之前需要了解掌握基本的 Linux 命令行操作，熟悉终端，才能远程控制服务器。\n查看当前目录pwd 用与查看当前的工作路径，\nls 可以查看当前目录下的内容。\n文件相关文件上传与下载上传文件，使用 scp —— secure copy。需要使用本地的 bash，并输入如下命令。如果需要将整个文件夹都上传到服务器，可以先将整个文件压缩近一个压缩包，在进行上传。需要注意的是这里的 destination server 需要使用服务器的地址，别傻不拉几的填个服务器名上去。\n1$ scp [source file] [username]@[destination server]: [destination dir]使用的一些选项：\n-p: Duplicate all file attributes (permissions, timestamps) when copying.\n-r: Recursively copy a directory and its contents.\n-v: Produce verbose output, useful for debugging.\n目前的日常使用中，我个人并不经常使用这个命令。因为比较常用的 Bitvise SSH Client 集成了可以进行上传下载文件的 GUI，对我这种菜鸟比较友好。而博士生学长给下载的 Xmanager - Xshell 大概用起来没那么顺手。毕竟日常使用 windows， 也没折腾 Ubuntu 双系统，平时接触终端也就仅限于 git bash 这种了。所以，作为 Linux 终端菜鸟，还要学习一个。\n文件解压缩Linux支持很多不同格式的压缩文件，因此解压缩的命令就各有不同。这篇博客就收录了常用压缩格式对应的解压命令。目前，通常使用 .zip 进行压缩\n1$ unzip [FileName.zip].tar 是对文件的打包而非压缩，也是比较常用的文件格式\n12$ tar xvf FileName.tar           # 解包$ tar cvf FileName.tar DirName   # 打包后台运行在当前终端运行一个脚本时，该终端会不占用而无法进行其他工作。比如在训练模型是想要查看GPU的使用情况，或者断开与远程服务器的连接但保持训练的进行。这时我们就需要借助 tmux 这个工具。tmux 是一个终端选择器（terminal multiplexer），它可以让用户在一个终端窗口中控制多个终端。对于连接远程服务器的工作有很大的帮助。\n1$ tmux                      # create a new terminal session在创建了新的 terminal session 之后，窗口会显示新的终端界面。想要退出新的终端界面，则可以在使用 ctrl+b 前缀之后再按 d （表示detached），这样就返回最初的终端。现在可以查看一共创建过多少个 terminal session：\n1$ tmux ls                   # list all terminal sessions键入以上的命令之后就可以看到下面的一段显示\n123ruihao@PremilabServer01:~$ tmux ls0: 1 windows (created Mon Apr  2 17:19:35 2018) [100x34]1: 1 windows (created Mon Apr  2 19:01:14 2018) [100x34]每一行就是一个 terminal session， 最前面的就是它们的索引。我们可以通过这些索引来进入我们想要的 session。\n1$ tmux attach -t 0这样就可以进入上面编号为0的 session 查看程序的执行进度或者执行其他操作。运用 tmux 能够保证进程在断开与服务器连接之后依然正常运行。服务器跑着模型，这别就可以直接溜了溜了，美滋滋。\nGPU使用nvidia-smi  可以查看GPU使用情况\nexport CUDA_VISIBLE_DEVICES=0 指定可用的GPU资源，防止占用所有GPU的显存 \n远程查看server端的log实验室server的框架是Tensorlow，它为我们提供了 tensorboard —— 一个很好的记录模型信息、训练过程并将其可视化的工具。该工具使用本地的port，并在浏览器上显示可视化的内容。然而通常情况下server端都是没有GUI的，因此我们需要一些额外的操作，才能将 tensorboard 提供的信息，在我们自己的PC上显示并查看。当然，PC和server并不一定处于同一个网络下，这里现只讨论处在同一网络下的查看方法。Github上有一段简单的教程可以作为参考。首先，在server的终端上正常使用 tensorboard 的命令\n1$ tensorboard --log path_to_log_files然后在本地的浏览器中输入server的地址再加上port的编号即可。\nReferencenvidia-smi 命令解读\nThe Linux Process Principle, PID、PGID、PPID、SID、TID、TTY\nShow All Running Processes in Linux\nstackoverflow - select GPU to use\ntmux cheat sheet\ntmux - a very simple beginner’s guide\n","plink":"https://magi003769.github.io/post/linux命令行/"},{"title":"Latex 代码高亮问题","date":"2018-03-25T07:00:00.000Z","date_formatted":{"ll":"Mar 25, 2018","L":"03/25/2018","MM-DD":"03-25"},"updated":"2020-05-10T08:11:18.000Z","content":"今天是这学期第一次写报告，好些个月没使 LaTex 竟然有些手生，又遇到了一些关于代码高亮的问题，于是就写了这个简短的 post 来记录一下问题和解决方法。作为一个理工科的学生，报告里面插代码是基本技能。基础的就是 listings 包的使用。\n12345678910111213141516171819\\usepackage&#123;listings&#125;\\definecolor&#123;vgreen&#125;&#123;RGB&#125;&#123;104,180,104&#125;\\definecolor&#123;vblue&#125;&#123;RGB&#125;&#123;49,49,255&#125;\\definecolor&#123;vorange&#125;&#123;RGB&#125;&#123;255,143,102&#125;\\lstset&#123;\tlanguage&#x3D;Verilog,                  % set language\tbasicstyle&#x3D;\\small\\ttfamily,        % set code style\tkeywordstyle&#x3D;\\color&#123;vblue&#125;,\tidentifierstyle&#x3D;\\color&#123;black&#125;,\tcommentstyle&#x3D;\\color&#123;vgreen&#125;,\tnumbers&#x3D;left,                      % set line numbers\tnumberstyle&#x3D;&#123;\\tiny \\color&#123;black&#125;&#125;, % set fonts of line numbers\tframe&#x3D;single,                      % set type of open \tnumbersep&#x3D;10pt,\tbreaklines&#x3D;true,                   % automatic line break\ttabsize&#x3D;4&#125;MATLAB 高亮前面的都是这些一些比较 general 的代码块设定。而不管是当时数学建模还是平时 coursework 的报告，MATLAB用的比较多（毕竟EE工科狗，不是CS成功人士），所以这里专门一个section写 MATLAB 的代码高亮。\n1234567% use mcode.sty file to highlight Matlab code\\usepackage[framed,numbered,autolinebreaks,useliterate]&#123;mcode&#125; \\begin&#123;document&#125;    \\textbf&#123;\\textcolor[rgb]&#123;0.98,0.00,0.00&#125;&#123;xxxx&#125;&#125;    \\lstinputlisting[language &#x3D; Matlab]&#123;.&#x2F;code&#x2F;xxxx.m&#125;\\end&#123;document&#125;以上这段 LaTex 也确实可以实现一般的MATLAB代码在文中高亮，然而在这次写报告的过程中，因为使用到了 Image Processing ToolBox，有许多参数的形式是以字符串的形式传进函数的。这样就无法避免单引号的使用，而这些单引号会引起许多迷之报错。这里并不深入讨论造成问题的具体原因和 LaTex 的相关机制，只提供一个可替代的方法来解决问题。\n123456\\usepackage&#123;matlab-prettifier&#125;\\begin&#123;document&#125;    \\textbf&#123;\\textcolor[rgb]&#123;0.98,0.00,0.00&#125;&#123;xxxx&#125;&#125;    \\lstinputlisting[language &#x3D; Matlab]&#123;.&#x2F;code&#x2F;xxxx.m&#125;\\end&#123;document&#125;matlab pretifier 是一个最早在 ctan 上发布的包，现在在 textlive 上面也可以使用了。它在 mcode.sty 的基础上做了许多方面的改进。用设置 lstlisting 的设定就可以方便的规范 MATLAB 代码，并有非常美观的关键字高亮，省去了手动设定颜色的麻烦。\n最后，“Search before you ask”。细节的问题目前还没碰到，各种各样的问题总会不断的出现，还是要没事儿谷歌自己去搜索问题解决的方法，多看官方文档（Reference有提到的包的文档）。毕竟面向谷歌编程，面向 Stack Overflow 编程。\n\nReferenceListings Package Doc\nmatlab-pretifier Doc\nwhat-is-the-difference-between-usepackagemcode-and-usepackagelistings\n","plink":"https://magi003769.github.io/post/latex代码高亮/"},{"title":"C++ vector","date":"2018-03-16T02:55:30.000Z","date_formatted":{"ll":"Mar 15, 2018","L":"03/15/2018","MM-DD":"03-15"},"updated":"2019-05-04T04:27:40.000Z","content":"st::vector is a common container in C++. Unlike C-style array with a fixed size that needs definition in advance, it provides a dynamic size and much more convenient methods to access the items in vector. Additionally, st::vector has a tool called iterator which is quite useful when implementation some algorithms such as various kinds of sort schemes. With the help of iterator, we can get rid of the C-style coding that uses indices. \nConstructorWhen creating a vector, we need firstly specify the data type of its item, a built-in type or user-define class. The argument of vector constructor can be: (1) size of vector, (2) iterator of another existent vector. Following block contains different types of constructor. We can see that the creation of vector nums is  a special case origins from a C-style array myints. The parameter passed into constructor are actually treated as two iterators although they refers to a C-style array object. \n12345678910111213141516std::vector&lt;int&gt; first;                                // empty vector of intsstd::vector&lt;int&gt; second (4,100);                       // four ints with value 100std::vector&lt;int&gt; third (second.begin(),second.end());  // iterating through secondstd::vector&lt;int&gt; fourth (third);                       // a copy of third// the iterator constructor can also be used to construct from arrays:int counts = 20;int myints[counts];for(int i=0; i&lt;counts; ++i) myints[i] = rand() % 100 + 1;   // generate random numbersstd::vector&lt;int&gt; nums(myints, myints + sizeof(myints) / sizeof(int));// output the vector:std::cout &lt;&lt; \"Unsorted array:\\n\";for(std::vector&lt;int&gt;::iterator i=nums.begin(); i!=nums.end(); ++i)    std::cout &lt;&lt; *i &lt;&lt; ' ';Iterator12345678std::vector&lt;int&gt; myvector;for(int i=1; i&lt;=10; ++i) myvector.push_back(i);// a vector of &#123;1, 2, 3, 4, ..., 9, 10&#125;std::cout &lt;&lt; *myvector.begin();      // exactly the first element 1std::cout &lt;&lt; *myvector.begin() + 9   // the last element 10std::cout &lt;&lt; *myvector.end();        // output is 0Add or delete elements in vectorFor a simple case, push_back is a public member function of vector class to add element at the end while pop_back is the one to delete element at the end. In most cases, however, we need to do some operations on intermediate elements in vector. Thus methods insert and erase make it possible. \n12345678910111213141516171819std::vector&lt;int&gt; myvector(3,100);std::vector&lt;int&gt;::iterator it;it = myvector.begin();it = myvector.insert ( it , 200 );   // add 200 at the beginning                                     // function returns another iterator of the beginningmyvector.insert (it,2,300);          // add two 300 at beginningit = myvector.begin();               // \"it\" no longer valid as beginning, get a new onestd::vector&lt;int&gt; anothervector(2,400);// insert another vector: we MUST use the two iterators of the inserted vector// the insert position is before the iterator of it+2 which refers to 200myvector.insert (it+2,anothervector.begin(),anothervector.end()); int myarray [] = &#123; 501,502,503 &#125;;myvector.insert (myvector.begin(), myarray, myarray+3);// myvector contains: [501, 502, 503, 300, 300, 400, 400, 200, 100, 100, 100]My intuitive explanation is that the insertion happens at position “before” the iterator. It is just like the typing pattern when you are typing. For instance, after line 12 in block above, iterator refers to the beginning of vector which currently contains {300, 300, 200, 100, 100, 100}. Thus, in line 17, it+2 refers to the third element, 200. The insertion of new elements occurs before 200 and vector now becomes {300, 300, 400, 400, 200, 100, 100}. \n12345678910std::vector&lt;int&gt; myvector;// set some values (from 1 to 10)for (int i=1; i&lt;=10; i++) myvector.push_back(i);// erase the 6th elementmyvector.erase (myvector.begin()+5);// erase the first 3 elements:myvector.erase (myvector.begin(),myvector.begin()+3);Referencecplusplus.com: vector\nHow to print out the contents of a vector?\nDifference between const_iterator and iterator\nSTL: Iterators for vector\n","plink":"https://magi003769.github.io/post/cpp-vector/"},{"title":"C++入门到放弃——指针和引用","date":"2018-03-08T03:55:30.000Z","date_formatted":{"ll":"Mar 7, 2018","L":"03/07/2018","MM-DD":"03-07"},"updated":"2019-05-04T04:33:06.000Z","content":"引用（Reference）\nA reference defines an alternative name for an object\n\n引用实际上并不是一个对象，它只是给已有的变量赋予了另外一个名字。当我们初始化一个变量时，我们会将initializer的值复制给这个我们正在创造的对象。而当我们初始化一个引用的时候，我们是将其initializer的引用和我们正在生成的引用绑定在一起。一旦引用被初始化，它就被限制在这个初始对象上，不能再将其与另外对象的引用重新绑定。所以，引用是必须被初始化的，而不能只是简单的“声明”。\n12345int iVal = 1024;         // define an int object named by iValint &amp;refVal = iVal;      // define a reference bound with reference of iValrefVal = 2;              // assigns 2 to the object that refVal refVal refers toint ii = refVal;         // the same as ii = iVal只有对象才有引用，而引用本身并不是一个对象，所以我们不能定义一个引用的引用。另外当我们用右值（比如一个具体数字）对引用进行初始化时，会出现错误。而当在命令之前加上const之后，就是可以通过编译了。但具体为什么，和这个命令的实际意义，在后面的学习再做探讨。\n12int &amp;a = 10;           // illegal, error occursconst int &amp;b = 11;     // legal, BUT, what's the meaning???指针（Pointer）指针也是一种compound type， 它和引用一样也用于间接使用其他对象。但和引用不同的是指针是一个对象，它可以被赋值、赋值；且一个至真可以在lifetime结束之前可以指向多个不同的对象。指针在定义是不一定要初始化，在需要它指向一些对象时使用即可。\n12int ival = 12;int *p = &amp;ival;      // p holds the address of ival; p is a pointer to ival在 p 定义为指向 ival 的指针后，p 实际上存储的是ival 的地址。当我们试图输出p 时，我们会看到一串16进制数，那就是 ival 的地址。只有在输出 *p 时我们才会看到 ival 在对应内存为之中所存储的数值。\n1234double dval;double *pd = dval;   // illegal: initializer is an int object rather than addressdouble *pd = &amp;dval;  // valid: initializer is the address of a doubledouble *pd2 = pd;    // valis: initializer is a pointer as well指针的值：\n存储在指针中的值可以是一下四种情况中的一种\n指向一个对象\n指向刚好过一个对象结尾处的位置 （WTF？？？）\n可以是一个空指针 null pointer\n也可以是invalid，不是上述三类中的\n第二类我实在是有点懵逼啊…\n两个运算符：&amp; and *\n&amp;:  address-of operator. Yields the address of the object which it is applied.\n*: Dereference operator. Dereference a pointer returns the object to which the pointer points. Assigning to the result of dereference assigns a new value to the underlying object.\n\n&amp; 除了有bitwise AND这样的逻辑运算意义之外，一个重要的作用就是获取变量的地址。它可以用在各种不同类请的变量上，只要变量是一个已经定义的对象。指针也是一个对象，也有地址。因此 &amp; 也可以用在指针变量上。而 &amp; 却只能应用于指针上。指针存储的是一个地址，而其他类型的变量对应的内存位置存储的是一个数值。\n12345678910111213141516int a = 100;int &amp;A = a;int *ptrA = &amp;A, *ptra = &amp;a;std::cout &lt;&lt; \"Address of a: \" &lt;&lt; &amp;a &lt;&lt; std::endl;std::cout &lt;&lt; \"Address of A: \" &lt;&lt; &amp;A &lt;&lt; std::endl;std::cout &lt;&lt; std::endl &lt;&lt; \"Pointer ptrA\" &lt;&lt; std::endl;std::cout &lt;&lt; \"Address of ptrA: \" &lt;&lt; &amp;ptrA &lt;&lt; std::endl;std::cout &lt;&lt; \"Stored value of ptrA: \" &lt;&lt; ptrA &lt;&lt; std::endl;std::cout &lt;&lt; \"The value of object pointed by ptrA: \" &lt;&lt; *ptrA &lt;&lt; std::endl;std::cout &lt;&lt; std::endl &lt;&lt; \"Pointer ptra\" &lt;&lt; std::endl;std::cout &lt;&lt; \"Address of ptra: \" &lt;&lt; &amp;ptra &lt;&lt; std::endl;std::cout &lt;&lt; \"Stored value of ptra: \" &lt;&lt; ptra &lt;&lt; std::endl;std::cout &lt;&lt; \"The value of object pointed by ptra: \" &lt;&lt; *ptra &lt;&lt; std::endl;下面是上面代码片段的输出。ptrA 和 ptra 作为两个指针有着自己不同的内存地址，在这两个内存位置又分别存储了 A 和 a 的地址。而作为同一对象的两个引用，A 和 a 有着同样的内存地址。所以两个指针实际上是指向的是同一个对象，存储完全相同的内存地址，* 运算符dereference之后得到的值也就是一样的了。\n\n","plink":"https://magi003769.github.io/post/指针和引用/"},{"title":"机器学习模型——感知机","date":"2018-03-07T03:55:30.000Z","date_formatted":{"ll":"Mar 6, 2018","L":"03/06/2018","MM-DD":"03-06"},"updated":"2020-05-10T07:57:44.000Z","content":"感知机模型假设特征空间是 \n\\mathcal{X} \\in \\mathbf{R}^n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n ，输出空间是 \n\\mathcal{Y} \\in \\{+1, -1\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n。输入 \nx \\in \\mathcal{X}\n\n\n\n\n\n\n \n \n \n\n 表示特征向量，对应于特征空间的点；输出 \ny\\in\\mathcal{Y}\n\n\n\n\n\n\n \n \n \n\n 表示实例的类别。由输入空间到输出空间的如下函数\n\nf(x) = sign(w \\cdot x + b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n即称为感知机。其中，\nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 为感知机的模型参数，\nw \\in \\mathbf{R}^n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n 叫做权值向量（weight vector），\nb \\in \\mathbf{R}\n\n\n\n\n\n\n \n \n \n\n 称为偏置（bias），\nsign\n\n\n\n\n\n\n\n \n \n \n \n\n 则是符号函数\n\nsign(x) = \\left\\{ \n\\begin{aligned}\n+1&amp;, \\quad x \\geq0 \\\\\n-1&amp;, \\quad x &lt; 0\n\\end{aligned}\n\\right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\n 感知机是一种线性分类模型，属于判别模型。感知机的假设空间是定义在特征空间上的线性分类模型或先行分类器，即函数集合 \n\\{f|f(x) = w \\cdot x + b\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 。从上面的 \nsign\n\n\n\n\n\n\n\n \n \n \n \n\n 函数我们可以看出，其括号内部分等于零是一个边界条件，于是我们可以得到一个等式\n\nw \\cdot x + b = 0\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n其对应于特征空间中 \n\\mathbf{R}^n\n\n\n\n\n\n \n \n\n 中的一个超平面 \n\\mathbf{S}\n\n\n\n\n \n\n , 而 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 则分别这个超平面的法向量和截距。这个平面将特征空间分成两部分，并将其中的特征向量（特征空间中的点）分成正负两类，因此又被称为分离超平面。\n感知机学习策略数据集的线性可分性对于一个数据集 \nT\n\n\n\n\n \n\n ，如果存在某个超平面 \nS\n\n\n\n\n \n\n 能够将数据集的正实例点和负实例点分开完全正确的分开在其两侧，则称该数据及线性可分；否则，该数据集线性不可分。\n感知机学习策略这一模型学习的最终目标就是找到可以分离数据集的超平面，即确定模型参数 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 。这样就需要确定一个损失函数。一个很自然的想法就是将误分类点作为目标函数，使得误分类点的个数最小。但这样这个函数并不是参数 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 的连续可导函数，不易优化。由此，损失函数的另一种选择就变成了误分类点到超平面的总距离。输入空间 \n\\mathbf{R}^n\n\n\n\n\n\n \n \n\n 中的任何一个点 \nx_0\n\n\n\n\n\n \n \n\n 到超平面的距离为\n\n\\frac{1}{||w||}|w \\cdot x + b|\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n\n这里，\n||w||\n\n\n\n\n\n \n \n \n \n \n\n 是 \nw\n\n\n\n\n \n\n 的 \nL_2\n\n\n\n\n\n \n \n\n 范数。而在 \n\\mathbf{R}^n\n\n\n\n\n\n \n \n\n 空间，即权值空间（weight space）中， \nL_2\n\n\n\n\n\n \n \n\n 范数可以理解为一个n维响亮的模长。其次，当对于误分类的数据 \n(x_i, y_i)\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n 来讲始终存在\n\n-y_i (w \\cdot x + b) &gt; 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n\n当 \nw \\cdot x + b &gt; 0\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 时，\ny_i = -1\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n；而 \nw \\cdot x + b &lt; 0\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 时，\ny_i = 1\n\n\n\n\n\n\n\n \n \n \n \n\n。因此，误差分类点 \nx_i\n\n\n\n\n\n \n \n\n 到超平面的距离是\n\n-\\frac{1}{||w||} y_i  (w \\cdot x + b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n \n \n \n \n\n\n\n\n \n \n\n \n \n \n \n \n \n \n\n\n若误差分类点集合为 \nM\n\n\n\n\n \n\n，则总距离为\n\n-\\frac{1}{||w||} \\sum_{x_i \\in M} y_i  (w \\cdot x + b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n \n \n \n \n\n\n\n\n \n\n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n\n\n而最终感知机学习的损失函数，是不考虑 \n 1 / ||w||\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的，而上面的式子使具有实际意义的几何间隔，而感知机最终选用的是函数间隔。这个概念在李航书中的第七章SVM会有介绍。\n\nL(w,b) = - \\sum_{x_i \\in M}y_i  (w \\cdot x + b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n\n\n有一点是需要我们非常明确的，那就是这个loss function只会考虑那些错分类点（集合M）。只要不存在误分类点的话，那么这个loss就是0。对于 \n 1 / ||w||\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的忽略，有一个比较intuitive但并不严格解释是 [3]，感知机的loss是误分类驱动的，只要最后不存在误分类的情况就可以，这是loss本身已经为零，并不存在了。而且集合距离和函数距离在这个情况下是线性关系，因此也没有必要在loss里面引入范数徒增计算量，只要在 \nw\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 的调整中，确定了最终不存在误分类的超平面就好了。比较数学一点的解释，可以看 [2], 之后可能也会更新关于忽略范数的这个问题。\n感知机学习算法在上面的策略决定之后，学习过程就转化成了损失函数的最优化问题：寻找使得上一章节中公式7的loss最小的权值矩阵和常数项。这一算法是误分类驱动的，最优化的方法是随机梯度下降法（stochastic gradient descent）。算法步骤可见下图伪代码。\n\n需要注意的是，我们不能随机取出样本进行多次的迭代，而应该在每个训练季里面遍历所有的样本，这样才能够更大几率的正确学习。另外，比起将初值定为0，使用正态分布的随机数来初始化参数，能够更好更快的地学习。这都是在下面的实现中发现的规律，可能仅仅适用于比较低维的情况，在一些参考的实现中，0初始和随机取样多次迭代也最终能够争取预测。\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cov_mat = [[0.09, 0.08], [0.08, 0.09]]def data_generation():    '''    Generate two groups of data based on 2D normal distribution        Returns    -------    X: data as features for training    y: labels has value 0 or 1    '''    n, dim = 100, 2    pos = np.random.multivariate_normal(mean=[1, 1], cov=cov_mat, size=n)    neg = np.random.multivariate_normal(mean=[1.5, 0.5], cov=cov_mat, size=n)    X = np.r_[pos, neg]    y = np.hstack((np.zeros(n), np.ones(n))).astype(np.int)    print(y.shape)    return X, yclass Perceptron(object):        def __init__(self, learning_rate, epoch):        self.learning_rate = learning_rate        self.max_iteration = epoch                def train_all(self, features, labels):    \t'''        Arguments        ---------        features: (N, n) numpy array, N samples of n-dimension feature        labels: (N, ) numpy array, N labels        Returns        -------        None, only modify attributes of perceptron        '''        self.w = np.random.randn(features[0].shape[0] + 1) # initialize weight matrix w with bias b                for epoch in range(self.max_iteration):            bingo = 0            for i in range(len(features)):                x = np.hstack((features[i], np.ones(1)))                y = 2 * labels[i] - 1                wx = sum(self.w * x)       # calculate wX_i + b                                if wx * y &gt; 0:                    bingo += 1                else:                    self.w += self.learning_rate * (y * x)            print(\"Epoch &#123;&#125;, Bingo &#123;&#125;\".format(epoch+1, bingo))                        def predict_one(self, x):        return int( sum(self.w * x) &gt;0 )                def predict(self, features):        labels = []        for feature in features:            x = list(feature)            x.append(1)            labels.append(self.predict_one(x))        return labels在0.005的学习率下经过30个训练季之后的可视化划分超平面如。虽然依然存在误分类点，但是基本可以大致解决该特征平面内的二分类问题了。\n\nReferenceLink’s Blog: Perceptron \n感知机的损失函数为什么可以采用函数间隔(忽略1/||w||)?\nCSDN-为什么忽略\n1/||w||\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\nwikipedia: Stochastic gradient descent\nPerceptron tutorial\n","plink":"https://magi003769.github.io/post/感知机/"},{"title":"统计学习基本概念（2）","date":"2018-03-01T03:55:30.000Z","date_formatted":{"ll":"Feb 28, 2018","L":"02/28/2018","MM-DD":"02-28"},"updated":"2020-05-10T08:00:34.000Z","content":"模型评估与模型选择训练误差和测试误差训练误差的大小，对判断给定问题是不是一个容易学习的问题是有意义的，但本质上不重要。测试误差反映了学习方法对未知的测试集数据的预测能力，是学习的重要概念。这样的能力也被称为模型的泛化能力，因此测试误差有时也称为泛化误差（西瓜书就用了这一描述）。\n过拟合（overfitting）以为的追求对训练数据的预测能力，使用参数过多的模型，而导致模型在未知的测试数据集上表现很差。这种现象被称为过拟合。一般来说，随着模型复杂度的增加，训练误差是会不断减小的。而测试误差则是先减小，在模型复杂度到达一定程度之后有增大。在学习过程中，要防止过拟合，选择适当复杂度的模型，达到测试误差最小的学习目的。于是有了下面介绍的模型选择方法。\n\n实际上，除了模型复杂度过高之外，训练迭代次数过多也有可能导致过拟合。就好比复习只准备一种类型的题目，考试的时候稍微一变就不能解决了。这些都属于过度追求在训练集上拿到非常高的准确率，而忽略了模型的泛化能力。\n正则化和交叉验证这一部分来讲我觉得可以看一下西瓜书的Chapter2， 里面有更加详细的解释。李航的这本对正则化做了不错的说明，但是交叉验证只是说了是什么、如何去做。\n正则化正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则化项或罚项。在讨论正则化之前，需要先普及一下范数的概念。这里先不讨论范数的具体意义，先说范数是如何定义和计算的。（具体的意义可以看一下Reference）\n\n\nL_0\n\n\n\n\n\n \n \n\n 范数：向量中非零非零的元素个数\n\nL_1\n\n\n\n\n\n \n \n\n 范数：各元素绝对值之和\n\nL_2\n\n\n\n\n\n \n \n\n 范数：平方和开根号\n\n范数可以作为一个衡量模型复杂度的标准，作为正则项来修改原有的经验风险。模型越复杂，参数向量包含的值越多，范数值就越大。正符合模型越复杂，正则项值越大的标准。\n交叉验证如果有充足的样本数据，交叉验证法将数据随机分成三组：\n训练集：训练模型\n验证集：模型选择\n测试集：用于对学习方法的评估\n简单的交叉验证只将数据分成训练集和测试集两个部分，在不同的条件下（不同模型复杂度、不同参数）训练模型，再用测试集评估，选出测试误差最小的模型。另外就是S-fold交叉验证，这一方法将数据集分成S个子集，S-1个子集用于训练，剩下一个子集用于测试。\n泛化能力泛化误差泛化能力（Generalization Ability）反映了模型对未知数据的预测能力。现实中我们依赖测试集得出的测试误差来评估泛化能力，然而测试集总是有限的，很有可能由此得到的结果并不可靠。于是，我们用泛化误差来衡量这一能力。假设模型为 \n\\hat f\n\n\n\n\n\n \n \n\n ，则泛化误差可表示为MATHJAX-SSR-544这个实际上就是所学习到模型的期望风险。因此，我们就要讨论，理想的期望风险和实际使用的经验风险之间的关系，于是就出现后面的泛化误差上界。\n泛化误差上界泛化能力分析，一般通过研究泛化误差的概率上线进行。泛化误差上界通常有一下两个性质：\n\n泛化误差上界：\n是样本容量的函数，样本容量越大，泛化上界越趋近于零\n是假设空间容量的函数，假设空间越大，模型越难学习，泛化上界就越大\n\n以二分类问题为例，\nX \\in \\mathbf{R}^n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，\nY \\in \\{-1, +1\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n.\n\n对于二分类问题，当假设空间为是有限个函数的集合 \n\\mathcal{F} = \\{ f_1, .... f_d\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n 时，对其中任一函数，至少以概率 \n 1-\\delta \n\n\n\n\n\n\n \n \n \n\n , 以下不等式成立：\n\n&gt; R(f) \\leq \\hat R(f) + \\epsilon(d,N,\\delta)\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中，\n\n&gt; \\epsilon(d,N,\\delta) = \\sqrt{\\frac{1}{2N}(logd+log\\frac{1}{\\delta})}\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n\n\n\n\n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n\n\n \n\n\n\n不等式左端是泛化误差（期望风险），右端即为泛化误差上界。通过不断训练，右侧第一项的经验风险会减小。\n\\epsilon\n\n\n\n\n \n\n 项表示期望风险和实际使用的经验风险之间的差值。它关于样本量 \nN\n\n\n\n\n \n\n 是一个单调递减的函数。同时还是一个 \n\\sqrt{logd}\n\n\n\n\n\n\n\n\n \n\n\n \n \n \n \n\n\n 的函数，函数个数 \nd\n\n\n\n\n \n\n 越大，这个差别越大，上界越高，越难学习。\n生成模型和判别模型监督学习方法可以分为：生成方法（generative）和判别方法（discrimination）。生成方法由数据的联合概率分布求出条件概率分布作为预测模型，即生成模型\n\nP(Y|X) = \\frac{P(X,Y)}{P(X)}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n\n\n这种方法称为生成方法，是因为模型表示了给定输入 \nX\n\n\n\n\n \n\n 产生输出 \nY\n\n\n\n\n \n\n 的生成关系。典型的生成模型有：朴素贝叶斯法和隐马尔科夫链。\n判别方法由数据直接学习决策函数或条件概率分布作为预测模型，即判别模型。这类方法真正关心的是给定输出 \nX\n\n\n\n\n \n\n，应该预测怎样的 \nY\n\n\n\n\n \n\n。典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。\n\n生成方法的特点：\n生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本数量增多时，学到的模型可以更快的收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，但这时判别模型就不行了。\n判别方法的特点：\n判别学习直接学习的是条件概率 \nP(Y|X)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 或决策函数 \nf(X)\n\n\n\n\n\n\n\n \n \n \n \n\n，直接面对预测，往往学习的准确率更高；由于直接学习 \nP(Y|X)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 或 \nf(X)\n\n\n\n\n\n\n\n \n \n \n \n\n，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。\n\n三类问题：分类、标注、回归分类也是一种预测，当输出变量 \nY\n\n\n\n\n \n\n 取有限个离散值的时候，预测问题便转化为一个分类问题。许多模型都可以用于分类问题：k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。\n标注标注问题可以认为是分类问题的一种推广，也可以认为是更复杂的结构预测（structure prediction）问题的简单形式。这类模型的输入输出都是一个序列。比如一个样本数据：\n\n(x_i, y_y) \\notag \\\\\nx_i =\\left (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(n)}\\right)^T \\notag \\\\\ny_i = \\left (y_i^{(1)}, y_i^{(2)}, ..., y_i^{(n)}\\right)^T \\notag \\\\\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\n\nNLP中经常出现这种带有序列的问题，书中的词性标注就是一个例子。这类问题常用的统计学方法有：隐马尔科夫链、条件随机场。\n回归回归问题用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之而发生的变化。回归问题多数时候应用于商务领域，分析市场趋势、用户满意度调查、风险投资管理等。这些场景都需要在自变量发生改变时观察结果的变化来做出决策。\nReference知乎-范数\n知乎-机器学习引入L2范数的意义\nCSDN-L0, L1, L2范数\n","plink":"https://magi003769.github.io/post/统计学习概论2/"},{"title":"pip User Guide","date":"2018-02-28T03:55:30.000Z","date_formatted":{"ll":"Feb 27, 2018","L":"02/27/2018","MM-DD":"02-27"},"updated":"2020-05-10T23:09:10.000Z","content":"This post is a collection of required commands to manage the packages. From cradle to grave, for a package, it will experience several stages:\n\nInstallation\nCheck status\nUpgrade\nUninstallation\n\nThus, following sections follow the order of these procedures.\nInstall PackagesCommon Use1$ pip install &lt;package-name&gt;Install a batch of packagesSometimes it is quite annoying to install require packages one by one. \n1$ pip install -r &lt;required-pakages.txt&gt;Upgrade Packages1$ pip install --upgrade &lt;package-name&gt;Check PackagesAn overview of all packages12345# list all packages$ pip list # list the packages need upgrading$ pip list --outdatedInformation of one specified packages12345# show the detail about a specified package$ pip show &lt;package-name&gt; ...# Show the full list of installed files for each package.$ pip show -f &lt;package-name&gt;Uninstall Packages1$ pip uninstall &lt;package-name&gt;Referencepip Official Guide\n","plink":"https://magi003769.github.io/post/package control/"},{"title":"统计学习基本概念（1）","date":"2018-02-26T03:55:30.000Z","date_formatted":{"ll":"Feb 25, 2018","L":"02/25/2018","MM-DD":"02-25"},"updated":"2020-05-10T08:00:04.000Z","content":"统计学习统计学习的特点以计算机网络为平台，建立在其上\n以数据为研究对象，Data Driven\n目的是对数据进行预测和分析\n以方法为中心：统计学习方法构建并应用模型进行预测和分析\n多学科交叉：概率论、统计学、信息论、计算理论、最优化理论及计算机科学等\n统计学习的方法统计学习方法的实现步骤\n\n得到一个有限的数据集合\n确定包含所有可能的模型的假设空间，即学习模型的集合\n确定模型选择的准则，及学习策略\n实现求解最优化模型的算法，即学习算法\n通过学习方法选择最优化模型\n利用学习的最优化模型对新数据进行预测和分析\n\n以上的步骤包含了统计学习方法的三要素：模型、策略、算法。在后面的内容中会提到。\n监督学习统计学习包括监督学习、非监督西、半监督学习以及增强学习。作为极其重要的统计学习分支，监督学习的目的是学习一个模型，使模型能能够对任意给定的输出，对其相应的输出做一个好的预测。（这里指的是系统的输入输出而不是指学习的输入和输出，两者有区别）\n基本概念输入空间、特征空间和输出空间\n特征空间的每一个维度都对应一个特征。这里我们需要着重区分特征空间和输入空间。有时二者相同，不做区分；而有时二者并不是同一空间，则需要将输入空间通过某种方法映射在特征空间上。而模型实际上是定义在特征空间上的。以原来naive的人脸识别项目为例。摄像头记录的数字图像实际上是组成了输入空间，而我们利用PCA进行了降维，利用一个特征向量表示一个实例（一张图片、一张脸）。而实际上我们的模型仅仅是计算欧氏距离来判别身份（想想当初是真鸡儿弱智）\n\nQ：这里我突然有了一个问题，像PCA和深度学习中的自编码器这些方法，究竟只能算是一个单纯降维和特征提取过程，还是说它们一定程度上也算是一种学习模型？\n\n联合概率分布（Joint Probability Distribution）监督学习假设输入与输出的随机变量 \nX\n\n\n\n\n \n\n 和 \nY\n\n\n\n\n \n\n 遵循联合分布 \nP(X,Y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n. 统计学习假设数据存在一定的统计规律。在学习过程中这也分布被假设存在，而对学习系统来讲，这个分布是未知的。训练数据和测试数据都是依据这一联合分布 \nP(X, Y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 独立同分布产生的。统计学习假设数据存在一定的统计规律，\nX\n\n\n\n\n \n\n 和 \nY\n\n\n\n\n \n\n 具有联合分布是监督学习的一个基本假设。\n假设空间（hypothesis space）监督学习的目的在于学习一个有输入到输出的映射，这一映射由模型来表示。模型属于由输入空间到输出空间的映射的集合，这个集合被称为假设空间。假设空间的确定，意味着学习范围的确定。\n监督学习的模型可以是概率模型或非概率模型，用条件概率 \nP(Y|X)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 或决策方程（decision function）\nY = f(X)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 来表示，对于具体的输入（某一实例）进行输出预测时变量换成小写。\n问题形式化\n统计学习三要素统计学习方法都是由模型、策略和算法构成的，即统计学习方法由三要素构成\n\n方法=模型+策略+算法\n\n\n\n\n\n方\n\n法\n\n \n\n模\n\n\n型\n\n \n\n策\n\n\n略\n\n \n\n算\n\n\n法\n\n\n\n以下讨论的是监督学习中的统计学学习三要素。非监督学习、强化学习也同样拥有这三要素。可以说一种统计学习方法就是确定具体的同喜学习三要素。\n模型如前面所说，模型就是所要学习的条件概率分布或者决策函数。而模型的假设空间则包含了所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。\n假设空间 \n\\mathcal{F}\n\n\n\n\n \n\n 表示，则假设空间可表示为集合：\n\n\\mathcal{F} = \\{ \\mathcal{f} | Y = \\mathcal{f}(X) \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中，\nX\n\n\n\n\n \n\n 和 \nY\n\n\n\n\n \n\n 是定义在输入空间 \n\\mathcal{X}\n\n\n\n\n \n\n 和输出空间 \n\\mathcal{Y}\n\n\n\n\n \n\n 上的变量。这时候 \n\\mathcal{F}\n\n\n\n\n \n\n 通常是有一个参数向量 \n\\theta\n\n\n\n\n \n\n 决定的函数组：\n\n\\mathcal{F} = \\{ \\mathcal{f} | Y = \\mathcal{f_\\theta}(X), \\theta \\in \\mathbf{R}^n \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n\n\n参数向量 \n\\theta\n\n\n\n\n \n\n 取值于n维欧式空间 \n\\mathbf{R}^n\n\n\n\n\n\n \n \n\n ，成为参数空间（parameter space）。例如在训练神经网络的过程中，BP算法就是通过调整layer之间的权值和卷积核内的参数来优化模型，从而使模型可以得到比较准确的预测结果。而调参的工作则是调整模型的一些超参数（hyper-parameters）。这些超参数往往是训练过程中不能改变的，例如卷积网络中卷积核的大小，后面结构风险中的系数 \n\\lambda\n\n\n\n\n \n\n 等在目标函数设计时的参数。\n策略损失函数和风险函数现在既然已经确定了模型和假设空间，那么就需要考虑用什么样的准则来学习或者说得到假设空间中的最优模型。首先引入损失函数 / 代价函数 **（loss function / cost function）与风险函数**（risk function）的概念。\n\n损失函数  / 代价函数：度量一次模型预测的好坏\n风险函数：度量平均意义下模型预测的好坏\n\n我们用一个损失函数或者代价函数来衡量预测错误的程度。这一函数是预测结果 \nf(X)\n\n\n\n\n\n\n\n \n \n \n \n\n 和 实际结果\nY\n\n\n\n\n \n\n 的非负实值函数，记为 \nL(Y, f(X)) \n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n。常用的有以下四种：\n0-1损失函数（0-1 Loss Function）:MATHJAX-SSR-573\n\n平方损失函数（Quadratic Loss Function）：\n\nL(f(X), Y) = (Y-f(X))^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n\n绝对损失函数（Absolute Loss Function）：\n\nL(Y, f(X)) = |Y-f(X)|\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n对数损失函数（Logarithmic Loss Function）：\n\nL(Y, P(Y|X)) = -logP(Y|X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n损失函数越小，模型效果越好。由于模型的输入、输出 \n(X,Y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 是遵循联合分布 \nP(X,Y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 的随机变量，所以损失函数的期望可以表示为\n\nR_{exp}(f) = E_p[L(Y, P(Y|X))] = \\int_{\\mathcal{X \\times Y}}L(y, P(y|x))P(x,y)dxdy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n这是理论模型 \nf(X)\n\n\n\n\n\n\n\n \n \n \n \n\n 关于联合分布 \nP(X,Y)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 的平均意义下的损失，称为风险函数或者期望损失（expected loss）。然而这个风险是关于联合分布的，而这个联合分布却正是我们要学习的东西，他是未知的。因此，对于一个具有 \nN\n\n\n\n\n \n\n 个样本的给定数据集 \nT\n\n\n\n\n \n\n，我们定义了它的平均损失，即经验风险或经验损失（empirical risk/loss），记作 \nR_{emp}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n。\n\nR_{emp}(f) = \\frac{1}{N}\\sum_{i=1}^N L(y_i, f(x_i))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n\n根据大数定律，当样本容量N趋近于无穷时，经验风险 \nR_{emp}(f)\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n\n 趋近于 \nR_{exp}(f)\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n\n。所以一个很自然的想法就是用经验风险来估计期望风险。但是由于现实中样本数目有限，甚至很小，所以这个方法往往并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化（ERM）和结构风险最小化（SRM）。\n经验风险最小化和结构风险最小化在假设空间、损失函数以及训练数据集确定的情况下，经验风险就可以确定。当样本数量足够大时ERM可以获得不错的学习效果，在现实中也被广泛采用。然而当训练数据较少时，很容易出现过拟合的现象。SRM就是为了防止过拟合而提出的策略。结构风险最小化（SRM）等价于正则化（regularization）。结构风险在经验风险上增加上了表示模型复杂度的 正则化项（regularizer）或 罚项（penalty term），在假设空间、损失函数以及训练数据集确定的情况下，结构风险定义为\n\nR_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^N L(y_i, f(x_i)) + \\lambda J(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n\n\n\nJ(f)\n\n\n\n\n\n\n\n \n \n \n \n\n 表示模型复杂度，是定义在假设空间 \n\\mathcal{F}\n\n\n\n\n \n\n 的泛函。模型 \nf\n\n\n\n\n \n\n 越复杂，复杂度 \nJ(f)\n\n\n\n\n\n\n\n \n \n \n \n\n 越大；反之，模型 \nf\n\n\n\n\n \n\n 越简单，复杂度 \nJ(f)\n\n\n\n\n\n\n\n \n \n \n \n\n 越小。因此复杂度表示的是对复杂模型的惩罚。系数 \n\\lambda \\geq 0\n\n\n\n\n\n\n \n \n \n\n 用来权衡经验风险和模型复杂度。 结构风险小需要经验风险与架构复杂度同时小。\n算法算法指的是学习模型的具体计算方法。确定了模型，知道了寻找假设空间中最优化模型的策略，接下来就应该思考如何用具体的计算方法来求解最优解了。根据前述的策略（ERM,SRM），统计学习最终回归为最优化问题。有显式的解析解是比较简单的case，而通常解析解并不存在，这就需要用数值计算的方法来求解。如何找到最优解，并且保证求解过程的高效是一大问题。\n","plink":"https://magi003769.github.io/post/统计学习概论/"},{"title":"Single Shot MultiBox Detector (SSD)","date":"2018-02-12T08:00:00.000Z","date_formatted":{"ll":"Feb 12, 2018","L":"02/12/2018","MM-DD":"02-12"},"updated":"2019-05-04T04:20:06.000Z","content":"物体检测是计算机视觉中的一个基本任务。传统方法用不同尺寸的滑窗寻找可能的物体位置，再通过在相应区域应用边缘检测或者 HOG 等特征来检测图像中物体是否存在。在深度学习出现之后，CNN 就迅速被作为特征提取的工具应用到这个领域。于是便出现了何恺明大佬著名的 R-CNN 系列。作为深度学习在物体检测中应用的开山之作，R-CNN使用 Region Proposal Network (RPN) 来确定物体的位置，并用另外一个CNN作为特征提取和分类的网络。这样，R-CNN就是一个位置预测和类别预测完全分开的 two-stage 结构。尽管R-CNN有着相当高的正确率，two-stage 结构都使得它们不论是在训练还是预测中，都需要更多时间和算力要求。在这样的背景下，主打速度的 one-stage 物体检测架构就成为了一个研究方向，SSD就是其中的一个代表。\nSSD Model\n上图是SSD整个网络结构。开始的部分即是一个经典的直筒式特征提取网络（truncated VGG-16 trough pool5 layer）。在目前的实现中，有人把这个网络换成了Inception，MobileNet 甚至 ResNet。这些基本都可以从谷歌开源的物体检测 API 中找到对应的模型。不同于一般的分类问题，物体检测的一大难题就是如何对不同尺寸大小的物体进行检测。特征提取网络后面新增构成 Pyramidal feature hierarchy 的卷积层就是为了解决这个问题。它们得到不同尺寸特征图，对应输入图片中不同的感受野，这样就能代表不同输入图片中不同尺寸的物体。\n除了尺寸问题，物体检测还需要确定图片中物体的位置。R-CNN利用 RPN 直接找出物体（虽然不同的 R-CNN 的 RPN 有着不太相同的机制），而SSD则是引入了非常类似于 anchor box 的机制，论文中称之为 default box。简单地说这些矩形区域都是以特征图上的某一个像素点作为中心，并有着不同的大小和长宽比。\n\n每一个 default box 都是一个要做预测的 candidate，这样对于整个模型，一共有6张不同尺寸的特征图需要预测，他们的尺寸分别为：\n[38\\times38,\\quad19\\times19,\\quad10\\times10,\\quad5\\times5,\\quad3\\times3,\\quad1\\times1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 。因此，我们就有7308个box需要进行识别和预测。\nTraining网络的训练是整个架构能否得到好结果的一个关键，SSD的训练里面有很多 tricky 的东西，这些也是为了解决 one-stage 结构的一些固有问题，今天就先写到这里。明天再更新。\n插播一条新闻，今天训练的时候发现连着几个训练季的 validation loss 都升高，而 training loss 则在仍在降低。后来看了这个 issue, 想想是真的蠢，训练的脚本还留着原来 fine-tuning 的权值加载，这一波直接过拟合了验证集误差不飞起来才算是出了邪了。\nQuantize the SSD architecture下面是对SSD的各层输入、输出尺寸以及连接情况的一个总结，调用 model.summary() 接口即可实现。除了处理过程中张量的尺寸，我们还能够得到整个模型需要训练的参数个数。在这个实现中整个模型有大约25.7M的参数需要训练。\nVGG16-based SSD\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 300, 300, 3)  0                                            __________________________________________________________________________________________________conv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_1[0][0]                    __________________________________________________________________________________________________conv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]                    __________________________________________________________________________________________________pool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]                    __________________________________________________________________________________________________conv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]                      __________________________________________________________________________________________________conv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]                    __________________________________________________________________________________________________pool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]                    __________________________________________________________________________________________________conv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]                      __________________________________________________________________________________________________conv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]                    __________________________________________________________________________________________________conv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]                    __________________________________________________________________________________________________pool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]                    __________________________________________________________________________________________________conv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]                      __________________________________________________________________________________________________conv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]                    __________________________________________________________________________________________________conv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]                    __________________________________________________________________________________________________pool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]                    __________________________________________________________________________________________________conv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]                      __________________________________________________________________________________________________conv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]                    __________________________________________________________________________________________________conv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]                    __________________________________________________________________________________________________pool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]                    __________________________________________________________________________________________________fc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]                      __________________________________________________________________________________________________fc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     fc6[0][0]                        __________________________________________________________________________________________________conv6_1 (Conv2D)                (None, 19, 19, 256)  262400      fc7[0][0]                        __________________________________________________________________________________________________conv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_1[0][0]                    __________________________________________________________________________________________________conv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]                    __________________________________________________________________________________________________zero_padding2d_1 (ZeroPadding2D (None, 12, 12, 128)  0           conv7_1[0][0]                    __________________________________________________________________________________________________conv7_2 (Conv2D)                (None, 5, 5, 256)    295168      zero_padding2d_1[0][0]           __________________________________________________________________________________________________conv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]                    __________________________________________________________________________________________________conv4_3_norm (Normalize)        (None, 38, 38, 512)  512         conv4_3[0][0]                    __________________________________________________________________________________________________conv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]                    __________________________________________________________________________________________________pool6 (GlobalAveragePooling2D)  (None, 256)          0           conv8_2[0][0]                    __________________________________________________________________________________________________conv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 63)   290367      conv4_3_norm[0][0]               __________________________________________________________________________________________________fc7_mbox_conf (Conv2D)          (None, 19, 19, 126)  1161342     fc7[0][0]                        __________________________________________________________________________________________________conv6_2_mbox_conf (Conv2D)      (None, 10, 10, 126)  580734      conv6_2[0][0]                    __________________________________________________________________________________________________conv7_2_mbox_conf (Conv2D)      (None, 5, 5, 126)    290430      conv7_2[0][0]                    __________________________________________________________________________________________________conv8_2_mbox_conf (Conv2D)      (None, 3, 3, 126)    290430      conv8_2[0][0]                    __________________________________________________________________________________________________conv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 12)   55308       conv4_3_norm[0][0]               __________________________________________________________________________________________________fc7_mbox_loc (Conv2D)           (None, 19, 19, 24)   221208      fc7[0][0]                        __________________________________________________________________________________________________conv6_2_mbox_loc (Conv2D)       (None, 10, 10, 24)   110616      conv6_2[0][0]                    __________________________________________________________________________________________________conv7_2_mbox_loc (Conv2D)       (None, 5, 5, 24)     55320       conv7_2[0][0]                    __________________________________________________________________________________________________conv8_2_mbox_loc (Conv2D)       (None, 3, 3, 24)     55320       conv8_2[0][0]                    __________________________________________________________________________________________________conv4_3_norm_mbox_conf_flat (Fl (None, 90972)        0           conv4_3_norm_mbox_conf[0][0]     __________________________________________________________________________________________________fc7_mbox_conf_flat (Flatten)    (None, 45486)        0           fc7_mbox_conf[0][0]              __________________________________________________________________________________________________conv6_2_mbox_conf_flat (Flatten (None, 12600)        0           conv6_2_mbox_conf[0][0]          __________________________________________________________________________________________________conv7_2_mbox_conf_flat (Flatten (None, 3150)         0           conv7_2_mbox_conf[0][0]          __________________________________________________________________________________________________conv8_2_mbox_conf_flat (Flatten (None, 1134)         0           conv8_2_mbox_conf[0][0]          __________________________________________________________________________________________________pool6_mbox_conf_flat (Dense)    (None, 126)          32382       pool6[0][0]                      __________________________________________________________________________________________________conv4_3_norm_mbox_loc_flat (Fla (None, 17328)        0           conv4_3_norm_mbox_loc[0][0]      __________________________________________________________________________________________________fc7_mbox_loc_flat (Flatten)     (None, 8664)         0           fc7_mbox_loc[0][0]               __________________________________________________________________________________________________conv6_2_mbox_loc_flat (Flatten) (None, 2400)         0           conv6_2_mbox_loc[0][0]           __________________________________________________________________________________________________conv7_2_mbox_loc_flat (Flatten) (None, 600)          0           conv7_2_mbox_loc[0][0]           __________________________________________________________________________________________________conv8_2_mbox_loc_flat (Flatten) (None, 216)          0           conv8_2_mbox_loc[0][0]           __________________________________________________________________________________________________pool6_mbox_loc_flat (Dense)     (None, 24)           6168        pool6[0][0]                      __________________________________________________________________________________________________mbox_conf (Merge)               (None, 153468)       0           conv4_3_norm_mbox_conf_flat[0][0]                                                                 fc7_mbox_conf_flat[0][0]                                                                          conv6_2_mbox_conf_flat[0][0]                                                                      conv7_2_mbox_conf_flat[0][0]                                                                      conv8_2_mbox_conf_flat[0][0]                                                                      pool6_mbox_conf_flat[0][0]       __________________________________________________________________________________________________pool6_reshaped (Reshape)        (None, 1, 1, 256)    0           pool6[0][0]                      __________________________________________________________________________________________________mbox_loc (Merge)                (None, 29232)        0           conv4_3_norm_mbox_loc_flat[0][0]                                                                  fc7_mbox_loc_flat[0][0]                                                                           conv6_2_mbox_loc_flat[0][0]                                                                       conv7_2_mbox_loc_flat[0][0]                                                                       conv8_2_mbox_loc_flat[0][0]                                                                       pool6_mbox_loc_flat[0][0]        __________________________________________________________________________________________________mbox_conf_logits (Reshape)      (None, 7308, 21)     0           mbox_conf[0][0]                  __________________________________________________________________________________________________conv4_3_norm_mbox_priorbox (Pri (None, 4332, 8)      0           conv4_3_norm[0][0]               __________________________________________________________________________________________________fc7_mbox_priorbox (PriorBox)    (None, 2166, 8)      0           fc7[0][0]                        __________________________________________________________________________________________________conv6_2_mbox_priorbox (PriorBox (None, 600, 8)       0           conv6_2[0][0]                    __________________________________________________________________________________________________conv7_2_mbox_priorbox (PriorBox (None, 150, 8)       0           conv7_2[0][0]                    __________________________________________________________________________________________________conv8_2_mbox_priorbox (PriorBox (None, 54, 8)        0           conv8_2[0][0]                    __________________________________________________________________________________________________pool6_mbox_priorbox (PriorBox)  (None, 6, 8)         0           pool6_reshaped[0][0]             __________________________________________________________________________________________________mbox_loc_final (Reshape)        (None, 7308, 4)      0           mbox_loc[0][0]                   __________________________________________________________________________________________________mbox_conf_final (Activation)    (None, 7308, 21)     0           mbox_conf_logits[0][0]           __________________________________________________________________________________________________mbox_priorbox (Merge)           (None, 7308, 8)      0           conv4_3_norm_mbox_priorbox[0][0]                                                                  fc7_mbox_priorbox[0][0]                                                                           conv6_2_mbox_priorbox[0][0]                                                                       conv7_2_mbox_priorbox[0][0]                                                                       conv8_2_mbox_priorbox[0][0]                                                                       pool6_mbox_priorbox[0][0]        __________________________________________________________________________________________________predictions (Merge)             (None, 7308, 33)     0           mbox_loc_final[0][0]                                                                              mbox_conf_final[0][0]                                                                             mbox_priorbox[0][0]              ==================================================================================================Total params: 25,765,497Trainable params: 25,765,497Non-trainable params: 0当特征提取器的网络换成更为轻量级的 MobileNet.v1，我们可以看到参数的量非常显著地减少了。这对于计算资源有限的移动设备或者嵌入式设备非常友好。也是做 FYP 的一个方向之一。\nMobileNet.v1-based SSD\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 300, 300, 3)  0                                            __________________________________________________________________________________________________conv0 (Conv2D)                  (None, 150, 150, 32) 896         input_1[0][0]                    __________________________________________________________________________________________________bn0 (BatchNormalization)        (None, 150, 150, 32) 128         conv0[0][0]                      __________________________________________________________________________________________________activation_1 (Activation)       (None, 150, 150, 32) 0           bn0[0][0]                        __________________________________________________________________________________________________conv1dw (SeparableConv2D)       (None, 150, 150, 32) 1344        activation_1[0][0]               __________________________________________________________________________________________________bn1dw (BatchNormalization)      (None, 150, 150, 32) 128         conv1dw[0][0]                    __________________________________________________________________________________________________activation_2 (Activation)       (None, 150, 150, 32) 0           bn1dw[0][0]                      __________________________________________________________________________________________________conv1 (Conv2D)                  (None, 150, 150, 64) 2112        activation_2[0][0]               __________________________________________________________________________________________________bn1 (BatchNormalization)        (None, 150, 150, 64) 256         conv1[0][0]                      __________________________________________________________________________________________________activation_3 (Activation)       (None, 150, 150, 64) 0           bn1[0][0]                        __________________________________________________________________________________________________conv2dw (SeparableConv2D)       (None, 75, 75, 64)   4736        activation_3[0][0]               __________________________________________________________________________________________________bn2dw (BatchNormalization)      (None, 75, 75, 64)   256         conv2dw[0][0]                    __________________________________________________________________________________________________activation_4 (Activation)       (None, 75, 75, 64)   0           bn2dw[0][0]                      __________________________________________________________________________________________________conv2 (Conv2D)                  (None, 75, 75, 128)  8320        activation_4[0][0]               __________________________________________________________________________________________________bn2 (BatchNormalization)        (None, 75, 75, 128)  512         conv2[0][0]                      __________________________________________________________________________________________________activation_5 (Activation)       (None, 75, 75, 128)  0           bn2[0][0]                        __________________________________________________________________________________________________conv3dw (SeparableConv2D)       (None, 75, 75, 128)  17664       activation_5[0][0]               __________________________________________________________________________________________________bn3dw (BatchNormalization)      (None, 75, 75, 128)  512         conv3dw[0][0]                    __________________________________________________________________________________________________activation_6 (Activation)       (None, 75, 75, 128)  0           bn3dw[0][0]                      __________________________________________________________________________________________________conv3 (Conv2D)                  (None, 75, 75, 128)  16512       activation_6[0][0]               __________________________________________________________________________________________________bn3 (BatchNormalization)        (None, 75, 75, 128)  512         conv3[0][0]                      __________________________________________________________________________________________________activation_7 (Activation)       (None, 75, 75, 128)  0           bn3[0][0]                        __________________________________________________________________________________________________conv4dw (SeparableConv2D)       (None, 38, 38, 128)  17664       activation_7[0][0]               __________________________________________________________________________________________________bn4dw (BatchNormalization)      (None, 38, 38, 128)  512         conv4dw[0][0]                    __________________________________________________________________________________________________activation_8 (Activation)       (None, 38, 38, 128)  0           bn4dw[0][0]                      __________________________________________________________________________________________________conv4 (Conv2D)                  (None, 38, 38, 256)  33024       activation_8[0][0]               __________________________________________________________________________________________________bn4 (BatchNormalization)        (None, 38, 38, 256)  1024        conv4[0][0]                      __________________________________________________________________________________________________activation_9 (Activation)       (None, 38, 38, 256)  0           bn4[0][0]                        __________________________________________________________________________________________________conv5dw (SeparableConv2D)       (None, 38, 38, 256)  68096       activation_9[0][0]               __________________________________________________________________________________________________bn5dw (BatchNormalization)      (None, 38, 38, 256)  1024        conv5dw[0][0]                    __________________________________________________________________________________________________activation_10 (Activation)      (None, 38, 38, 256)  0           bn5dw[0][0]                      __________________________________________________________________________________________________conv5 (Conv2D)                  (None, 38, 38, 256)  65792       activation_10[0][0]              __________________________________________________________________________________________________bn5 (BatchNormalization)        (None, 38, 38, 256)  1024        conv5[0][0]                      __________________________________________________________________________________________________activation_11 (Activation)      (None, 38, 38, 256)  0           bn5[0][0]                        __________________________________________________________________________________________________conv6dw (SeparableConv2D)       (None, 19, 19, 256)  68096       activation_11[0][0]              __________________________________________________________________________________________________bn6dw (BatchNormalization)      (None, 19, 19, 256)  1024        conv6dw[0][0]                    __________________________________________________________________________________________________activation_12 (Activation)      (None, 19, 19, 256)  0           bn6dw[0][0]                      __________________________________________________________________________________________________conv6 (Conv2D)                  (None, 19, 19, 512)  131584      activation_12[0][0]              __________________________________________________________________________________________________bn6 (BatchNormalization)        (None, 19, 19, 512)  2048        conv6[0][0]                      __________________________________________________________________________________________________activation_13 (Activation)      (None, 19, 19, 512)  0           bn6[0][0]                        __________________________________________________________________________________________________conv7dw (SeparableConv2D)       (None, 19, 19, 512)  267264      activation_13[0][0]              __________________________________________________________________________________________________bn7dw (BatchNormalization)      (None, 19, 19, 512)  2048        conv7dw[0][0]                    __________________________________________________________________________________________________activation_14 (Activation)      (None, 19, 19, 512)  0           bn7dw[0][0]                      __________________________________________________________________________________________________conv7 (Conv2D)                  (None, 19, 19, 512)  262656      activation_14[0][0]              __________________________________________________________________________________________________bn7 (BatchNormalization)        (None, 19, 19, 512)  2048        conv7[0][0]                      __________________________________________________________________________________________________activation_15 (Activation)      (None, 19, 19, 512)  0           bn7[0][0]                        __________________________________________________________________________________________________conv8dw (SeparableConv2D)       (None, 19, 19, 512)  267264      activation_15[0][0]              __________________________________________________________________________________________________bn8dw (BatchNormalization)      (None, 19, 19, 512)  2048        conv8dw[0][0]                    __________________________________________________________________________________________________activation_16 (Activation)      (None, 19, 19, 512)  0           bn8dw[0][0]                      __________________________________________________________________________________________________conv8 (Conv2D)                  (None, 19, 19, 512)  262656      activation_16[0][0]              __________________________________________________________________________________________________bn8 (BatchNormalization)        (None, 19, 19, 512)  2048        conv8[0][0]                      __________________________________________________________________________________________________activation_17 (Activation)      (None, 19, 19, 512)  0           bn8[0][0]                        __________________________________________________________________________________________________conv9dw (SeparableConv2D)       (None, 19, 19, 512)  267264      activation_17[0][0]              __________________________________________________________________________________________________bn9dw (BatchNormalization)      (None, 19, 19, 512)  2048        conv9dw[0][0]                    __________________________________________________________________________________________________activation_18 (Activation)      (None, 19, 19, 512)  0           bn9dw[0][0]                      __________________________________________________________________________________________________conv9 (Conv2D)                  (None, 19, 19, 512)  262656      activation_18[0][0]              __________________________________________________________________________________________________bn9 (BatchNormalization)        (None, 19, 19, 512)  2048        conv9[0][0]                      __________________________________________________________________________________________________activation_19 (Activation)      (None, 19, 19, 512)  0           bn9[0][0]                        __________________________________________________________________________________________________conv10dw (SeparableConv2D)      (None, 19, 19, 512)  267264      activation_19[0][0]              __________________________________________________________________________________________________bn10dw (BatchNormalization)     (None, 19, 19, 512)  2048        conv10dw[0][0]                   __________________________________________________________________________________________________activation_20 (Activation)      (None, 19, 19, 512)  0           bn10dw[0][0]                     __________________________________________________________________________________________________conv10 (Conv2D)                 (None, 19, 19, 512)  262656      activation_20[0][0]              __________________________________________________________________________________________________bn10 (BatchNormalization)       (None, 19, 19, 512)  2048        conv10[0][0]                     __________________________________________________________________________________________________activation_21 (Activation)      (None, 19, 19, 512)  0           bn10[0][0]                       __________________________________________________________________________________________________conv11dw (SeparableConv2D)      (None, 19, 19, 512)  267264      activation_21[0][0]              __________________________________________________________________________________________________b11dw (BatchNormalization)      (None, 19, 19, 512)  2048        conv11dw[0][0]                   __________________________________________________________________________________________________activation_22 (Activation)      (None, 19, 19, 512)  0           b11dw[0][0]                      __________________________________________________________________________________________________conv11 (Conv2D)                 (None, 19, 19, 512)  262656      activation_22[0][0]              __________________________________________________________________________________________________bn11 (BatchNormalization)       (None, 19, 19, 512)  2048        conv11[0][0]                     __________________________________________________________________________________________________activation_23 (Activation)      (None, 19, 19, 512)  0           bn11[0][0]                       __________________________________________________________________________________________________conv12dw (SeparableConv2D)      (None, 10, 10, 512)  267264      activation_23[0][0]              __________________________________________________________________________________________________bn12dw (BatchNormalization)     (None, 10, 10, 512)  2048        conv12dw[0][0]                   __________________________________________________________________________________________________activation_24 (Activation)      (None, 10, 10, 512)  0           bn12dw[0][0]                     __________________________________________________________________________________________________conv12 (Conv2D)                 (None, 10, 10, 1024) 525312      activation_24[0][0]              __________________________________________________________________________________________________bn12 (BatchNormalization)       (None, 10, 10, 1024) 4096        conv12[0][0]                     __________________________________________________________________________________________________activation_25 (Activation)      (None, 10, 10, 1024) 0           bn12[0][0]                       __________________________________________________________________________________________________conv13dw (SeparableConv2D)      (None, 10, 10, 1024) 1058816     activation_25[0][0]              __________________________________________________________________________________________________bn13dw (BatchNormalization)     (None, 10, 10, 1024) 4096        conv13dw[0][0]                   __________________________________________________________________________________________________activation_26 (Activation)      (None, 10, 10, 1024) 0           bn13dw[0][0]                     __________________________________________________________________________________________________conv13 (Conv2D)                 (None, 10, 10, 1024) 1049600     activation_26[0][0]              __________________________________________________________________________________________________bn13 (BatchNormalization)       (None, 10, 10, 1024) 4096        conv13[0][0]                     __________________________________________________________________________________________________activation_27 (Activation)      (None, 10, 10, 1024) 0           bn13[0][0]                       __________________________________________________________________________________________________conv14_1 (Conv2D)               (None, 10, 10, 256)  262400      activation_27[0][0]              __________________________________________________________________________________________________bn14_1 (BatchNormalization)     (None, 10, 10, 256)  1024        conv14_1[0][0]                   __________________________________________________________________________________________________activation_28 (Activation)      (None, 10, 10, 256)  0           bn14_1[0][0]                     __________________________________________________________________________________________________conv14_2 (Conv2D)               (None, 5, 5, 512)    1180160     activation_28[0][0]              __________________________________________________________________________________________________bn14_2 (BatchNormalization)     (None, 5, 5, 512)    2048        conv14_2[0][0]                   __________________________________________________________________________________________________activation_29 (Activation)      (None, 5, 5, 512)    0           bn14_2[0][0]                     __________________________________________________________________________________________________conv15_1 (Conv2D)               (None, 5, 5, 128)    65664       activation_29[0][0]              __________________________________________________________________________________________________bn15_1 (BatchNormalization)     (None, 5, 5, 128)    512         conv15_1[0][0]                   __________________________________________________________________________________________________activation_30 (Activation)      (None, 5, 5, 128)    0           bn15_1[0][0]                     __________________________________________________________________________________________________conv15_2 (Conv2D)               (None, 3, 3, 256)    295168      activation_30[0][0]              __________________________________________________________________________________________________bn15_2 (BatchNormalization)     (None, 3, 3, 256)    1024        conv15_2[0][0]                   __________________________________________________________________________________________________activation_31 (Activation)      (None, 3, 3, 256)    0           bn15_2[0][0]                     __________________________________________________________________________________________________conv16_1 (Conv2D)               (None, 3, 3, 128)    32896       activation_31[0][0]              __________________________________________________________________________________________________bn16_1 (BatchNormalization)     (None, 3, 3, 128)    512         conv16_1[0][0]                   __________________________________________________________________________________________________activation_32 (Activation)      (None, 3, 3, 128)    0           bn16_1[0][0]                     __________________________________________________________________________________________________conv16_2 (Conv2D)               (None, 2, 2, 256)    295168      activation_32[0][0]              __________________________________________________________________________________________________bn16_2 (BatchNormalization)     (None, 2, 2, 256)    1024        conv16_2[0][0]                   __________________________________________________________________________________________________activation_33 (Activation)      (None, 2, 2, 256)    0           bn16_2[0][0]                     __________________________________________________________________________________________________conv17_1 (Conv2D)               (None, 2, 2, 64)     16448       activation_33[0][0]              __________________________________________________________________________________________________bn17_1 (BatchNormalization)     (None, 2, 2, 64)     256         conv17_1[0][0]                   __________________________________________________________________________________________________activation_34 (Activation)      (None, 2, 2, 64)     0           bn17_1[0][0]                     __________________________________________________________________________________________________conv17_2 (Conv2D)               (None, 1, 1, 128)    73856       activation_34[0][0]              __________________________________________________________________________________________________bn17_2 (BatchNormalization)     (None, 1, 1, 128)    512         conv17_2[0][0]                   __________________________________________________________________________________________________activation_35 (Activation)      (None, 1, 1, 128)    0           bn17_2[0][0]                     __________________________________________________________________________________________________conv11_mbox_conf (Conv2D)       (None, 19, 19, 63)   32319       activation_23[0][0]              __________________________________________________________________________________________________conv13_mbox_conf (Conv2D)       (None, 10, 10, 126)  129150      activation_27[0][0]              __________________________________________________________________________________________________conv14_2_mbox_conf (Conv2D)     (None, 5, 5, 126)    64638       activation_29[0][0]              __________________________________________________________________________________________________conv15_2_mbox_conf (Conv2D)     (None, 3, 3, 126)    32382       activation_31[0][0]              __________________________________________________________________________________________________conv16_2_mbox_conf (Conv2D)     (None, 2, 2, 126)    32382       activation_33[0][0]              __________________________________________________________________________________________________conv17_2_mbox_conf (Conv2D)     (None, 1, 1, 126)    16254       activation_35[0][0]              __________________________________________________________________________________________________conv11_mbox_loc (Conv2D)        (None, 19, 19, 12)   6156        activation_23[0][0]              __________________________________________________________________________________________________conv13_mbox_loc (Conv2D)        (None, 10, 10, 24)   24600       activation_27[0][0]              __________________________________________________________________________________________________conv14_2_mbox_loc (Conv2D)      (None, 5, 5, 24)     12312       activation_29[0][0]              __________________________________________________________________________________________________conv15_2_mbox_loc (Conv2D)      (None, 3, 3, 24)     6168        activation_31[0][0]              __________________________________________________________________________________________________conv16_2_mbox_loc (Conv2D)      (None, 2, 2, 24)     6168        activation_33[0][0]              __________________________________________________________________________________________________conv17_2_mbox_loc (Conv2D)      (None, 1, 1, 24)     3096        activation_35[0][0]              __________________________________________________________________________________________________conv11_mbox_conf_flat (Flatten) (None, 22743)        0           conv11_mbox_conf[0][0]           __________________________________________________________________________________________________conv13_mbox_conf_flat (Flatten) (None, 12600)        0           conv13_mbox_conf[0][0]           __________________________________________________________________________________________________conv14_2_mbox_conf_flat (Flatte (None, 3150)         0           conv14_2_mbox_conf[0][0]         __________________________________________________________________________________________________conv15_2_mbox_conf_flat (Flatte (None, 1134)         0           conv15_2_mbox_conf[0][0]         __________________________________________________________________________________________________conv16_2_mbox_conf_flat (Flatte (None, 504)          0           conv16_2_mbox_conf[0][0]         __________________________________________________________________________________________________conv17_2_mbox_conf_flat (Flatte (None, 126)          0           conv17_2_mbox_conf[0][0]         __________________________________________________________________________________________________conv11_mbox_loc_flat (Flatten)  (None, 4332)         0           conv11_mbox_loc[0][0]            __________________________________________________________________________________________________conv13_mbox_loc_flat (Flatten)  (None, 2400)         0           conv13_mbox_loc[0][0]            __________________________________________________________________________________________________conv14_2_mbox_loc_flat (Flatten (None, 600)          0           conv14_2_mbox_loc[0][0]          __________________________________________________________________________________________________conv15_2_mbox_loc_flat (Flatten (None, 216)          0           conv15_2_mbox_loc[0][0]          __________________________________________________________________________________________________conv16_2_mbox_loc_flat (Flatten (None, 96)           0           conv16_2_mbox_loc[0][0]          __________________________________________________________________________________________________conv17_2_mbox_loc_flat (Flatten (None, 24)           0           conv17_2_mbox_loc[0][0]          __________________________________________________________________________________________________mbox_conf (Concatenate)         (None, 40257)        0           conv11_mbox_conf_flat[0][0]                                                                       conv13_mbox_conf_flat[0][0]                                                                       conv14_2_mbox_conf_flat[0][0]                                                                     conv15_2_mbox_conf_flat[0][0]                                                                     conv16_2_mbox_conf_flat[0][0]                                                                     conv17_2_mbox_conf_flat[0][0]    __________________________________________________________________________________________________mbox_loc (Concatenate)          (None, 7668)         0           conv11_mbox_loc_flat[0][0]                                                                        conv13_mbox_loc_flat[0][0]                                                                        conv14_2_mbox_loc_flat[0][0]                                                                      conv15_2_mbox_loc_flat[0][0]                                                                      conv16_2_mbox_loc_flat[0][0]                                                                      conv17_2_mbox_loc_flat[0][0]     __________________________________________________________________________________________________mbox_conf_logits (Reshape)      (None, 1917, 21)     0           mbox_conf[0][0]                  __________________________________________________________________________________________________conv11_mbox_priorbox (PriorBox) (None, 1083, 8)      0           activation_23[0][0]              __________________________________________________________________________________________________conv13_mbox_priorbox (PriorBox) (None, 600, 8)       0           activation_27[0][0]              __________________________________________________________________________________________________conv14_2_mbox_priorbox (PriorBo (None, 150, 8)       0           activation_29[0][0]              __________________________________________________________________________________________________conv15_2_mbox_priorbox (PriorBo (None, 54, 8)        0           activation_31[0][0]              __________________________________________________________________________________________________conv16_2_mbox_priorbox (PriorBo (None, 24, 8)        0           activation_33[0][0]              __________________________________________________________________________________________________conv17_2_mbox_priorbox (PriorBo (None, 6, 8)         0           activation_35[0][0]              __________________________________________________________________________________________________mbox_loc_final (Reshape)        (None, 1917, 4)      0           mbox_loc[0][0]                   __________________________________________________________________________________________________mbox_conf_final (Activation)    (None, 1917, 21)     0           mbox_conf_logits[0][0]           __________________________________________________________________________________________________mbox_priorbox (Concatenate)     (None, 1917, 8)      0           conv11_mbox_priorbox[0][0]                                                                        conv13_mbox_priorbox[0][0]                                                                        conv14_2_mbox_priorbox[0][0]                                                                      conv15_2_mbox_priorbox[0][0]                                                                      conv16_2_mbox_priorbox[0][0]                                                                      conv17_2_mbox_priorbox[0][0]     __________________________________________________________________________________________________predictions (Concatenate)       (None, 1917, 33)     0           mbox_loc_final[0][0]                                                                              mbox_conf_final[0][0]                                                                             mbox_priorbox[0][0]              ==================================================================================================Total params: 8,624,505Trainable params: 8,599,161Non-trainable params: 25,344__________________________________________________________________________________________________ReferenceSingle Shot MultiBox Detector\nGitHub - rykov8/ssd_keras\nZhihu - Why is SSD weak in small object detection?\nZhihu - What improvements does SSD method have?\nZhihu-SSD\nZhihu-What does SSD train?\nPaper-Reading: SSD\nFeature Pyramid Networks\nPrecision &amp; Recall\nPseudo Code for precision\n","plink":"https://magi003769.github.io/post/SSD/"},{"title":"数据结构算法笔记：算法分析","date":"2018-02-06T03:55:30.000Z","date_formatted":{"ll":"Feb 5, 2018","L":"02/05/2018","MM-DD":"02-05"},"updated":"2019-05-04T04:15:02.000Z","content":"Algorithm AnalysisMathematical Background以下是几个算法复杂度分析中常用的定义，用来描述和比较两个函数，之间的关系。研究算法的意义在于，必须保证程序在处理数量众多的数据时依然保证尽可能的高效，而并非仅仅满足没些特殊的案例。 且在比较两个函数时，单纯的比较大小是没有没有意义的，一切忽视输入量的比较都是耍流氓。因此，我们更关注当处理数据的数量\nN\n\n\n\n\n \n\n 非常大时，两个函数的大小关系，或者，我们关注随着\nN\n\n\n\n\n \n\n 的增大函数增长速率的变化情况。\n\nDef 1\n\nT(N) = O(f(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n if there are positive constant \nc\n\n\n\n\n \n\n and \nn_0\n\n\n\n\n\n \n \n\n such that \nT(N) \\le cf(N)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n when \nN \\ge n_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n\n\nT(N) = O(f(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n 表示 \nT(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率小于等于 \nf(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率。\n\nDef 2\n\nT(N) = \\Omega(g(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n if there are positive constant \nc\n\n\n\n\n \n\n and \nn_0\n\n\n\n\n\n \n \n\n such that \nT(N) \\ge cg(N)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n when \nN \\ge n_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n\n\nT(N) = \\Omega(g(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n 表示 \nT(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率大于等于 \ng(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率。\n\nDef 3\n\nT(N) = \\Theta(h(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n if and only if \nT(N) = O(h(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n and \nT(N) = \\Omega(h(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nT(N) = \\Theta(h(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n 表示 \nT(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率等于 \ng(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率。\n\nDef 4\n\nT(N) = o(p(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n if, for all positive constant \nc\n\n\n\n\n \n\n , there exist an \nn_0\n\n\n\n\n\n \n \n\n such that \nT(N) &lt; cp(N)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n when \nN&gt;n_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n. \nOr, less formally, \nT(N) = o(p(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n if \nT(N)=O(p(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n and \nT(N) \\neq \\Theta(p(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nT(N) = o(p(N))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n 表示 \nT(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率小于 \nf(N)\n\n\n\n\n\n\n\n \n \n \n \n\n 的增长速率。\n下面的表格中列举了几个典型的函数类型，另外还有几个重要的结论我们需要注意：\n\nRule 1\nIf \nT_1(N) = O(f(N))\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n and \nT_2(N) = O(g(N))\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n, then\n\nT_1(N) + T_2(N) = O(f(N) + g(N))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\nT_1(N) * T_2(N) = O(f(N) * g(N))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\nRule 2\nIf T(N) is a polynomial of degree \nk\n\n\n\n\n \n\n, then \nT(N) = \\Theta(N^k)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n\nRule 3\n\nlog^k N = O(N)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n \n \n\n for any constant \nk\n\n\n\n\n \n\n. This tells us that logarithms grow very slowly.\n\n\nFunctionName\n\n\nc\n\n\n\n\n \n\nconstant\n\n\nlogN\n\n\n\n\n\n\n\n \n \n \n \n\nlogarithmic\n\n\nlog^2N\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\nlog-squared\n\n\nN\n\n\n\n\n \n\nlinear\n\n\nNlogN\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n\nN^2\n\n\n\n\n\n \n \n\nquadratic\n\n\nN^3\n\n\n\n\n\n \n \n\ncubic\n\n\n2^N\n\n\n\n\n\n \n \n\nexponential\nComplexityTime complexity\nDef. Time complexity quantifies the amount of time taken by an algorithm to run as a function of the length \nn\n\n\n\n\n \n\n of input string.\n\nThe time complexity is commonly expressed by Big O notation ** which is described as **asymptotically as it excludes coefficients and lower order terms. \n\nFor the measurement of time complexity, it is based on the counting the number of elementary operations which takes fixed amount of time to perform in algorithm. Thus, the amount of time taken and the number of elementary operations performed differ by at most a constant factor, or in other word, are proportional. \nReferenceWiki: Analysis of algorithms\nWiki: Big O notation\nBigOCheatSheet\n","plink":"https://magi003769.github.io/post/Algorithm Analysis/"},{"title":"LeetCode：Bit Manipulation","date":"2018-01-28T03:55:30.000Z","date_formatted":{"ll":"Jan 27, 2018","L":"01/27/2018","MM-DD":"01-27"},"updated":"2020-05-10T23:06:28.000Z","content":"Bit Operation\nOperationSymbolIllustration\n\nBit-wise AND&amp;\n\nBit-wise OR|\n\nBit-wise XOR^\n\nInvert~\n\nShift left&lt;&lt;Logic Shift: covering positions with 0s\n\nShift right&gt;&gt;Logic Shift: covering positions with 0s\n\nShift (unsigned)&gt;&gt;&gt;Arithmetic Shift: original right-side bits will be shifted to the left-side\nBoolean AlgebraIt could be better to know some knowledge of Boolean Algebra when we try to understand some elegant solution of LeetCode problems in the discipline of bit manipulation. This section may also have some contents from my Digital Circuit module. \nAdder\nFigure above shows a half adder. The mathematic expression for such a unit can be describe as formula below. This adder is called “half” because it doesn’t consider about the carry input. \n\n\\begin{aligned}\nS &amp;= A \\oplus B \\\\\nC &amp;= AB\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n \n \n \n\n\n \n \n \n\n\n\n\n\nThus, for a full adder, the configuration considers about carry input as shown in figure below. \n\nThe mathematic expression of each output is\n\n\\begin{aligned}\nS &amp;= A \\oplus B \\oplus C_{in} \\\\\nC_{out} &amp;= AB + (C_{in}(A \\oplus B))\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n\n\n\n\n \n \n \n \n \n\n \n\n \n \n\n\n \n \n \n \n \n \n\n\n\n\n\nThese information may provide us some methods to resolve the problem “Addition of two integers”. Guided by the mathematic expression above (Here, the may only use half-adder), we have following procedures to implement addition with bit operators:\nsum &lt;- A ^ B\nC &lt;- A &amp; B\nB &lt;- C &lt;&lt; 1\nRepeat the procedures until the carry \nC\n\n\n\n\n \n\n becomes 0. This is exactly what we do when we manually calculate the binary addition. We firstly add the two number in bitwise and note the carries which should be shifted one bit to the left. \n\nComparatorAbove all, it is quite easy to compare whether two numbers are equal as we have ^ operator. If A and B are equal, the result of A^B will be 0, otherwise, it will be none-zero value. Of course, this inference confirms only when A and B have the same data type (e.g. both are int). \nExercisesAddition of two integers1234567891011121314151617181920#include &lt;iostream&gt;class Solution&#123;public:    static int getSum(int x, int y)    &#123;        while(y)&#123;            int carry = x &amp; y;            x ^= y;            y = carry &lt;&lt; 1;        &#125;        return x;    &#125;&#125;;int main()&#123;    std::cout &lt;&lt; Solution::getSum(18, 5);    return 0;&#125;Hamming Weight12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;time.h&gt;class Solution&#123;public:    static int Hamming_weight(int x)    &#123;        int weight = 0;        while (x != 0)&#123;            x &amp;= x - 1;            ++ weight;        &#125;        return weight;    &#125;&#125;;int main()&#123;    int result = Solution::Hamming_weight(64);    std::cout &lt;&lt; result;    return 0;&#125;To evaluate the performance, a C-standard-library time.h is used to measure the execution time. It can exactly measure the time when the function runs.\n12345678clock_t start, finish;start = clock();&#123;    /*      * the part you want to measure     */&#125;finish = clock();Here is a little trick to get rid of the annoying instantiation of class Solution during the practice coding.\n\nstatic methods are functions which only use the class as a namespace, and do not require an instance.\n\nHamming Distance123456789101112131415161718192021#include &lt;iostream&gt;class Solution&#123;public:    static int Hamming_Dist(int x, int y)    &#123;        int weight = 0;        while(x != 0)&#123;            x &amp;= x - 1;            ++ weight;        &#125;        return weight;    &#125;&#125;;int main()&#123;    int x = 7, y = 4;    std::cout &lt;&lt; Solution::Hamming_Dist(x, y);    return 0;&#125;Single Number1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;vector&gt;class Solution &#123;public:    static int singleNumber(std::vector&lt;int&gt;&amp; nums) &#123;        int result = nums[0];        for(int i=1; i&lt;nums.size(); i++)&#123;            result = result ^ nums[i];        &#125;        return result;    &#125;&#125;;int main()&#123;    int my_nums[] = &#123;1, 1, 2, 2, 3, 3, 4, 4, 5, 6, 6, 7, 7, 8, 8&#125;;    std::vector&lt;int&gt; nums(my_nums, my_nums + sizeof(my_nums) / sizeof(int) ); // following C++98, the old fashion    std::cout &lt;&lt; Solution::singleNumber(nums);    return 0;&#125;","plink":"https://magi003769.github.io/post/Bit Operation/"},{"title":"Python小爬虫——网页pdf文档批量下载","date":"2017-09-05T21:55:30.000Z","date_formatted":{"ll":"Sep 5, 2017","L":"09/05/2017","MM-DD":"09-05"},"updated":"2019-05-04T04:20:46.000Z","content":"最近在看MIT的算法课，需要看当堂的notes、assignment还有solution。但是本人实在是懒得不行的那种人，不想一个一个点着下载在整理，再加上早听说过原来就有ICS的大佬自己写过爬虫的东西。于是想试试python的乞丐版爬虫，实践一把，算是强行给自己加戏吧，写个脚本，连工程都谈不上。\nBased onrequests\nBeautifulSoup\nImplementation以下是基于抓取单个网页的所有PDF的例子加以简单修改的程序，用户可以自定义下载文件夹实现多次下载的分类。\n123456789101112131415#file-name: pdf_download.py__author__ = 'rxread'import requestsfrom bs4 import BeautifulSoupimport osdef download_file(url, index, folder):    download_dir = '.\\\\' + folder + '\\\\' + '&#123;:02&#125;'.format(index) + url.split('/')[-1]    r = requests.get(url, stream=True)    with open(download_dir, 'wb') as f:        for chunk in r.iter_content(chunk_size=1024):            if chunk:                f.write(chunk)                f.flush()    return download_dir以上是下载pdf文件的函数。下面才是重点\n1234567891011121314151617def from_page(folder):    root_link = 'https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-notes/'    r = requests.get(root_link)    if r.status_code == 200:        soup = BeautifulSoup(r.text, \"html5lib\")        index = 1        print(\"\\n=============== &#123;0:10&#125; ===============\\n\".format('Start Downloading'))        for link in soup.find_all('a'):            new_link = 'https://ocw.mit.edu' + link.get('href')            if new_link.endswith('.pdf'):                download_dir = download_file(new_link, index, folder)                print(\"Dowloading: &#123;0:30&#125; ==&gt;  &#123;0:30&#125;\".format(new_link.split('/')[-1], download_dir))                index += 1        print(\"\\n=============== &#123;0:10&#125; ===============\\n\".format('Download Finished'))        print('Totally %d files have been downloaded.')    else:        print(\"ERRORS occur !!!\")request用于请求网页，并获取网页信息。之后这些获取的信息（html的tag信息）则会由BeautifulSoup包来解析并提取下载链接。\nReference Resources先谢谢大佬们带小白入门。\n抓取单个网页的所有PDF\n知乎-如何入门python爬虫\n","thumbnail":"https://source.unsplash.com/eIhH7RTlTZA","plink":"https://magi003769.github.io/post/Python小爬虫——网页pdf文档批量下载/"},{"title":"A guide for Hexo and Github pages","date":"2017-07-10T05:32:35.000Z","date_formatted":{"ll":"Jul 9, 2017","L":"07/09/2017","MM-DD":"07-09"},"updated":"2020-05-11T05:20:49.335Z","content":"\n“Remember that the faith that moves mountains always carries a pick.” — Mark Twin\n\nThis is the first post written in this blog which mainly recorded the construction of environment to use hexo. Following links can be good guide for use and understand.\n手把手\n简书，起今之行\nLocal environment constructionSuch a construction can be divided into following steps:\nInstall nodejs \nGit is necessary\nCreate a fold in a directory you prefer and then, under this directory, use commands in cmd (yes, the command line of windows)123456npm install hexo-cli -ghexo init &lt;name-of-bolg&gt;cd .\\name-of-blognmp install        # this step is crucial, only in this directory will the hexo workhexo g             # or hexo generate to create the public fileshexo s -p 3600     # sometimes the default local host 4000 can be occupied\nTheme configurationInstallation and Usage12hexo cleangit clone &lt;the-url-of-theme-repository&gt; &lt;.\\\\themes\\\\name-of-theme&gt;What should be emphasized here is that the name of configuration file should be modified to the correct form _config.yml. Additionally, it is necessary to edit theme feature in _config.yml for the whole project as well. After that, use command to investigate preview of blog where the theme has changed. \n12hexo ghexo s -p 3600Update12cd theme/&lt;name-of-theme&gt; # change working directory to the local repository for the themegit pull origin master # synchronize with originIt is better to backup your  _config.yml file of the theme before starting update. \nDeploy to Github pagesA special repository whose name is ‘user-name.github.io’ should be firstly created on Github and set the Github pages selections. Then, it is time to deploy the Hexo static blog to Github. Note that our construction must be done with in this folder. Otherwise, you cannot use Git to deploy your blog to Github pages.\nMake following modifications in _config.yml for the whole project\n1234deploy:  type: git  repo: url of your special repository  branch: masterIn cmd, \n1hexo dEvery time you deploy the blog, it is equivalent to make a commit on the repository which means changes will occur on pages hosted by Github as well. Such a synchronization may not perfect as there might some delay for the update on Github page.\nIn fact, before the command of deployment, we need to ensure the installation of an extension of Hexo:\n1npm install hexo-deployer-git --saveStart your writingCommand hexo n will automatically create a .md file in ./source/_posts. Your writing is just in those .md files. Actually, once the blog constructed, you just need create .md files for your posts.\nA new page can be created by hexo new page &lt;page-name&gt;. \nExtend packagesThis section is a collection of some required extend packages for different features. This really depends on your theme and their implementation. It is better to check documents of specified themes. Here is just a records.\nReading InformationShown below the post title, read information contains a statistic of word count and an estimation of reading time. My current blog theme archer has this feature which depends on hexo-wordcount. Type following command to install this required package and add segment reading_info: true into theme configuration file. \n1npm install --save hexo-wordcountMath ExpressionSometimes, the rendering of formula in .md file using LaTex syntax can goes wrong. The solution is to use Mathjax and Kramed rather than hexo-math and Marked. This link introduce us the solution step by step. \n搭建一个支持LaTEX的hexo博客\n","plink":"https://magi003769.github.io/post/hexo-guide/"},{"title":"Hello World","date":"1995-10-19T05:32:35.000Z","date_formatted":{"ll":"Oct 18, 1995","L":"10/18/1995","MM-DD":"10-18"},"updated":"2017-09-30T21:32:14.000Z","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new \"My New Post\"More info: Writing\nRun server1$ hexo serverMore info: Server\nGenerate static files1$ hexo generateMore info: Generating\nDeploy to remote sites1$ hexo deployMore info: Deployment\n","plink":"https://magi003769.github.io/post/hello-world/"},{"title":"","date":"2020-05-11T04:06:30.028Z","date_formatted":{"ll":"May 10, 2020","L":"05/10/2020","MM-DD":"05-10"},"updated":"2020-05-11T04:06:30.023Z","content":"","plink":"https://magi003769.github.io//404/"},{"title":"Hello there","date":"2019-05-04T04:36:12.000Z","date_formatted":{"ll":"May 3, 2019","L":"05/03/2019","MM-DD":"05-03"},"updated":"2019-06-25T21:40:34.000Z","content":"This is a trail page under editing.\nCheck my Resume in Chinese / English version.\n","plink":"https://magi003769.github.io/about/"}]