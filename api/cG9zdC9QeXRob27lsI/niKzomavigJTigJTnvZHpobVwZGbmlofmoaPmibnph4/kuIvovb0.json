{"title":"Python小爬虫——网页pdf文档批量下载","date":"2017-09-05T21:55:30.000Z","date_formatted":{"ll":"Sep 5, 2017","L":"09/05/2017","MM-DD":"09-05"},"thumbnail":"https://source.unsplash.com/eIhH7RTlTZA","link":"post/Python小爬虫——网页pdf文档批量下载","tags":["python","爬虫"],"categories":["Python"],"updated":"2019-05-04T04:20:46.000Z","content":"<p>最近在看MIT的算法课，需要看当堂的notes、assignment还有solution。但是本人实在是懒得不行的那种人，不想一个一个点着下载在整理，再加上早听说过原来就有ICS的大佬自己写过爬虫的东西。于是想试试python的乞丐版爬虫，实践一把，算是强行给自己加戏吧，写个脚本，连工程都谈不上。</p>\n<a id=\"more\"></a><h1 id=\"based-on\">Based on<a href=\"#based-on\" title=\"Based on\"></a></h1><ul><li><a href=\"http://docs.python-requests.org/en/master/\" target=\"_blank\">requests</a></li>\n<li><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\">BeautifulSoup</a></li>\n</ul><h1 id=\"implementation\">Implementation<a href=\"#implementation\" title=\"Implementation\"></a></h1><p>以下是基于<a href=\"http://blog.zanlabs.com/2014/11/11/python-webpage-crawling/\" target=\"_blank\">抓取单个网页的所有PDF</a>的例子加以简单修改的程序，用户可以自定义下载文件夹实现多次下载的分类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#file-name: pdf_download.py</span></span><br><span class=\"line\">__author__ = <span class=\"string\">'rxread'</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">download_file</span><span class=\"params\">(url, index, folder)</span>:</span></span><br><span class=\"line\">    download_dir = <span class=\"string\">'.\\\\'</span> + folder + <span class=\"string\">'\\\\'</span> + <span class=\"string\">'&#123;:02&#125;'</span>.format(index) + url.split(<span class=\"string\">'/'</span>)[<span class=\"number\">-1</span>]</span><br><span class=\"line\">    r = requests.get(url, stream=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(download_dir, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> chunk <span class=\"keyword\">in</span> r.iter_content(chunk_size=<span class=\"number\">1024</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> chunk:</span><br><span class=\"line\">                f.write(chunk)</span><br><span class=\"line\">                f.flush()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> download_dir</span><br></pre></td></tr></table></figure><p>以上是下载pdf文件的函数。下面才是重点</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">from_page</span><span class=\"params\">(folder)</span>:</span></span><br><span class=\"line\">    root_link = <span class=\"string\">'https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-notes/'</span></span><br><span class=\"line\">    r = requests.get(root_link)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> r.status_code == <span class=\"number\">200</span>:</span><br><span class=\"line\">        soup = BeautifulSoup(r.text, <span class=\"string\">\"html5lib\"</span>)</span><br><span class=\"line\">        index = <span class=\"number\">1</span></span><br><span class=\"line\">        print(<span class=\"string\">\"\\n=============== &#123;0:10&#125; ===============\\n\"</span>.format(<span class=\"string\">'Start Downloading'</span>))</span><br><span class=\"line\">        <span class=\"keyword\">for</span> link <span class=\"keyword\">in</span> soup.find_all(<span class=\"string\">'a'</span>):</span><br><span class=\"line\">            new_link = <span class=\"string\">'https://ocw.mit.edu'</span> + link.get(<span class=\"string\">'href'</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> new_link.endswith(<span class=\"string\">'.pdf'</span>):</span><br><span class=\"line\">                download_dir = download_file(new_link, index, folder)</span><br><span class=\"line\">                print(<span class=\"string\">\"Dowloading: &#123;0:30&#125; ==&gt;  &#123;0:30&#125;\"</span>.format(new_link.split(<span class=\"string\">'/'</span>)[<span class=\"number\">-1</span>], download_dir))</span><br><span class=\"line\">                index += <span class=\"number\">1</span></span><br><span class=\"line\">        print(<span class=\"string\">\"\\n=============== &#123;0:10&#125; ===============\\n\"</span>.format(<span class=\"string\">'Download Finished'</span>))</span><br><span class=\"line\">        print(<span class=\"string\">'Totally %d files have been downloaded.'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"ERRORS occur !!!\"</span>)</span><br></pre></td></tr></table></figure><p>request用于请求网页，并获取网页信息。之后这些获取的信息（html的tag信息）则会由BeautifulSoup包来解析并提取下载链接。</p>\n<h1 id=\"reference-resources\">Reference Resources<a href=\"#reference-resources\" title=\"Reference Resources\"></a></h1><p>先谢谢大佬们带小白入门。</p>\n<ul><li><a href=\"http://blog.zanlabs.com/2014/11/11/python-webpage-crawling/\" target=\"_blank\">抓取单个网页的所有PDF</a></li>\n<li><a href=\"https://www.zhihu.com/question/20899988\" target=\"_blank\">知乎-如何入门python爬虫</a></li>\n</ul>","prev":{"title":"LeetCode：Bit Manipulation","link":"post/Bit Operation"},"next":{"title":"A guide for Hexo and Github pages","link":"post/hexo-guide"},"plink":"https://magi003769.github.io/post/Python小爬虫——网页pdf文档批量下载/","toc":[{"id":"based-on","title":"Based on","index":"1"},{"id":"implementation","title":"Implementation","index":"2"},{"id":"reference-resources","title":"Reference Resources","index":"3"}]}