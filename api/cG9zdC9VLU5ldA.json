{"title":"图像语义分割入门：FCN与U-Net","date":"2018-08-17T07:00:00.000Z","date_formatted":{"ll":"Aug 17, 2018","L":"08/17/2018","MM-DD":"08-17"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_segmentation.jpeg","link":"post/U-Net","tags":["FCN","Segmentation","U-Net"],"categories":["语义分割"],"updated":"2020-05-10T23:12:24.000Z","content":"<p>实习的第二天，台风中的七夕，承受着暴雨和暴击，开始读 paper。因为任务大致是检测出影像中的病灶区域，所以大致的方向就是语义分割这一块。老板说让读一读 U-Net 相关的的论文来了解一下，今天权当是语义分割入个门吧。</p>\n<a id=\"more\"></a><h1 id=\"fcn\">FCN<a href=\"#fcn\" title=\"FCN\"></a></h1><p><img src=\"http://www.2cto.com/uploadfile/Collfiles/20160426/20160426090440100.jpg\" class=\"φcy\" alt=\"FCN\"></p>\n<p>在 U-Net 之前，就已经有了 FCN (Fully convolutional networks) 用于图像的语义分割领域。所以，U-Net也多少受到了 FCN 工作的一些启发。因此，在讨论U-Net之前，先对 FCN 有一个简单的了解。</p>\n<p>从结构上可以看出，FCN的大部分结构还是与一般进行分类的 CNN 网络差别不大，都是数个卷积层 + 一个池化层的组合构成不同的 convolutional stage，从而产生多级的特征图。</p>\n<p>不同的地方在于后面的输出部分。一般用于分类的 CNN 模型，都是学习到 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"10.069ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewBox=\"0 -791.3 4335.4 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">1\\times1\\times N</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-4E\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"0\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"722\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"1723\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"2446\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMATHI-4E\" x=\"3446\" y=\"0\"></use>\n</g>\n</svg> 的特征图输出，再将其延展（flatten）成一维的特征向量，通过若干全连接层和最后的 Softmax 输出一个置信向量进行分类。而 FCN 则没有延展的过程，并将尺寸较小的特征图坐上采样或者将不同的特征图融合后上采样，来输出分割的结果。因此，上面的框图结构是并不特别准确的。通道数为21的最小特征图到最终的分割预测，中间其实经过了如下图所示的过程，其实还是有点复杂的。</p>\n<p><img src=\"https://post-pic.nos-eastchina1.126.net/Deep-Learning/FCN-convs.jpg\" class=\"φcy\" alt=\"FCN convs\"></p>\n<p>上采样可以是插值 (Interpolation) 也可以是反卷积 (Deconvolution)，FCN使用的则是反卷积。它实际上也是一种卷积操作，只不过会根据输出尺寸、kernel 的大小以及步长等来改变 padding 部分的大小。个人感觉（我乱讲的）这种情况实际上引入了一定的噪声，并且会有一些信息损失。从下面作者给出的效果来说，<strong>融合了更多的特征图的预测效果更好</strong>。</p>\n<p><img src=\"https://post-pic.nos-eastchina1.126.net/Deep-Learning/FCN-Ns.jpg\" class=\"φcy\" alt=\"FCN-Ns\"></p>\n<h1 id=\"u-net\">U-Net<a href=\"#u-net\" title=\"U-Net\"></a></h1><p>分割模型我们大概都可以理解为一个 Encoder-Decoder 结果，先将原始图片中的特征进行编码，在想这些特征做解码，解码为分割结果。编码的通常是尺寸逐渐收缩的卷积层和池化层，而解码部分则是翻卷积或者有些简单粗暴的会直接上采样。这一点上 FCN 和 U-Net 都是一样的。U-Net 则是引入了一个类似 skip connection 的结构。</p>\n<h2 id=\"u-net-网络结构\">U-Net 网络结构<a href=\"#u-net-网络结构\" title=\"U-Net 网络结构\"></a></h2><p><img src=\"http://img.mp.itc.cn/upload/20170710/8d56f05766b24aa2b32af3c18c96cd61_th.jpg\" class=\"φcy\" alt=\"U-net architecture\"></p>\n<p>U-Net 的卷积层还是很常见的那种：<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"5.165ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewBox=\"0 -791.3 2223.9 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">3\\times3</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMAIN-33\" d=\"M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMAIN-33\" x=\"0\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"722\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-33\" x=\"1723\" y=\"0\"></use>\n</g>\n</svg> 卷积 + ReLU。每一个 convolutional stage 之间都是步长为2的 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"5.165ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewBox=\"0 -791.3 2223.9 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">2\\times2</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMAIN-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMAIN-32\" x=\"0\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"722\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-32\" x=\"1723\" y=\"0\"></use>\n</g>\n</svg> 卷积。作者在文中专门提到了，要非常认真的选择输入的尺寸，并且保证每一次 pooling 层的输入尺寸都是偶数，以保证分割的准确。</p>\n<p>除此之外，<strong>这个网络另一个有点神奇的地方就是：输入和输出的尺寸竟然是不一样的。</strong> 这一点也非常值得注意，原图和目标分割结果应该都适合输出尺寸一致的。所以在输入时我们需要对图片进行预处理，这样就出现了一个叫 Overlap-tile的方法。</p>\n<h2 id=\"解决的问题\">解决的问题<a href=\"#解决的问题\" title=\"解决的问题\"></a></h2><ul><li>可以分割（localization）</li>\n<li>需要相对少的数据训练，仍能达到不错的效果</li>\n<li>对目标部分形变的鲁棒</li>\n<li>分离相互接触的区域（cell segmentation）</li>\n</ul><h2 id=\"一些策略\">一些策略<a href=\"#一些策略\" title=\"一些策略\"></a></h2><ul><li>Overlap-tile Strategy: 在补全输入图像的时候，对边缘 padding 时间上是做镜像。</li>\n<li>Data augmentation: 增加数据量，模拟现实中的病灶形变</li>\n<li>Weighted Loss：相接触的部分中间的背景像素在 loss 中有更大的权重</li>\n</ul><h1 id=\"reference\">Reference<a href=\"#reference\" title=\"Reference\"></a></h1><ul><li><a href=\"https://zhuanlan.zhihu.com/p/31428783\" target=\"_blank\">图像语义分割入门+FCN/U-Net网络解析</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/22308032\" target=\"_blank\">图像语义分割之FCN和CRF</a></li>\n</ul>","prev":{"title":"经典网络结构——残差网络","link":"post/ResNet"},"next":{"title":"机器学习模型——决策树","link":"post/决策树"},"plink":"https://magi003769.github.io/post/U-Net/","toc":[{"id":"fcn","title":"FCN","index":"1"},{"id":"u-net","title":"U-Net","index":"2","children":[{"id":"u-net-网络结构","title":"U-Net 网络结构","index":"2.1"},{"id":"解决的问题","title":"解决的问题","index":"2.2"},{"id":"一些策略","title":"一些策略","index":"2.3"}]},{"id":"reference","title":"Reference","index":"3"}]}