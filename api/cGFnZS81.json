{"per_page":7,"total":8,"current":5,"data":[{"title":"机器学习模型——决策树","date":"2018-07-25T07:00:00.000Z","date_formatted":{"ll":"Jul 25, 2018","L":"07/25/2018","MM-DD":"07-25"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_Decision_tree.png","excerpt":"<p>这一篇post来讲一讲决策树，内容杂糅了李航的《统计学习方法》和周志华的“西瓜书”。内容包括决策树的基本算法、信息熵和信息增益、简单的算法实现。</p>","link":"post/决策树","tags":["Decision Tree","李航"],"categories":["机器学习"]},{"title":"机器学习模型——朴素贝叶斯法","date":"2018-07-15T07:00:00.000Z","date_formatted":{"ll":"Jul 15, 2018","L":"07/15/2018","MM-DD":"07-15"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_naive-bayes-classifier.jpg","excerpt":"<blockquote>\n<p>Too young. Too simple. Sometimes Naive. —— The Elder</p>\n</blockquote>","link":"post/朴素贝叶斯","tags":["Naive Bayes","分类模型","李航"],"categories":["机器学习"]},{"title":"Python中的装饰器与类中的property","date":"2018-06-30T07:00:00.000Z","date_formatted":{"ll":"Jun 30, 2018","L":"06/30/2018","MM-DD":"06-30"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/post-bg.jpg","excerpt":"<p>原来读 python 代码的时候，经常遇到 <code>@</code> 这个符号却并不知道是个啥意思。它经常出现在某一个类的函数定义前。这就是 python 的一个语法糖 —— 装饰器（Decorator）。考虑到模块化的设计，<strong>装饰器帮助我们在不改变函数定义的情况下，增加需要的功能</strong>，比如输出日志，性能测试，事务处理、缓存、权限校验等。</p>","link":"post/decorator","tags":["Decorator","Python","基本概念","面试"],"categories":["Python"]},{"title":"机器学习模型——逻辑斯谛回归","date":"2018-06-28T07:00:00.000Z","date_formatted":{"ll":"Jun 28, 2018","L":"06/28/2018","MM-DD":"06-28"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/linear_vs_logistic_regression.jpg","excerpt":"<p>逻辑斯谛回归（Logistic Regression）是统计学习中的一个经典方法。最大熵是概率模型学习中的一种准则，将其推广到分类问题得到最大熵模型（Maximum Entropy Model）。二者都属于对数线性模型模型。</p>","link":"post/逻辑斯谛","tags":["Logistic Regression","Machine Learning"],"categories":["机器学习"]},{"title":"PyTorch Note (2) —— Autograd","date":"2018-06-22T07:00:00.000Z","date_formatted":{"ll":"Jun 22, 2018","L":"06/22/2018","MM-DD":"06-22"},"excerpt":"<p><code>autograd</code> 包是 pytorch 构建神经网络的核心。他为所有的张量运算提供了自动求导的方法。而这是一个 <strong>define-by-run</strong> 的框架，这意味着反向传播的过程取决于你的代码如何运行。甚至每一次迭代都可以有所不同。</p>","link":"post/pytorch_autograd","tags":["Frameworks","PyTorch"],"categories":["学习笔记"]},{"title":"经典的降维方法——PCA","date":"2018-06-20T07:00:00.000Z","date_formatted":{"ll":"Jun 20, 2018","L":"06/20/2018","MM-DD":"06-20"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/pixel-water.jpg","excerpt":"<p>在数据处理和一部分特征工程中，降维一直都是非常重要的一步。这篇 post 就简单介绍一下主成分分析法（PCA）的计算过程和实现。</p>","link":"post/PCA","tags":["Machine Learning","PCA"],"categories":["机器学习"]},{"title":"PyTorch Note (1) —— Tensors and Operations","date":"2018-06-15T07:00:00.000Z","date_formatted":{"ll":"Jun 15, 2018","L":"06/15/2018","MM-DD":"06-15"},"excerpt":"<p>最近听说 PyTorch 很好用，所以就开始看一下。起码不用每次想看 tensor 维度的时候都必须要 feed 然后 run session。PyTorch 中的张量实现非常类似于 Numpy 中的 <code>ndarray</code>，因此在使用时有很多地方很类似于 Numpy。这里就记录一些 PyTorch 的常用接口。</p>","link":"post/pytorch_tensor_operation","tags":["Frameworks","PyTorch"],"categories":["学习笔记"]}]}