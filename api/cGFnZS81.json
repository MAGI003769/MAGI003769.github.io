{"per_page":7,"total":8,"current":5,"data":[{"title":"图像语义分割入门：FCN与U-Net","date":"2018-08-17T07:00:00.000Z","date_formatted":{"ll":"Aug 17, 2018","L":"08/17/2018","MM-DD":"08-17"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_segmentation.jpeg","excerpt":"<p>实习的第二天，台风中的七夕，承受着暴雨和暴击，开始读 paper。因为任务大致是检测出影像中的病灶区域，所以大致的方向就是语义分割这一块。老板说让读一读 U-Net 相关的的论文来了解一下，今天权当是语义分割入个门吧。</p>","link":"post/U-Net","tags":["FCN","Segmentation","U-Net"],"categories":["语义分割"]},{"title":"机器学习模型——决策树","date":"2018-07-25T07:00:00.000Z","date_formatted":{"ll":"Jul 25, 2018","L":"07/25/2018","MM-DD":"07-25"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_Decision_tree.png","excerpt":"<p>这一篇post来讲一讲决策树，内容杂糅了李航的《统计学习方法》和周志华的“西瓜书”。内容包括决策树的基本算法、信息熵和信息增益、简单的算法实现。</p>","link":"post/决策树","tags":["Decision Tree","李航"],"categories":["机器学习"]},{"title":"机器学习模型——朴素贝叶斯法","date":"2018-07-15T07:00:00.000Z","date_formatted":{"ll":"Jul 15, 2018","L":"07/15/2018","MM-DD":"07-15"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_naive-bayes-classifier.jpg","excerpt":"<blockquote>\n<p>Too young. Too simple. Sometimes Naive. —— The Elder</p>\n</blockquote>","link":"post/朴素贝叶斯","tags":["Naive Bayes","分类模型","李航"],"categories":["机器学习"]},{"title":"Python中的装饰器与类中的property","date":"2018-06-30T07:00:00.000Z","date_formatted":{"ll":"Jun 30, 2018","L":"06/30/2018","MM-DD":"06-30"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/post-bg.jpg","excerpt":"<p>原来读 python 代码的时候，经常遇到 <code>@</code> 这个符号却并不知道是个啥意思。它经常出现在某一个类的函数定义前。这就是 python 的一个语法糖 —— 装饰器（Decorator）。考虑到模块化的设计，<strong>装饰器帮助我们在不改变函数定义的情况下，增加需要的功能</strong>，比如输出日志，性能测试，事务处理、缓存、权限校验等。</p>","link":"post/decorator","tags":["Decorator","Python","基本概念","面试"],"categories":["Python"]},{"title":"机器学习模型——逻辑斯谛回归","date":"2018-06-28T07:00:00.000Z","date_formatted":{"ll":"Jun 28, 2018","L":"06/28/2018","MM-DD":"06-28"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/linear_vs_logistic_regression.jpg","excerpt":"<p>逻辑斯谛回归（Logistic Regression）是统计学习中的一个经典方法。最大熵是概率模型学习中的一种准则，将其推广到分类问题得到最大熵模型（Maximum Entropy Model）。二者都属于对数线性模型模型。</p>","link":"post/逻辑斯谛","tags":["Logistic Regression","Machine Learning"],"categories":["机器学习"]},{"title":"PyTorch Note (2) —— Autograd","date":"2018-06-22T07:00:00.000Z","date_formatted":{"ll":"Jun 22, 2018","L":"06/22/2018","MM-DD":"06-22"},"excerpt":"<p><code>autograd</code> 包是 pytorch 构建神经网络的核心。他为所有的张量运算提供了自动求导的方法。而这是一个 <strong>define-by-run</strong> 的框架，这意味着反向传播的过程取决于你的代码如何运行。甚至每一次迭代都可以有所不同。</p>","link":"post/pytorch_autograd","tags":["Frameworks","PyTorch"],"categories":["学习笔记"]},{"title":"经典的降维方法——PCA","date":"2018-06-20T07:00:00.000Z","date_formatted":{"ll":"Jun 20, 2018","L":"06/20/2018","MM-DD":"06-20"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/pixel-water.jpg","excerpt":"<p>在数据处理和一部分特征工程中，降维一直都是非常重要的一步。这篇 post 就简单介绍一下主成分分析法（PCA）的计算过程和实现。</p>","link":"post/PCA","tags":["Machine Learning","PCA"],"categories":["机器学习"]}]}