{"per_page":7,"total":7,"current":4,"data":[{"title":"经典网络结构——DenseNet","date":"2018-09-07T07:00:00.000Z","date_formatted":{"ll":"Sep 7, 2018","L":"09/07/2018","MM-DD":"09-07"},"excerpt":"<h1 id=\"densenet\">DenseNet<a href=\"#densenet\" title=\"DenseNet\"></a></h1><p>在引入 skip connection 的 ResNet 在训练更深层的网络取得成功之后，一种更加大胆的网络结构被提出了，这就是 DenseNet。相比于一般的残差网络每两层或者 bottleneck 中每三层做一次连接，DenseNet 中的模块则是将当前卷积层的输出与之前所有卷积层的输出拼接，并作为下一卷积层的输入。下图就说明了这样一个包含四个卷积层以及多个 skip connection 的模块。假设每个模块中有 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.583ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewBox=\"0 -791.3 681.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">L</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-4C\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"0\" y=\"0\"></use>\n</g>\n</svg> 个卷积层，则一共有 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"9.814ex\" height=\"5.676ex\" style=\"vertical-align: -1.838ex;\" viewBox=\"0 -1652.5 4225.4 2443.8\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">\\frac{L(L+1)}{2}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-4C\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n<g transform=\"translate(120,0)\">\n<rect stroke=\"none\" width=\"3985\" height=\"60\" x=\"0\" y=\"220\"></rect>\n<g transform=\"translate(60,770)\">\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"0\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"681\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"1071\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-2B\" x=\"1974\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"2975\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"3475\" y=\"0\"></use>\n</g>\n <use xlink:href=\"#E1-MJMAIN-32\" x=\"1742\" y=\"-687\"></use>\n</g>\n</g>\n</svg> 个跨层连接。</p>","link":"post/DenseNet","tags":["Deep Learning","DenseNet"],"categories":["深度学习"]},{"title":"经典网络结构——残差网络","date":"2018-09-01T07:00:00.000Z","date_formatted":{"ll":"Sep 1, 2018","L":"09/01/2018","MM-DD":"09-01"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/ryoji-iwata-697773-unsplash.jpg","excerpt":"<h1 id=\"residual-learning\">Residual Learning<a href=\"#residual-learning\" title=\"Residual Learning\"></a></h1><p>从2012年的 AlexNet 开始，深度学习模型在计算机视觉中得到了广泛的应用。2014的 GoogleNet 和牛津大学视觉组的 VGG，也都是非常成功的深度学习模型。这些模型都利用了深度卷积网络，后两者在深度和复杂度上的提升使得其性能在 AlexNet 的基础上又提升了数个百分点。在这样的背景下，就产生了一个假设：</p>","link":"post/ResNet","tags":["Deep Learning","ResNet"],"categories":["深度学习"]},{"title":"图像语义分割入门：FCN与U-Net","date":"2018-08-17T07:00:00.000Z","date_formatted":{"ll":"Aug 17, 2018","L":"08/17/2018","MM-DD":"08-17"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_segmentation.jpeg","excerpt":"<p>实习的第二天，台风中的七夕，承受着暴雨和暴击，开始读 paper。因为任务大致是检测出影像中的病灶区域，所以大致的方向就是语义分割这一块。老板说让读一读 U-Net 相关的的论文来了解一下，今天权当是语义分割入个门吧。</p>","link":"post/U-Net","tags":["FCN","Segmentation","U-Net"],"categories":["语义分割"]},{"title":"机器学习模型——决策树","date":"2018-07-25T07:00:00.000Z","date_formatted":{"ll":"Jul 25, 2018","L":"07/25/2018","MM-DD":"07-25"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_Decision_tree.png","excerpt":"<p>这一篇post来讲一讲决策树，内容杂糅了李航的《统计学习方法》和周志华的“西瓜书”。内容包括决策树的基本算法、信息熵和信息增益、简单的算法实现。</p>","link":"post/决策树","tags":["Decision Tree","李航"],"categories":["机器学习"]},{"title":"机器学习模型——朴素贝叶斯法","date":"2018-07-15T07:00:00.000Z","date_formatted":{"ll":"Jul 15, 2018","L":"07/15/2018","MM-DD":"07-15"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/HD_naive-bayes-classifier.jpg","excerpt":"<blockquote>\n<p>Too young. Too simple. Sometimes Naive. —— The Elder</p>\n</blockquote>","link":"post/朴素贝叶斯","tags":["Naive Bayes","分类模型","李航"],"categories":["机器学习"]},{"title":"Python中的装饰器与类中的property","date":"2018-06-30T07:00:00.000Z","date_formatted":{"ll":"Jun 30, 2018","L":"06/30/2018","MM-DD":"06-30"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/post-bg.jpg","excerpt":"<p>原来读 python 代码的时候，经常遇到 <code>@</code> 这个符号却并不知道是个啥意思。它经常出现在某一个类的函数定义前。这就是 python 的一个语法糖 —— 装饰器（Decorator）。考虑到模块化的设计，<strong>装饰器帮助我们在不改变函数定义的情况下，增加需要的功能</strong>，比如输出日志，性能测试，事务处理、缓存、权限校验等。</p>","link":"post/decorator","tags":["Decorator","Python","基本概念","面试"],"categories":["Python"]},{"title":"机器学习模型——逻辑斯谛回归","date":"2018-06-28T07:00:00.000Z","date_formatted":{"ll":"Jun 28, 2018","L":"06/28/2018","MM-DD":"06-28"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Machine-Learning/linear_vs_logistic_regression.jpg","excerpt":"<p>逻辑斯谛回归（Logistic Regression）是统计学习中的一个经典方法。最大熵是概率模型学习中的一种准则，将其推广到分类问题得到最大熵模型（Maximum Entropy Model）。二者都属于对数线性模型模型。</p>","link":"post/逻辑斯谛","tags":["Logistic Regression","Machine Learning"],"categories":["机器学习"]}]}