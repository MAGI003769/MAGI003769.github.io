{"per_page":7,"total":8,"current":4,"data":[{"title":"Binary Tree（2）","date":"2019-04-15T07:00:00.000Z","date_formatted":{"ll":"Apr 15, 2019","L":"04/15/2019","MM-DD":"04-15"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/alice-donovan-rouse-177313-unsplash.jpg","excerpt":"<p>这一篇post是之前Binary Tree的后续，主要记录一些Binary Tree相关的题目。其实LeetCode上有很多题是相同或者存在极大关联的，基本思想都差不多，一道解决了，其他的也就都解决了。解法的分析也尽量将递归和迭代两种方法都实现一下（当然我也只是尽量，迭代方法还真不一定看得懂）。</p>","link":"post/Tree-02","tags":["Binary Tree","LeetCode"],"categories":["算法数据结构"]},{"title":"Binary Tree（1）","date":"2019-04-05T07:00:00.000Z","date_formatted":{"ll":"Apr 5, 2019","L":"04/05/2019","MM-DD":"04-05"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/hello-i-m-nik-1399233-unsplash.jpg","excerpt":"<p>LeetCode 上面 <a href=\"https://leetcode.com/explore/learn/card/data-structure-tree/\" target=\"_blank\">Binary Tree</a> 专题的搬运。介绍二叉树的基本概念和一些 LeetCode 相关题目。</p>","link":"post/Tree-01","tags":["Binary Tree","LeetCode"],"categories":["算法数据结构"]},{"title":"牛顿法","date":"2019-04-05T07:00:00.000Z","date_formatted":{"ll":"Apr 5, 2019","L":"04/05/2019","MM-DD":"04-05"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Mathematics/HD_blackboard.jpg","excerpt":"<p>最早听说这个方法是在知乎的一个回答里，大佬手写 sqrt(x) 没有用 binary search 而是用了牛顿法去迭代，把面试官都懵逼了。后面刷题遇到了 <a href=\"https://leetcode.com/problems/sqrtx/\" target=\"_blank\">69. Sqrt(x)</a> 和 <a href=\"https://leetcode.com/problems/valid-perfect-square/\" target=\"_blank\">367.Valid Perfect Square</a>，就对这个牛顿法有了一些兴趣，本文大部分是搬运 references 中的东西。</p>","link":"post/牛顿法","tags":["Mathematics","Newton Method"],"categories":["学点数学"]},{"title":"Array and String","date":"2019-03-25T07:00:00.000Z","date_formatted":{"ll":"Mar 25, 2019","L":"03/25/2019","MM-DD":"03-25"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/home-3.jpg","excerpt":"<p>LeetCode 上面 <a href=\"https://leetcode.com/explore/learn/card/array-and-string/\" target=\"_blank\">Array and String</a> 专题的搬运。其实这两个没什么可讲的，主要就是记一下这个Card下面比较难刷的题目，和偶然发现的一些坑。</p>","link":"post/Array_String","tags":["Array","LeetCode","String"],"categories":["算法数据结构"]},{"title":"Batch Normalization：定义与实现","date":"2018-10-01T07:00:00.000Z","date_formatted":{"ll":"Oct 1, 2018","L":"10/01/2018","MM-DD":"10-01"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/Deep-Learning/HD_BN.png","excerpt":"<p>现在BN基本上是深度学习或者说CNN的标配，他可以帮助提升训练速度，提升模型的效果。强烈推荐这两篇专栏文章：<a href=\"https://zhuanlan.zhihu.com/p/34879333\" target=\"_blank\">Batch Normalization原理与实战</a>、<a href=\"https://zhuanlan.zhihu.com/p/33173246\" target=\"_blank\">详解深度学习中的Normalization，BN/LN/WN</a>。本篇也基本就是这两篇的搬运和拼凑。</p>","link":"post/Batch norm","tags":["Batch Normalization","Deep Learning"],"categories":["深度学习"]},{"title":"经典网络结构——DenseNet","date":"2018-09-07T07:00:00.000Z","date_formatted":{"ll":"Sep 7, 2018","L":"09/07/2018","MM-DD":"09-07"},"excerpt":"<h1 id=\"densenet\">DenseNet<a href=\"#densenet\" title=\"DenseNet\"></a></h1><p>在引入 skip connection 的 ResNet 在训练更深层的网络取得成功之后，一种更加大胆的网络结构被提出了，这就是 DenseNet。相比于一般的残差网络每两层或者 bottleneck 中每三层做一次连接，DenseNet 中的模块则是将当前卷积层的输出与之前所有卷积层的输出拼接，并作为下一卷积层的输入。下图就说明了这样一个包含四个卷积层以及多个 skip connection 的模块。假设每个模块中有 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.583ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewBox=\"0 -791.3 681.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">L</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-4C\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"0\" y=\"0\"></use>\n</g>\n</svg> 个卷积层，则一共有 <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"9.814ex\" height=\"5.676ex\" style=\"vertical-align: -1.838ex;\" viewBox=\"0 -1652.5 4225.4 2443.8\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">\\frac{L(L+1)}{2}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-4C\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"></path>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n<g transform=\"translate(120,0)\">\n<rect stroke=\"none\" width=\"3985\" height=\"60\" x=\"0\" y=\"220\"></rect>\n<g transform=\"translate(60,770)\">\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"0\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"681\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMATHI-4C\" x=\"1071\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-2B\" x=\"1974\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"2975\" y=\"0\"></use>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"3475\" y=\"0\"></use>\n</g>\n <use xlink:href=\"#E1-MJMAIN-32\" x=\"1742\" y=\"-687\"></use>\n</g>\n</g>\n</svg> 个跨层连接。</p>","link":"post/DenseNet","tags":["Deep Learning","DenseNet"],"categories":["深度学习"]},{"title":"经典网络结构——残差网络","date":"2018-09-01T07:00:00.000Z","date_formatted":{"ll":"Sep 1, 2018","L":"09/01/2018","MM-DD":"09-01"},"thumbnail":"https://post-pic.nos-eastchina1.126.net/header_imgs/ryoji-iwata-697773-unsplash.jpg","excerpt":"<h1 id=\"residual-learning\">Residual Learning<a href=\"#residual-learning\" title=\"Residual Learning\"></a></h1><p>从2012年的 AlexNet 开始，深度学习模型在计算机视觉中得到了广泛的应用。2014的 GoogleNet 和牛津大学视觉组的 VGG，也都是非常成功的深度学习模型。这些模型都利用了深度卷积网络，后两者在深度和复杂度上的提升使得其性能在 AlexNet 的基础上又提升了数个百分点。在这样的背景下，就产生了一个假设：</p>","link":"post/ResNet","tags":["Deep Learning","ResNet"],"categories":["深度学习"]}]}